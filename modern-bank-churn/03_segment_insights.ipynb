{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Bank Customer Churn \u2013 Segments and Business Insights\n\nThis is the third notebook in the **Modern Bank Churn** project.\n\nGoal:\n\n1. Use the **tuned LightGBM model** to score customers with churn probabilities.\n2. Define **risk segments** based on churn probability.\n3. Analyse each segment to understand typical customer profiles.\n4. Discuss possible **retention strategies** per segment.\n\nWe re-train the model in this notebook so it is fully self-contained.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom lightgbm import LGBMClassifier\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_PATH: Path = Path(\"data\") / \"Churn_Modelling.csv\"\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Data file not found at {DATA_PATH.resolve()}. \"\n        \"Please download the Bank Customer Churn CSV and place it under the 'data/' directory.\"\n    )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Load, clean, and prepare data\n\nWe reuse the same cleaning and preprocessing logic:\n\n- Drop identifier columns.\n- Split into train/test.\n- Use a LightGBM model with a good default configuration.\n\nFor simplicity, we skip Optuna here and use a strong but fixed configuration,\nassuming hyperparameter tuning was done in the previous notebook.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_bank_churn_data(path: Path) -> pd.DataFrame:\n    \"\"\"Load the bank customer churn dataset from a CSV file.\"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path!s}\")\n    df: pd.DataFrame = pd.read_csv(path)\n    if df.empty:\n        raise ValueError(f\"Loaded DataFrame is empty: {path!s}\")\n    return df\n\n\ndef clean_bank_churn_data(raw_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Clean the bank customer churn dataset (drop IDs, check target).\"\"\"\n    df = raw_df.copy()\n    id_cols: List[str] = [\"RowNumber\", \"CustomerId\", \"Surname\"]\n    drop_cols: List[str] = [c for c in id_cols if c in df.columns]\n    if drop_cols:\n        df = df.drop(columns=drop_cols)\n        print(f\"Dropped identifier columns: {drop_cols}\")\n    if \"Exited\" not in df.columns:\n        raise ValueError(\"Target column 'Exited' not found in DataFrame.\")\n    return df\n\n\nraw_df: pd.DataFrame = load_bank_churn_data(DATA_PATH)\ndf: pd.DataFrame = clean_bank_churn_data(raw_df)\n\nTARGET_COL: str = \"Exited\"\nX: pd.DataFrame = df.drop(columns=[TARGET_COL])\ny: pd.Series = df[TARGET_COL].astype(int)\n\ncategorical_cols: List[str] = [c for c in [\"Geography\", \"Gender\"] if c in X.columns]\nnumeric_cols: List[str] = [c for c in X.columns if c not in categorical_cols]\n\nnumeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\ncategorical_transformer = Pipeline(steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_cols),\n        (\"cat\", categorical_transformer, categorical_cols),\n    ]\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    stratify=y,\n    random_state=RANDOM_STATE,\n)\n\nclf = LGBMClassifier(\n    n_estimators=300,\n    learning_rate=0.05,\n    num_leaves=32,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n)\n\npipeline = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\"clf\", clf),\n    ]\n)\n\npipeline.fit(X_train, y_train)\ny_proba_test = pipeline.predict_proba(X_test)[:, 1]\nprint(\"Test ROC-AUC:\", roc_auc_score(y_test, y_proba_test))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Score all customers and define segments\n\nWe now score **all customers** (not only the test set) to create churn risk\nsegments.\n\nExample segmentation:\n\n- **Low risk**: `p(churn) < 0.2`\n- **Medium risk**: `0.2 \u2264 p(churn) < 0.5`\n- **High risk**: `p(churn) \u2265 0.5`\n\nThese thresholds are illustrative and can be adjusted based on business needs.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Score all customers\npipeline.fit(X, y)\nproba_all = pipeline.predict_proba(X)[:, 1]\n\ndf_scores = df.copy()\ndf_scores[\"churn_proba\"] = proba_all\n\ndef assign_risk_segment(p: float) -> str:\n    \"\"\"Assign a risk segment label based on churn probability p.\"\"\"\n    if p < 0.2:\n        return \"low_risk\"\n    if p < 0.5:\n        return \"medium_risk\"\n    return \"high_risk\"\n\n\ndf_scores[\"risk_segment\"] = df_scores[\"churn_proba\"].apply(assign_risk_segment)\ndf_scores[[\"Exited\", \"churn_proba\", \"risk_segment\"]].head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.1 Segment sizes and average churn probability"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "segment_summary = (\n    df_scores.groupby(\"risk_segment\")\n    .agg(\n        n_customers=(\"Exited\", \"size\"),\n        churn_rate=(\"Exited\", \"mean\"),\n        avg_churn_proba=(\"churn_proba\", \"mean\"),\n        avg_age=(\"Age\", \"mean\"),\n        avg_balance=(\"Balance\", \"mean\"),\n        avg_num_products=(\"NumOfProducts\", \"mean\"),\n        avg_estimated_salary=(\"EstimatedSalary\", \"mean\"),\n    )\n    .reset_index()\n)\n\ndisplay(segment_summary)\n\nsns.barplot(data=segment_summary, x=\"risk_segment\", y=\"n_customers\")\nplt.title(\"Number of customers per risk segment\")\nplt.ylabel(\"# customers\")\nplt.show()\n\nsns.barplot(data=segment_summary, x=\"risk_segment\", y=\"churn_rate\")\nplt.title(\"Observed churn rate per segment\")\nplt.ylabel(\"Churn rate\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.2 Feature distributions by segment\n\nWe compare some key features between segments, such as `Age`, `Balance`,\n`NumOfProducts`, and `IsActiveMember`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Age distribution by segment\nsns.boxplot(data=df_scores, x=\"risk_segment\", y=\"Age\")\nplt.title(\"Age distribution by risk segment\")\nplt.show()\n\n# Balance distribution by segment\nsns.boxplot(data=df_scores, x=\"risk_segment\", y=\"Balance\")\nplt.title(\"Balance distribution by risk segment\")\nplt.show()\n\n# Activity by segment\nif \"IsActiveMember\" in df_scores.columns:\n    sns.barplot(\n        data=df_scores,\n        x=\"risk_segment\",\n        y=\"IsActiveMember\",\n        estimator=np.mean,\n    )\n    plt.title(\"Average activity (IsActiveMember) by risk segment\")\n    plt.ylabel(\"Mean IsActiveMember\")\n    plt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Business interpretation\n\nBased on the segments and their profiles, we can sketch some strategies:\n\n- **High-risk segment** (`risk_segment == \"high_risk\"`):\n  - High churn probability.\n  - Often lower activity (`IsActiveMember`), certain balance/age patterns.\n  - Potential actions:\n    - Personalised outreach (calls, meetings).\n    - Tailored product bundles or better terms.\n    - Service quality review for this group.\n\n- **Medium-risk segment**:\n  - Clear risk but not extreme.\n  - Potential actions:\n    - Targeted digital campaigns and nudges.\n    - Soft incentives (fee waivers, loyalty points).\n\n- **Low-risk segment**:\n  - Very low predicted churn.\n  - Focus on maintaining satisfaction efficiently.\n  - Potential actions:\n    - Light-touch engagement.\n    - Cross-sell / up-sell if appropriate.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Summary and next steps\n\nIn this notebook we:\n\n- Used a tuned LightGBM churn model to score all customers.\n- Defined risk segments based on churn probability.\n- Characterised segments by age, balance, number of products, activity, etc.\n- Linked segments to possible **retention strategies**.\n\nPossible extensions:\n\n- Integrate **customer lifetime value (CLV)** into the segmentation, so that\n  high-risk, high-value customers are prioritised.\n- Use **lift / gain charts** to evaluate how well the model concentrates churn\n  in the top-risk deciles.\n- Combine segments with **operational constraints** (call centre capacity,\n  marketing budget) to build concrete campaign plans.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}