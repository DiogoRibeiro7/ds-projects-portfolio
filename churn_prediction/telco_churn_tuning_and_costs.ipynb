{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Telco Churn \u2013 Model Tuning and Cost-Sensitive Decision Making\n\nThis notebook builds on the **Telco churn project** and focuses on:\n\n1. **Hyperparameter tuning** for better predictive performance.\n2. **Cost-sensitive evaluation** \u2013 picking a decision threshold that aligns with\n   business costs (losing a customer vs. contacting a non-churner).\n\nWe use the same IBM Telco Customer Churn dataset and a similar preprocessing\npipeline, but now we:\n\n- Tune Logistic Regression and Random Forest with cross-validation.\n- Optimise the classification threshold with a simple cost model.\n\n> You can run this notebook independently of the previous one; it repeats the\n> essential data loading and preprocessing steps.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and Configuration\n\nWe import the usual stack:\n\n- `pandas`, `numpy` for data handling.\n- `matplotlib`, `seaborn` for plotting.\n- `scikit-learn` for preprocessing, model tuning, and evaluation.\n\nWe also set a random seed for reproducibility.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    precision_recall_curve,\n    roc_auc_score,\n    RocCurveDisplay,\n)\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.base import BaseEstimator\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_PATH: Path = Path(\"data\") / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Data file not found at {DATA_PATH.resolve()}. \"\n        \"Please download the Telco churn CSV from Kaggle and place it under the 'data/' directory.\"\n    )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data Loading and Cleaning\n\nWe reuse the same logic as in the first notebook:\n\n1. Load the CSV.\n2. Convert `TotalCharges` to numeric.\n3. Drop rows with missing `TotalCharges`.\n4. Drop duplicate `customerID` entries.\n\nThis gives us a clean customer-level dataset for modelling.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_telco_data(path: Path) -> pd.DataFrame:\n    \"\"\"Load the Telco churn data from CSV.\n\n    Args:\n        path: Path to the CSV file.\n\n    Returns:\n        DataFrame with raw Telco data.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the loaded DataFrame is empty.\n    \"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path!s}\")\n\n    df: pd.DataFrame = pd.read_csv(path)\n    if df.empty:\n        raise ValueError(f\"Loaded DataFrame is empty: {path!s}\")\n    return df\n\n\ndef clean_telco_data(raw_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Clean Telco data (types, missing values, duplicates).\n\n    Args:\n        raw_df: Raw Telco churn DataFrame.\n\n    Returns:\n        Cleaned DataFrame.\n    \"\"\"\n    df = raw_df.copy()\n\n    # Convert TotalCharges to numeric, coercing errors to NaN\n    if \"TotalCharges\" not in df.columns:\n        raise ValueError(\"Expected 'TotalCharges' column not found.\")\n    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n\n    # Show missing values\n    missing = df.isna().sum()\n    print(\"Missing values per column (non-zero only):\")\n    display(missing[missing > 0])\n\n    # Drop rows with missing TotalCharges\n    before = df.shape[0]\n    df = df.dropna(subset=[\"TotalCharges\"])\n    after = df.shape[0]\n    print(f\"Dropped {before - after} rows with missing TotalCharges.\")\n\n    # Drop duplicate customers\n    before = df.shape[0]\n    df = df.drop_duplicates(subset=[\"customerID\"])\n    after = df.shape[0]\n    print(f\"Dropped {before - after} duplicate customerID rows.\")\n\n    df = df.reset_index(drop=True)\n    return df\n\n\nraw_df = load_telco_data(DATA_PATH)\ntelco_df = clean_telco_data(raw_df)\ndisplay(telco_df.head())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe loaded and cleaned the Telco churn dataset. The data is now ready for:\n\n- Train\u2013test splitting.\n- Preprocessing (encoding + scaling).\n- Model training and tuning.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Train\u2013Test Split and Preprocessing\n\nWe now:\n\n1. Map the target `Churn` to 0/1.\n2. Drop `customerID` from the features.\n3. Split into train and test sets with stratification.\n4. Define a `ColumnTransformer` to:\n\n   - Scale numeric features.\n   - One-hot encode categorical features.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "TARGET_COL: str = \"Churn\"\n\nif TARGET_COL not in telco_df.columns:\n    raise KeyError(f\"Target column {TARGET_COL!r} not found.\")\n\nX: pd.DataFrame = telco_df.drop(columns=[TARGET_COL, \"customerID\"])\ny: pd.Series = telco_df[TARGET_COL].map({\"No\": 0, \"Yes\": 1})\n\ncategorical_cols: List[str] = [c for c in X.columns if X[c].dtype == \"O\"]\nnumeric_cols: List[str] = [c for c in X.columns if c not in categorical_cols]\n\nprint(\"Categorical columns:\", categorical_cols)\nprint(\"Numeric columns:\", numeric_cols)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    stratify=y,\n    random_state=RANDOM_STATE,\n)\n\nprint(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n\nnumeric_transformer = Pipeline(\n    steps=[(\"scaler\", StandardScaler())]\n)\ncategorical_transformer = Pipeline(\n    steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_cols),\n        (\"cat\", categorical_transformer, categorical_cols),\n    ]\n)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe prepared:\n\n- Feature matrix `X` and target `y` (0 = no churn, 1 = churn).\n- A stratified train\u2013test split.\n- A reusable preprocessing pipeline with scaling and one-hot encoding.\n\nNext we will define a general evaluation helper and establish a simple baseline.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Evaluation Helper and Baseline Model\n\nWe define a helper function `evaluate_classifier` that:\n\n- Fits the model.\n- Computes accuracy and ROC-AUC.\n- Prints a classification report.\n- Shows a confusion matrix and ROC curve.\n\nThen we fit a **dummy baseline** that always predicts the majority class.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def evaluate_classifier(\n    name: str,\n    model: BaseEstimator,\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n) -> Dict[str, float]:\n    \"\"\"Fit a classifier and evaluate it on train and test data.\n\n    Args:\n        name: Model name (for printing).\n        model: Unfitted sklearn estimator or pipeline.\n        X_train: Training features.\n        X_test: Test features.\n        y_train: Training labels (0/1).\n        y_test: Test labels (0/1).\n\n    Returns:\n        Dictionary with key metrics on the test set.\n    \"\"\"\n    print(f\"\\n===== {name} =====\")\n    model.fit(X_train, y_train)\n\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n\n    if hasattr(model, \"predict_proba\"):\n        y_proba_test = model.predict_proba(X_test)[:, 1]\n        roc_auc = roc_auc_score(y_test, y_proba_test)\n    else:\n        y_proba_test = None\n        roc_auc = np.nan\n\n    acc_train = accuracy_score(y_train, y_pred_train)\n    acc_test = accuracy_score(y_test, y_pred_test)\n\n    print(f\"Train accuracy: {acc_train:.3f}\")\n    print(f\"Test accuracy:  {acc_test:.3f}\")\n    if not np.isnan(roc_auc):\n        print(f\"Test ROC-AUC:  {roc_auc:.3f}\")\n\n    print(\"\\nClassification report (test):\")\n    print(classification_report(y_test, y_pred_test, target_names=[\"No churn\", \"Churn\"]))\n\n    cm = confusion_matrix(y_test, y_pred_test)\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\",\n        xticklabels=[\"Pred No\", \"Pred Yes\"],\n        yticklabels=[\"True No\", \"True Yes\"],\n    )\n    plt.title(f\"Confusion matrix - {name}\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.show()\n\n    if y_proba_test is not None:\n        RocCurveDisplay.from_predictions(y_test, y_proba_test)\n        plt.title(f\"ROC curve - {name}\")\n        plt.show()\n\n    return {\n        \"model\": name,\n        \"train_accuracy\": acc_train,\n        \"test_accuracy\": acc_test,\n        \"roc_auc\": float(roc_auc) if not np.isnan(roc_auc) else np.nan,\n    }\n\n\nbaseline_clf = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\"clf\", DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE)),\n    ]\n)\n\nbaseline_metrics = evaluate_classifier(\n    \"Baseline (Most Frequent)\", baseline_clf, X_train, X_test, y_train, y_test\n)\nbaseline_metrics\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nThe dummy classifier gives us a **reference level** of performance. Any\nreal model should:\n\n- Have significantly better ROC-AUC than ~0.5.\n- Improve recall and precision for the churn class.\n\nNow we move to **hyperparameter tuning** for Logistic Regression and Random Forest.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Hyperparameter Tuning with RandomizedSearchCV\n\nWe tune two models:\n\n1. **Logistic Regression** \u2013 mainly the regularisation strength `C`.\n2. **Random Forest** \u2013 depth, number of trees, and split criteria.\n\nWe use:\n\n- **RandomizedSearchCV** with 5-fold cross-validation.\n- `roc_auc` as the scoring metric.\n\nThe output is the set of best hyperparameters and their cross-validated score.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# 5.1 Logistic Regression tuning\n\nlog_reg_base = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\n            \"clf\",\n            LogisticRegression(\n                max_iter=1000,\n                random_state=RANDOM_STATE,\n                n_jobs=-1,\n            ),\n        ),\n    ]\n)\n\nlog_reg_param_distributions = {\n    \"clf__C\": np.logspace(-2, 2, 20),\n    \"clf__penalty\": [\"l2\"],\n    \"clf__solver\": [\"lbfgs\"],\n}\n\nlog_reg_search = RandomizedSearchCV(\n    estimator=log_reg_base,\n    param_distributions=log_reg_param_distributions,\n    n_iter=20,\n    scoring=\"roc_auc\",\n    cv=5,\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n    verbose=1,\n)\n\nlog_reg_search.fit(X_train, y_train)\n\nprint(\"Best Logistic Regression params:\", log_reg_search.best_params_)\nprint(\"Best CV ROC-AUC:\", log_reg_search.best_score_)\n\nbest_log_reg = log_reg_search.best_estimator_\nlog_reg_tuned_metrics = evaluate_classifier(\n    \"Logistic Regression (tuned)\", best_log_reg, X_train, X_test, y_train, y_test\n)\nlog_reg_tuned_metrics\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# 5.2 Random Forest tuning\n\nrf_base = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\n            \"clf\",\n            RandomForestClassifier(\n                n_estimators=200,\n                random_state=RANDOM_STATE,\n                n_jobs=-1,\n            ),\n        ),\n    ]\n)\n\nrf_param_distributions = {\n    \"clf__n_estimators\": [100, 200, 300, 400],\n    \"clf__max_depth\": [None, 5, 10, 15],\n    \"clf__min_samples_split\": [2, 4, 6, 8],\n    \"clf__min_samples_leaf\": [1, 2, 3, 4],\n    \"clf__max_features\": [\"sqrt\", \"log2\", 0.5, 0.8],\n}\n\nrf_search = RandomizedSearchCV(\n    estimator=rf_base,\n    param_distributions=rf_param_distributions,\n    n_iter=25,\n    scoring=\"roc_auc\",\n    cv=5,\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n    verbose=1,\n)\n\nrf_search.fit(X_train, y_train)\n\nprint(\"Best Random Forest params:\", rf_search.best_params_)\nprint(\"Best CV ROC-AUC:\", rf_search.best_score_)\n\nbest_rf = rf_search.best_estimator_\nrf_tuned_metrics = evaluate_classifier(\n    \"Random Forest (tuned)\", best_rf, X_train, X_test, y_train, y_test\n)\nrf_tuned_metrics\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe tuned Logistic Regression and Random Forest using **RandomizedSearchCV**.\nWe obtained:\n\n- Best hyperparameters for each model.\n- Improved ROC-AUC compared to default settings (typically).\n- Updated evaluation metrics on the held-out test set.\n\nNext we move beyond ROC-AUC and look at **decision thresholds** under a cost model.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Cost-Sensitive Evaluation and Threshold Tuning\n\nIn many churn problems, the cost of errors is **asymmetric**:\n\n- **False negative (FN)** \u2013 we fail to identify a churner \u2192 we lose a customer.\n- **False positive (FP)** \u2013 we flag a non-churner and maybe contact them\n  unnecessarily (discount, call, email).\n\nUsually:\n\n> Cost(FN) >> Cost(FP)\n\nWe introduce a simple cost model:\n\n- Cost per FN = `C_FN`\n- Cost per FP = `C_FP`\n\nFor a chosen threshold `t` on the churn probability:\n\n- If `p(churn) >= t` \u2192 predict churn (1).\n- Else \u2192 predict non-churn (0).\n\nWe then compute the **expected cost** per customer for each threshold and\nchoose the threshold that **minimises cost**.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def compute_cost_for_thresholds(\n    y_true: np.ndarray,\n    y_proba: np.ndarray,\n    thresholds: np.ndarray,\n    cost_fp: float,\n    cost_fn: float,\n) -> pd.DataFrame:\n    \"\"\"Compute cost for a range of thresholds.\n\n    Args:\n        y_true: True labels (0/1).\n        y_proba: Predicted probabilities for class 1.\n        thresholds: Array of thresholds to evaluate.\n        cost_fp: Cost of a false positive.\n        cost_fn: Cost of a false negative.\n\n    Returns:\n        DataFrame with threshold, confusion matrix components, and total cost.\n    \"\"\"\n    records: List[Dict[str, float]] = []\n\n    for t in thresholds:\n        y_pred = (y_proba >= t).astype(int)\n        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n        total_cost = cost_fp * fp + cost_fn * fn\n        cost_per_customer = total_cost / len(y_true)\n\n        records.append(\n            {\n                \"threshold\": float(t),\n                \"tn\": float(tn),\n                \"fp\": float(fp),\n                \"fn\": float(fn),\n                \"tp\": float(tp),\n                \"total_cost\": float(total_cost),\n                \"cost_per_customer\": float(cost_per_customer),\n            }\n        )\n\n    return pd.DataFrame.from_records(records)\n\n\n# We'll use the tuned Random Forest as our main model for threshold analysis\nbest_rf.fit(X_train, y_train)\ny_proba_test = best_rf.predict_proba(X_test)[:, 1]\n\n# Define costs (you can adjust these values to match a real business context)\nC_FP: float = 1.0   # e.g. cost of contacting a non-churner\nC_FN: float = 5.0   # e.g. cost of losing a churner\n\nthresholds = np.linspace(0.1, 0.9, 41)\ncost_df = compute_cost_for_thresholds(\n    y_true=y_test.to_numpy(),\n    y_proba=y_proba_test,\n    thresholds=thresholds,\n    cost_fp=C_FP,\n    cost_fn=C_FN,\n)\n\ndisplay(cost_df.head())\n\nbest_row = cost_df.loc[cost_df[\"cost_per_customer\"].idxmin()]\nprint(\"Best threshold by cost:\")\ndisplay(best_row)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "plt.figure(figsize=(8, 5))\nsns.lineplot(data=cost_df, x=\"threshold\", y=\"cost_per_customer\")\nplt.axvline(best_row[\"threshold\"], linestyle=\"--\")\nplt.title(\"Cost per customer vs threshold (Random Forest tuned)\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"Cost per customer\")\nplt.show()\n\n# Precision\u2013recall curve for additional context\nprec, rec, pr_thresholds = precision_recall_curve(y_test, y_proba_test)\n\nplt.figure(figsize=(8, 5))\nplt.plot(rec, prec)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision\u2013Recall curve (Random Forest tuned)\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe:\n\n- Defined a simple cost model with `C_FN` and `C_FP`.\n- Computed the cost per customer for many thresholds.\n- Identified the **threshold that minimises expected cost**.\n- Plotted cost vs threshold, and inspected the precision\u2013recall trade-off.\n\nThe optimal threshold is **rarely 0.5**, especially when the costs of errors\nare asymmetric. This is a key takeaway for real churn deployments.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Final Model, Threshold, and Business Interpretation\n\nWe now summarise the final configuration:\n\n1. **Model** \u2013 tuned Random Forest (or tuned Logistic Regression, depending on\n   the results).\n2. **Threshold** \u2013 the value that minimises cost under our assumptions.\n3. **Operational rule** \u2013 flag customers with `p(churn) >= threshold` for a\n   retention action.\n\nThis is easy to implement in production:\n\n- Score customers daily/weekly with the churn model.\n- Apply the chosen threshold.\n- Generate a list of high-risk customers for campaigns.\n\nYou can also:\n\n- Re-tune the cost parameters (`C_FP`, `C_FN`) to reflect actual business\n  estimates.\n- Recompute the optimal threshold for each scenario.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Next Steps\n\nPossible extensions to this notebook:\n\n1. **More sophisticated cost models**  \n   - Include *value of customer* (CLV) instead of a constant FN cost.  \n   - Different thresholds for different segments (e.g. high-value vs low-value).\n\n2. **Calibration and uplift**  \n   - Calibrate probabilities (Platt scaling / isotonic regression).  \n   - Build an uplift model to predict the *incremental* effect of an action.\n\n3. **Monitoring in production**  \n   - Track data drift in features.  \n   - Monitor churn model performance over time.  \n   - Retrain when performance degrades.\n\nTaken together with the first notebook, you now have:\n\n- A complete churn pipeline (EDA \u2192 models \u2192 interpretation).  \n- A tuned model with a **cost-aware decision threshold**, ready for a realistic\n  deployment scenario.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}