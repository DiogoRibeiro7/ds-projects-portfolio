{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# MovieLens Recommender System \u2013 Extended Project\n\nThis project focuses on **personalized recommendations** using the\n**MovieLens (ml-latest-small)** dataset.\n\nWe go beyond a basic recommender and include:\n\n- Solid **EDA with visuals** on users, items and ratings.\n- Multiple **baseline models** and visual comparison.\n- **Item-based collaborative filtering** with cosine similarity.\n- **Matrix factorization** with latent factors (implemented from scratch).\n- **Evaluation in both rating space and ranking space**:\n  - RMSE.\n  - Precision@K, Recall@K, Hit-rate.\n- A short section on **modern tools / libraries** in recommender systems.\n\nWe assume you have downloaded MovieLens `ml-latest-small` and placed:\n\n```text\ndata/ml-latest-small/ratings.csv\n```\n\nOptionally, if you also have:\n\n```text\ndata/ml-latest-small/movies.csv\n```\n\nwe will use it to display movie titles in recommendations.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration\n\nWe use:\n\n- `pandas`, `numpy` \u2013 core data handling.\n- `matplotlib`, `seaborn` \u2013 visual analysis.\n- `scikit-learn` \u2013 train/test split, cosine similarity, RMSE.\n\nAll recommender logic is implemented in **plain Python/NumPy** for clarity.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Tuple, Set\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nRATINGS_PATH: Path = Path(\"data\") / \"ml-latest-small\" / \"ratings.csv\"\nMOVIES_PATH: Path = Path(\"data\") / \"ml-latest-small\" / \"movies.csv\"\n\nif not RATINGS_PATH.exists():\n    raise FileNotFoundError(\n        f\"Ratings file not found at {RATINGS_PATH.resolve()}. \"\n        \"Please download MovieLens (ml-latest-small) and place ratings.csv under data/ml-latest-small/.\"\n    )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Load and inspect MovieLens ratings\n\nMovieLens ratings have the structure:\n\n- `userId` \u2013 integer user identifier.\n- `movieId` \u2013 integer item identifier.\n- `rating` \u2013 explicit rating (0.5\u20135.0).\n- `timestamp` \u2013 unix time.\n\nWe focus on `userId`, `movieId`, and `rating`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_movielens_ratings(path: Path) -> pd.DataFrame:\n    \"\"\"Load MovieLens ratings from CSV.\n\n    Args:\n        path: Path to `ratings.csv`.\n\n    Returns:\n        DataFrame with ratings.\n    \"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path!s}\")\n\n    df: pd.DataFrame = pd.read_csv(path)\n    if df.empty:\n        raise ValueError(f\"Loaded ratings DataFrame is empty: {path!s}\")\n    return df\n\n\nratings_raw: pd.DataFrame = load_movielens_ratings(RATINGS_PATH)\n\nprint(\"Shape:\", ratings_raw.shape)\nratings_raw.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.1 Basic statistics and visuals\n\nWe look at:\n\n- Number of users / movies / ratings.\n- Sparsity of the user\u2013item matrix.\n- Rating distribution.\n- Ratings per user and per movie (long\u2013tail behaviour).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "n_users: int = ratings_raw[\"userId\"].nunique()\nn_movies: int = ratings_raw[\"movieId\"].nunique()\n\nprint(f\"Users:  {n_users}\")\nprint(f\"Movies: {n_movies}\")\nprint(f\"Ratings: {len(ratings_raw)}\")\nprint(f\"Density: {len(ratings_raw) / (n_users * n_movies):.6f}\")\n\nfig, ax = plt.subplots(1, 1)\nsns.histplot(ratings_raw[\"rating\"], bins=10, kde=False, ax=ax)\nax.set_title(\"Rating distribution\")\nax.set_xlabel(\"Rating\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "user_counts = ratings_raw.groupby(\"userId\")[\"rating\"].count()\nmovie_counts = ratings_raw.groupby(\"movieId\")[\"rating\"].count()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.histplot(user_counts, bins=30, ax=axes[0])\naxes[0].set_title(\"Ratings per user\")\naxes[0].set_xlabel(\"# ratings\")\n\nsns.histplot(movie_counts, bins=30, ax=axes[1])\naxes[1].set_title(\"Ratings per movie\")\naxes[1].set_xlabel(\"# ratings\")\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Users \u2013 quantiles of ratings per user:\")\nprint(user_counts.quantile([0.25, 0.5, 0.9, 0.99]))\nprint(\"\\nMovies \u2013 quantiles of ratings per movie:\")\nprint(movie_counts.quantile([0.25, 0.5, 0.9, 0.99]))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We see a typical **long\u2013tail** pattern: a few very active users and very\npopular movies, and many with few interactions.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Train/test split\n\nWe randomly split into train and test:\n\n- Train: used to fit baselines and models.\n- Test: used **only for evaluation**.\n\nFor simplicity we do not use a temporal split here, but in production a\n**time\u2013based split** is often more realistic.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "train_df, test_df = train_test_split(\n    ratings_raw,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\nprint(\"Train size:\", train_df.shape[0])\nprint(\"Test size: \", test_df.shape[0])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Baseline models (with comparison plot)\n\nWe define several baselines:\n\n1. **Global mean** \u2013 always predict the same rating.\n2. **Movie mean** \u2013 per\u2013movie average rating.\n3. **User + movie bias** model:\n\n\\begin{align}\n\\hat r_{ui} = \\mu + b_u + b_i\n\\end{align}\n\nwhere:\n\n- `\u03bc` is global mean.\n- `b_u` is user bias.\n- `b_i` is movie bias.\n\nWe will compute **RMSE** on the test set and compare visually.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute root mean squared error.\n\n    Args:\n        y_true: True ratings.\n        y_pred: Predicted ratings.\n\n    Returns:\n        RMSE value.\n    \"\"\"\n    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n\n\ny_true = test_df[\"rating\"].to_numpy()\n\n# 4.1 Global mean\n\nglobal_mean: float = float(train_df[\"rating\"].mean())\nprint(\"Global mean rating:\", global_mean)\n\ny_pred_global = np.full_like(y_true, fill_value=global_mean, dtype=float)\nrmse_global = rmse(y_true, y_pred_global)\nprint(f\"Global mean RMSE: {rmse_global:.4f}\")\n\n# 4.2 Movie mean\n\nmovie_mean = train_df.groupby(\"movieId\")[\"rating\"].mean()\nmovie_mean_map = movie_mean.to_dict()\n\ny_pred_movie = test_df[\"movieId\"].map(movie_mean_map).fillna(global_mean).to_numpy()\nrmse_movie = rmse(y_true, y_pred_movie)\nprint(f\"Movie mean RMSE:  {rmse_movie:.4f}\")\n\n# 4.3 User + movie bias\n\nmu: float = global_mean\nuser_bias = train_df.groupby(\"userId\")[\"rating\"].mean() - mu\nmovie_bias = train_df.groupby(\"movieId\")[\"rating\"].mean() - mu\n\nuser_bias_map: Dict[int, float] = user_bias.to_dict()\nmovie_bias_map: Dict[int, float] = movie_bias.to_dict()\n\n\ndef predict_bias_row(row: pd.Series) -> float:\n    \"\"\"Predict rating using user+movie bias model for a single row.\n\n    Args:\n        row: Row with userId and movieId.\n\n    Returns:\n        Predicted rating.\n    \"\"\"\n    bu: float = float(user_bias_map.get(row[\"userId\"], 0.0))\n    bi: float = float(movie_bias_map.get(row[\"movieId\"], 0.0))\n    return mu + bu + bi\n\n\ny_pred_bias = test_df.apply(predict_bias_row, axis=1).to_numpy()\nrmse_bias = rmse(y_true, y_pred_bias)\nprint(f\"User+movie bias RMSE: {rmse_bias:.4f}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Visual comparison of baselines\n\nbaseline_results = pd.DataFrame(\n    {\n        \"model\": [\"Global mean\", \"Movie mean\", \"User+movie bias\"],\n        \"rmse\": [rmse_global, rmse_movie, rmse_bias],\n    }\n)\n\nsns.barplot(data=baseline_results, x=\"model\", y=\"rmse\")\nplt.title(\"Baseline models \u2013 RMSE on test set\")\nplt.ylabel(\"RMSE\")\nplt.xticks(rotation=15)\nplt.show()\n\n# Visualise distribution of user and movie biases\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.histplot(user_bias, bins=30, ax=axes[0])\naxes[0].set_title(\"User bias distribution\")\naxes[0].set_xlabel(\"b_u\")\n\nsns.histplot(movie_bias, bins=30, ax=axes[1])\naxes[1].set_title(\"Movie bias distribution\")\naxes[1].set_xlabel(\"b_i\")\n\nplt.tight_layout()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The user+movie bias model often becomes a strong **baseline**. More complex\nmodels should meaningfully improve over this.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. User\u2013item matrix\n\nWe build a **dense** user\u2013item matrix for the training set.\n\nThis is useful for:\n\n- Collaborative filtering with similarities.\n- Visual checks of sparsity.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Map ids to consecutive indices\n\nunique_users: np.ndarray = train_df[\"userId\"].unique()\nunique_movies: np.ndarray = train_df[\"movieId\"].unique()\n\nuser_id_to_index: Dict[int, int] = {uid: idx for idx, uid in enumerate(unique_users)}\nmovie_id_to_index: Dict[int, int] = {mid: idx for idx, mid in enumerate(unique_movies)}\n\nn_users_train: int = len(unique_users)\nn_movies_train: int = len(unique_movies)\n\nprint(f\"Train users: {n_users_train}, Train movies: {n_movies_train}\")\n\nR = np.zeros((n_users_train, n_movies_train), dtype=np.float32)\n\nfor row in train_df.itertuples(index=False):\n    u_idx = user_id_to_index[row.userId]\n    m_idx = movie_id_to_index[row.movieId]\n    R[u_idx, m_idx] = row.rating\n\nR[:5, :5]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Quick visual: small random block of the rating matrix\n\nsample_users = min(40, n_users_train)\nsample_movies = min(40, n_movies_train)\n\nsns.heatmap(\n    R[:sample_users, :sample_movies],\n    cmap=\"viridis\",\n    cbar=True,\n)\nplt.title(\"User\u2013item rating matrix (small block)\")\nplt.xlabel(\"Movies (subset)\")\nplt.ylabel(\"Users (subset)\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Most entries are zero (missing ratings). Collaborative filtering leverages\n**overlaps** between rows/columns to make predictions.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Item-based collaborative filtering (k-NN)\n\nWe use **cosine similarity** between movie rating vectors.\n\nFor a given `(user, movie)` pair:\n\n1. Consider all movies the user has rated.\n2. Look at similarities between the target movie and these movies.\n3. Take a similarity\u2013weighted average of their ratings.\n\nWe restrict evaluation to a sample of the test set for speed.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def compute_item_similarity_matrix(R: np.ndarray) -> np.ndarray:\n    \"\"\"Compute cosine similarity between items.\n\n    Args:\n        R: User\u2013item matrix of shape (n_users, n_items).\n\n    Returns:\n        Item\u2013item similarity matrix of shape (n_items, n_items).\n    \"\"\"\n    item_matrix = R.T  # items x users\n    sim = cosine_similarity(item_matrix)\n    return sim\n\n\nitem_sim_matrix = compute_item_similarity_matrix(R)\nitem_sim_matrix.shape\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def predict_item_based(\n    user_id: int,\n    movie_id: int,\n    R: np.ndarray,\n    item_sim: np.ndarray,\n    user_id_to_index: Dict[int, int],\n    movie_id_to_index: Dict[int, int],\n    k: int = 20,\n    default: float | None = None,\n) -> float:\n    \"\"\"Predict rating via item-based k-NN collaborative filtering.\n\n    Args:\n        user_id: User identifier.\n        movie_id: Movie identifier.\n        R: User\u2013item rating matrix.\n        item_sim: Item\u2013item similarity matrix.\n        user_id_to_index: Map from userId to matrix row.\n        movie_id_to_index: Map from movieId to matrix column.\n        k: Number of neighbours.\n        default: Fallback prediction if user/movie unknown.\n\n    Returns:\n        Predicted rating.\n    \"\"\"\n    if default is None:\n        default = float(global_mean)\n\n    u_idx = user_id_to_index.get(user_id)\n    m_idx = movie_id_to_index.get(movie_id)\n\n    if u_idx is None or m_idx is None:\n        return float(default)\n\n    user_ratings = R[u_idx, :]\n    sims = item_sim[m_idx, :]\n\n    rated_mask = user_ratings > 0\n    rated_indices = np.where(rated_mask)[0]\n\n    if rated_indices.size == 0:\n        return float(default)\n\n    sims_rated = sims[rated_indices]\n\n    k_use = min(k, rated_indices.size)\n    top_idx = np.argsort(sims_rated)[-k_use:]\n\n    neighbour_indices = rated_indices[top_idx]\n    neighbour_sims = sims_rated[top_idx]\n    neighbour_ratings = user_ratings[neighbour_indices]\n\n    if np.all(neighbour_sims == 0):\n        return float(neighbour_ratings.mean())\n\n    pred = np.dot(neighbour_sims, neighbour_ratings) / np.sum(np.abs(neighbour_sims))\n    return float(pred)\n\n\n# Evaluate on a subset of test set\n\ntest_sample = test_df.sample(n=min(5000, len(test_df)), random_state=RANDOM_STATE)\n\ny_true_sample = test_sample[\"rating\"].to_numpy()\n\nitem_cf_preds: List[float] = []\nfor row in test_sample.itertuples(index=False):\n    item_cf_preds.append(\n        predict_item_based(\n            user_id=row.userId,\n            movie_id=row.movieId,\n            R=R,\n            item_sim=item_sim_matrix,\n            user_id_to_index=user_id_to_index,\n            movie_id_to_index=movie_id_to_index,\n            k=30,\n            default=global_mean,\n        )\n    )\n\nrmse_item_cf = rmse(y_true_sample, np.array(item_cf_preds))\nprint(f\"Item-based CF RMSE (sample): {rmse_item_cf:.4f}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Item-based CF usually improves over simple baselines and is easy to\nexplain: *\"movies similar to what you rated highly\"*.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Matrix factorization (latent factors, SGD)\n\nWe now build a **latent factor** model with biases:\n\n\\begin{align}\n\\hat r_{ui} = \\mu + b_u + b_i + p_u^T q_i\n\\end{align}\n\nWhere:\n\n- `\u03bc` \u2013 global mean.\n- `b_u`, `b_i` \u2013 user and item biases.\n- `p_u`, `q_i` \u2013 k\u2013dimensional latent vectors.\n\nWe train using **stochastic gradient descent** on the training ratings.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "@dataclass\nclass MFParams:\n    n_factors: int = 40\n    n_epochs: int = 20\n    lr: float = 0.01\n    reg: float = 0.05\n\n\nclass MatrixFactorization:\n    \"\"\"Matrix factorization with biases trained via SGD.\n\n    This class is intentionally simple and readable, not optimised.\n    \"\"\"\n\n    def __init__(self, params: MFParams, random_state: int = 42) -> None:\n        self.params = params\n        self.random_state = random_state\n\n        self.mu: float | None = None\n        self.user_bias: Dict[int, float] = {}\n        self.item_bias: Dict[int, float] = {}\n        self.P: Dict[int, np.ndarray] = {}\n        self.Q: Dict[int, np.ndarray] = {}\n\n    def fit(self, df_ratings: pd.DataFrame) -> None:\n        \"\"\"Fit the model on a ratings DataFrame.\n\n        Args:\n            df_ratings: DataFrame with (`userId`, `movieId`, `rating`).\n        \"\"\"\n        rng = np.random.default_rng(self.random_state)\n\n        user_ids = df_ratings[\"userId\"].unique()\n        item_ids = df_ratings[\"movieId\"].unique()\n\n        self.mu = float(df_ratings[\"rating\"].mean())\n\n        self.user_bias = {u: 0.0 for u in user_ids}\n        self.item_bias = {i: 0.0 for i in item_ids}\n\n        k = self.params.n_factors\n\n        self.P = {u: 0.1 * rng.standard_normal(k) for u in user_ids}\n        self.Q = {i: 0.1 * rng.standard_normal(k) for i in item_ids}\n\n        lr = self.params.lr\n        reg = self.params.reg\n\n        user_arr = df_ratings[\"userId\"].to_numpy()\n        item_arr = df_ratings[\"movieId\"].to_numpy()\n        rating_arr = df_ratings[\"rating\"].to_numpy()\n        n = len(df_ratings)\n\n        for epoch in range(self.params.n_epochs):\n            idx = rng.permutation(n)\n            se = 0.0\n\n            for t in idx:\n                u = int(user_arr[t])\n                i = int(item_arr[t])\n                r_ui = float(rating_arr[t])\n\n                bu = self.user_bias[u]\n                bi = self.item_bias[i]\n                pu = self.P[u]\n                qi = self.Q[i]\n\n                pred = self.mu + bu + bi + float(np.dot(pu, qi))\n                err = r_ui - pred\n\n                se += err * err\n\n                # Bias updates\n                self.user_bias[u] += lr * (err - reg * bu)\n                self.item_bias[i] += lr * (err - reg * bi)\n\n                # Latent factors updates\n                pu_new = pu + lr * (err * qi - reg * pu)\n                qi_new = qi + lr * (err * pu - reg * qi)\n\n                self.P[u] = pu_new\n                self.Q[i] = qi_new\n\n            rmse_epoch = float(np.sqrt(se / n))\n            print(f\"Epoch {epoch+1}/{self.params.n_epochs} - train RMSE: {rmse_epoch:.4f}\")\n\n    def predict_single(self, user_id: int, movie_id: int, default: float | None = None) -> float:\n        \"\"\"Predict rating for a single user\u2013movie pair.\n\n        Args:\n            user_id: User identifier.\n            movie_id: Movie identifier.\n            default: Fallback rating when user/movie unseen.\n\n        Returns:\n            Predicted rating.\n        \"\"\"\n        if self.mu is None:\n            raise RuntimeError(\"Model must be fitted before prediction.\")\n\n        if default is None:\n            default = self.mu\n\n        bu = self.user_bias.get(user_id)\n        bi = self.item_bias.get(movie_id)\n        pu = self.P.get(user_id)\n        qi = self.Q.get(movie_id)\n\n        if bu is None or bi is None or pu is None or qi is None:\n            return float(default)\n\n        return float(self.mu + bu + bi + float(np.dot(pu, qi)))\n\n    def predict_df(self, df_pairs: pd.DataFrame) -> np.ndarray:\n        \"\"\"Predict ratings for a set of (userId, movieId) pairs.\n\n        Args:\n            df_pairs: DataFrame with `userId` and `movieId`.\n\n        Returns:\n            Numpy array with predicted ratings.\n        \"\"\"\n        preds: List[float] = []\n        for row in df_pairs.itertuples(index=False):\n            preds.append(self.predict_single(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nmf_params = MFParams(\n    n_factors=40,\n    n_epochs=20,\n    lr=0.01,\n    reg=0.05,\n)\n\nmf_model = MatrixFactorization(params=mf_params, random_state=RANDOM_STATE)\n\nmf_model.fit(train_df)\n\n# Evaluate on full test set\ny_pred_mf = mf_model.predict_df(test_df)\nrmse_mf = rmse(test_df[\"rating\"].to_numpy(), y_pred_mf)\nprint(f\"Matrix factorization RMSE (test): {rmse_mf:.4f}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Visual: predicted vs true ratings\n\nplt.scatter(test_df[\"rating\"], y_pred_mf, alpha=0.2)\nplt.xlabel(\"True rating\")\nplt.ylabel(\"Predicted rating (MF)\")\nplt.title(\"Matrix factorization \u2013 true vs predicted\")\nplt.plot([0.5, 5], [0.5, 5], linestyle=\"--\", color=\"black\")\nplt.xlim(0.5, 5.0)\nplt.ylim(0.5, 5.0)\nplt.show()\n\n# Error distribution\nerrors = test_df[\"rating\"].to_numpy() - y_pred_mf\nsns.histplot(errors, bins=30)\nplt.title(\"Prediction error distribution (MF)\")\nplt.xlabel(\"True - predicted\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We now compare **all main models** on RMSE: baselines, item-based CF\n(sampled), and matrix factorization.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "rmse_summary = pd.DataFrame(\n    {\n        \"model\": [\n            \"Global mean\",\n            \"Movie mean\",\n            \"User+movie bias\",\n            \"Item-based CF (sample)\",\n            \"Matrix factorization\",\n        ],\n        \"rmse\": [\n            rmse_global,\n            rmse_movie,\n            rmse_bias,\n            rmse_item_cf,\n            rmse_mf,\n        ],\n    }\n)\n\nsns.barplot(data=rmse_summary, x=\"model\", y=\"rmse\")\nplt.xticks(rotation=20)\nplt.title(\"Model comparison \u2013 RMSE\")\nplt.ylabel(\"RMSE (lower is better)\")\nplt.show()\n\nrmse_summary.sort_values(\"rmse\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "RMSE gives a sense of **absolute rating prediction quality**, but in\nrecommenders we often care more about **ranking**: which items are in\nthe top\u2013N list.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Ranking metrics: Precision@K, Recall@K, Hit-rate\n\nWe evaluate the MF model as a **ranker**:\n\nFor each user in the test set:\n\n1. Treat items with rating \u2265 4.0 as **relevant**.\n2. Generate a top\u2013K list of recommendations among unseen items.\n3. Compute:\n   - Hit-rate: fraction of users with at least one relevant item in top\u2013K.\n   - Precision@K: average proportion of recommended items that are relevant.\n   - Recall@K: average proportion of relevant items that appear in top\u2013K.\n\nWe do this on a subset of users for speed.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def get_user_seen_movies(df_ratings: pd.DataFrame, user_id: int) -> Set[int]:\n    \"\"\"Return set of movieIds rated by a user.\n\n    Args:\n        df_ratings: Ratings DataFrame.\n        user_id: User identifier.\n\n    Returns:\n        Set of movieIds.\n    \"\"\"\n    return set(df_ratings.loc[df_ratings[\"userId\"] == user_id, \"movieId\"].unique())\n\n\ndef get_user_relevant_movies(df_ratings: pd.DataFrame, user_id: int, threshold: float = 4.0) -> Set[int]:\n    \"\"\"Return set of relevant (liked) movieIds for user in a DataFrame.\n\n    Args:\n        df_ratings: Ratings DataFrame.\n        user_id: User identifier.\n        threshold: Rating threshold for relevance.\n\n    Returns:\n        Set of relevant movieIds.\n    \"\"\"\n    mask = (df_ratings[\"userId\"] == user_id) & (df_ratings[\"rating\"] >= threshold)\n    return set(df_ratings.loc[mask, \"movieId\"].unique())\n\n\ndef recommend_top_n(\n    user_id: int,\n    mf_model: MatrixFactorization,\n    all_movie_ids: Iterable[int],\n    train_ratings: pd.DataFrame,\n    n: int = 10,\n) -> pd.DataFrame:\n    \"\"\"Generate top\u2013N recommendations for a user via MF.\n\n    Args:\n        user_id: User identifier.\n        mf_model: Fitted MatrixFactorization model.\n        all_movie_ids: Iterable of candidate movieIds.\n        train_ratings: Training ratings DataFrame.\n        n: Number of recommendations.\n\n    Returns:\n        DataFrame with movieId, predicted_rating.\n    \"\"\"\n    seen = get_user_seen_movies(train_ratings, user_id)\n\n    candidates: List[int] = [mid for mid in all_movie_ids if mid not in seen]\n    if not candidates:\n        raise ValueError(f\"User {user_id} has rated all movies; no candidates.\")\n\n    preds: List[Tuple[int, float]] = []\n    for mid in candidates:\n        score = mf_model.predict_single(user_id, mid)\n        preds.append((mid, score))\n\n    preds_sorted = sorted(preds, key=lambda x: x[1], reverse=True)[:n]\n    return pd.DataFrame(preds_sorted, columns=[\"movieId\", \"predicted_rating\"])\n\n\nall_movie_ids: List[int] = sorted(ratings_raw[\"movieId\"].unique())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def evaluate_ranking_mf(\n    mf_model: MatrixFactorization,\n    train_df: pd.DataFrame,\n    test_df: pd.DataFrame,\n    all_movie_ids: List[int],\n    k: int = 10,\n    rel_threshold: float = 4.0,\n    max_users: int = 200,\n) -> Dict[str, float]:\n    \"\"\"Evaluate MF ranking via hit-rate, precision@K, recall@K.\n\n    Args:\n        mf_model: Fitted MatrixFactorization.\n        train_df: Train ratings.\n        test_df: Test ratings.\n        all_movie_ids: List of movieIds.\n        k: Cutoff for top-K.\n        rel_threshold: Relevance rating threshold.\n        max_users: Max number of users to sample for evaluation.\n\n    Returns:\n        Dictionary with metrics.\n    \"\"\"\n    users_in_test = test_df[\"userId\"].unique()\n    rng = np.random.default_rng(RANDOM_STATE)\n    if len(users_in_test) > max_users:\n        eval_users = rng.choice(users_in_test, size=max_users, replace=False)\n    else:\n        eval_users = users_in_test\n\n    hits = 0\n    total_users_with_relevant = 0\n    sum_precision = 0.0\n    sum_recall = 0.0\n    n_eval_users = 0\n\n    for u in eval_users:\n        relevant = get_user_relevant_movies(test_df, u, threshold=rel_threshold)\n        if not relevant:\n            # Skip users with no relevant items in test\n            continue\n\n        n_eval_users += 1\n        total_users_with_relevant += 1\n\n        recs_df = recommend_top_n(\n            user_id=int(u),\n            mf_model=mf_model,\n            all_movie_ids=all_movie_ids,\n            train_ratings=train_df,\n            n=k,\n        )\n\n        recs = list(recs_df[\"movieId\"])\n        recs_set = set(recs)\n\n        n_relevant_in_recs = len(recs_set & relevant)\n\n        if n_relevant_in_recs > 0:\n            hits += 1\n\n        precision_u = n_relevant_in_recs / k\n        recall_u = n_relevant_in_recs / len(relevant)\n\n        sum_precision += precision_u\n        sum_recall += recall_u\n\n    if n_eval_users == 0:\n        raise ValueError(\"No users with relevant items in test for evaluation.\")\n\n    hit_rate = hits / total_users_with_relevant\n    precision_at_k = sum_precision / n_eval_users\n    recall_at_k = sum_recall / n_eval_users\n\n    return {\n        \"hit_rate\": hit_rate,\n        \"precision_at_k\": precision_at_k,\n        \"recall_at_k\": recall_at_k,\n        \"n_eval_users\": float(n_eval_users),\n    }\n\n\nranking_metrics_k10 = evaluate_ranking_mf(\n    mf_model=mf_model,\n    train_df=train_df,\n    test_df=test_df,\n    all_movie_ids=all_movie_ids,\n    k=10,\n    rel_threshold=4.0,\n    max_users=200,\n)\n\nranking_metrics_k10\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Visualise ranking metrics\n\nmetrics_df = pd.DataFrame(\n    {\n        \"metric\": [\"hit_rate\", \"precision_at_k\", \"recall_at_k\"],\n        \"value\": [\n            ranking_metrics_k10[\"hit_rate\"],\n            ranking_metrics_k10[\"precision_at_k\"],\n            ranking_metrics_k10[\"recall_at_k\"],\n        ],\n    }\n)\n\nsns.barplot(data=metrics_df, x=\"metric\", y=\"value\")\nplt.ylim(0, 1)\nplt.title(\"MF ranking performance (K=10)\")\nplt.ylabel(\"Score\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "These ranking metrics better reflect how a user experiences the system:\n**are good movies appearing near the top of their list?**\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Example recommendations with titles\n\nWe can inspect recommendations for a random user to see if they look\nreasonable.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "sample_user_id: int = int(train_df[\"userId\"].sample(1, random_state=RANDOM_STATE).iloc[0])\nprint(\"Sample user:\", sample_user_id)\n\nuser_seen = get_user_seen_movies(train_df, sample_user_id)\nprint(\"Movies rated in train:\", len(user_seen))\n\nrecs_df = recommend_top_n(\n    user_id=sample_user_id,\n    mf_model=mf_model,\n    all_movie_ids=all_movie_ids,\n    train_ratings=train_df,\n    n=10,\n)\n\nif MOVIES_PATH.exists():\n    movies_df = pd.read_csv(MOVIES_PATH)\n    recs_with_titles = recs_df.merge(movies_df, on=\"movieId\", how=\"left\")\n    recs_with_titles[[\"movieId\", \"title\", \"predicted_rating\"]]\nelse:\n    print(\"movies.csv not found; showing IDs only.\")\n    recs_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You can manually inspect whether these recommended films match the\nuser's taste, based on the movies they rated highly in the training set.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Modern tools and extensions for recommender systems\n\nIn this notebook, we implemented core models **manually** to keep things\ntransparent. In practice, many teams rely on specialised libraries and\nnewer techniques.\n\n### 10.1 Python libraries commonly used\n\nYou can explore these outside this notebook (code sketch below):\n\n- **`implicit`** \u2013 matrix factorization and nearest-neighbour models for\n  implicit feedback (clicks, views, purchases).\n- **`lightfm`** \u2013 hybrid (content + collaborative) models with various\n  losses (BPR, WARP) for ranking.\n- **`scikit-surprise`** \u2013 classic algorithms (SVD, KNNBaseline, etc.) with\n  built-in evaluation tools.\n- **`RecBole`, `Spotlight`** \u2013 more research-oriented frameworks for\n  deep learning recommenders.\n\nExample (for your environment, not run here):\n\n```python\n# !pip install scikit-surprise\nfrom surprise import Dataset, Reader, SVD\nfrom surprise.model_selection import train_test_split, accuracy\n\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(ratings_raw[[\"userId\", \"movieId\", \"rating\"]], reader)\ntrainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n\nalgo = SVD(n_factors=50, reg_all=0.02, lr_all=0.005)\nalgo.fit(trainset)\n\npredictions = algo.test(testset)\nrmse_surprise = accuracy.rmse(predictions)\n```\n\n### 10.2 Modelling directions\n\nModern recommender systems often add:\n\n- **Implicit feedback**:\n  - Treat interactions as positive events (view, click, watch) instead of\n    relying only on explicit ratings.\n  - Use models like BPR, WARP, or implicit MF.\n- **Sequence-aware / session-based models**:\n  - Use RNNs, Transformers or attention to model user sessions as\n    sequences of events.\n- **Context-aware models**:\n  - Include time, device, location, or other context as features.\n- **Bandits / online learning**:\n  - Contextual bandits for balancing exploration and exploitation.\n\n### 10.3 Production considerations\n\nFor deployment, you typically add:\n\n- **Feature store** for user/item features.\n- **Candidate generation** + **ranking** stages.\n- **Real-time serving API** (e.g. via FastAPI).\n- **Monitoring** and evaluation pipelines (A/B testing, offline metrics).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11. Summary\n\nIn this extended recommender project we:\n\n1. Performed EDA on MovieLens with several **visuals**.\n2. Built and compared **baseline models**:\n   - Global mean, movie mean, user+movie bias.\n3. Constructed a **user\u2013item rating matrix** and visualised sparsity.\n4. Implemented **item-based collaborative filtering**.\n5. Implemented **matrix factorization** with SGD and biases.\n6. Evaluated with **RMSE** and several **ranking metrics**.\n7. Generated **top\u2013N recommendations** and, when titles are available,\n   printed them for inspection.\n8. Discussed **modern tools and research directions** in recommender systems.\n\nThis notebook should give you a complete, end-to-end template for\nrecommendation projects, while still being compact enough to adapt to your\nown datasets or to plug into a larger system.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}