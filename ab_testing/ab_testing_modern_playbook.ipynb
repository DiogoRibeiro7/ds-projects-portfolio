{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5572e361",
   "metadata": {},
   "source": [
    "\n",
    "# Modern A/B Testing Playbook — From Classical to Advanced Methods\n",
    "\n",
    "This notebook is designed as a **teaching + reference notebook** for A/B testing:\n",
    "from basic statistical tests to more modern techniques used in large-scale experimentation.\n",
    "\n",
    "We cover:\n",
    "\n",
    "1. **Core frequentist tools**\n",
    "   - Proportion z-test, t-test on means, chi-square.\n",
    "   - Confidence intervals, effect sizes, MDE / power.\n",
    "2. **Permutation / randomization tests** for non-parametric inference.\n",
    "3. **Regression-based A/B analysis**\n",
    "   - Logistic regression, linear regression, covariate adjustment.\n",
    "4. **Variance reduction**\n",
    "   - CUPED (pre-period covariate) and regression adjustment.\n",
    "5. **Sequential testing and peeking**\n",
    "   - Type I error inflation.\n",
    "   - Simple alpha-spending illustration.\n",
    "6. **Bayesian A/B testing**\n",
    "   - Beta–Binomial model for conversion.\n",
    "   - Posterior decision metrics (P(B > A), expected loss).\n",
    "7. **Multi-armed bandits (brief)**\n",
    "   - Thompson sampling for Bernoulli arms, regret curves (simulation).\n",
    "8. **Causal / observational setting**\n",
    "   - IPW and doubly-robust (DR) estimation under confounding.\n",
    "\n",
    "The notebook uses **simulated data** for most examples so that all sections are reproducible.\n",
    "You can plug in your own experimental dataset by mapping it to the same column structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4791c",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm  # type: ignore\n",
    "except Exception as e:  # pragma: no cover\n",
    "    sm = None\n",
    "    print(\"statsmodels not available; some regression sections will be skipped:\", e)\n",
    "\n",
    "try:\n",
    "    from sklearn.linear_model import LogisticRegression  # type: ignore\n",
    "except Exception as e:  # pragma: no cover\n",
    "    LogisticRegression = None\n",
    "    print(\"scikit-learn not available; logistic sections will be skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f19963",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Simulated experimental dataset\n",
    "\n",
    "We generate a simple, but realistic, experiment:\n",
    "\n",
    "- Binary outcome: `converted` (0/1).\n",
    "- Continuous metric: `revenue` (0 if not converted).\n",
    "- Treatment: `group ∈ {control, treatment}`.\n",
    "- Covariate: `pre_activity` (pre-period proxy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_experiment(\n",
    "    n: int = 20_000,\n",
    "    p_control: float = 0.10,\n",
    "    lift_treatment: float = 0.02,\n",
    "    mean_revenue: float = 100.0,\n",
    "    revenue_sd: float = 40.0,\n",
    "    seed: int | None = 123,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Simulate a simple A/B test dataset.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    user_id = np.arange(n)\n",
    "    group_flag = rng.binomial(1, 0.5, size=n)\n",
    "    group = np.where(group_flag == 0, \"control\", \"treatment\")\n",
    "\n",
    "    p_treat = p_control + lift_treatment\n",
    "    p = np.where(group_flag == 0, p_control, p_treat)\n",
    "    converted = rng.binomial(1, p)\n",
    "\n",
    "    rev = rng.normal(loc=mean_revenue, scale=revenue_sd, size=n)\n",
    "    rev = np.where(converted == 1, rev, 0.0)\n",
    "\n",
    "    pre_activity = rng.normal(loc=0.0, scale=1.0, size=n) + converted * 0.7\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"user_id\": user_id,\n",
    "            \"group\": group,\n",
    "            \"converted\": converted.astype(int),\n",
    "            \"revenue\": rev.astype(float),\n",
    "            \"pre_activity\": pre_activity.astype(float),\n",
    "        }\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = simulate_experiment()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9003f13",
   "metadata": {},
   "source": [
    "## 2) Core frequentist tools — conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class PropSummary:\n",
    "    \"\"\"Summary of a Bernoulli proportion.\"\"\"\n",
    "    p: float\n",
    "    n: int\n",
    "    x: int\n",
    "\n",
    "\n",
    "def summarize_prop(x: int, n: int) -> PropSummary:\n",
    "    \"\"\"Validate and summarize a proportion sample.\"\"\"\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive.\")\n",
    "    if not (0 <= x <= n):\n",
    "        raise ValueError(\"x must satisfy 0 <= x <= n.\")\n",
    "    return PropSummary(p=x / n, n=n, x=x)\n",
    "\n",
    "\n",
    "def invPhi(u: float) -> float:\n",
    "    \"\"\"Inverse standard normal CDF using erfcinv.\"\"\"\n",
    "    if not 0.0 < u < 1.0:\n",
    "        raise ValueError(\"u must be in (0,1).\")\n",
    "    return math.sqrt(2.0) * math.erfcinv(2.0 * (1.0 - u))\n",
    "\n",
    "\n",
    "def two_prop_ztest(\n",
    "    x1: int,\n",
    "    n1: int,\n",
    "    x2: int,\n",
    "    n2: int,\n",
    "    two_sided: bool = True,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Two-sample z-test for proportions with pooled variance.\"\"\"\n",
    "    s1, s2 = summarize_prop(x1, n1), summarize_prop(x2, n2)\n",
    "    p_pool = (s1.x + s2.x) / (s1.n + s2.n)\n",
    "    se = math.sqrt(p_pool * (1.0 - p_pool) * (1.0 / s1.n + 1.0 / s2.n))\n",
    "    if se == 0.0:\n",
    "        raise ZeroDivisionError(\"Standard error is zero; check inputs.\")\n",
    "    z = (s2.p - s1.p) / se\n",
    "    if two_sided:\n",
    "        p = 2.0 * (1.0 - 0.5 * (1.0 + math.erf(abs(z) / math.sqrt(2.0))))\n",
    "    else:\n",
    "        p = 1.0 - 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
    "    return float(z), float(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6041c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conv_by_group = (\n",
    "    df.groupby(\"group\")[\"converted\"]\n",
    "      .agg([\"sum\", \"count\", \"mean\"])\n",
    "      .rename(columns={\"sum\": \"x\", \"count\": \"n\", \"mean\": \"rate\"})\n",
    ")\n",
    "conv_by_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dbe6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xA = int(conv_by_group.loc[\"control\", \"x\"])\n",
    "nA = int(conv_by_group.loc[\"control\", \"n\"])\n",
    "xB = int(conv_by_group.loc[\"treatment\", \"x\"])\n",
    "nB = int(conv_by_group.loc[\"treatment\", \"n\"])\n",
    "\n",
    "sA = summarize_prop(xA, nA)\n",
    "sB = summarize_prop(xB, nB)\n",
    "\n",
    "z_conv, p_conv = two_prop_ztest(xA, nA, xB, nB, two_sided=True)\n",
    "\n",
    "alpha = 0.05\n",
    "z_alpha = abs(invPhi(1.0 - alpha / 2.0))\n",
    "diff_conv = sB.p - sA.p\n",
    "se_diff_conv = math.sqrt(\n",
    "    (sA.p * (1.0 - sA.p)) / sA.n + (sB.p * (1.0 - sB.p)) / sB.n\n",
    ")\n",
    "ci_lo_conv = diff_conv - z_alpha * se_diff_conv\n",
    "ci_hi_conv = diff_conv + z_alpha * se_diff_conv\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"arm\": [\"control\", \"treatment\"],\n",
    "        \"n\": [sA.n, sB.n],\n",
    "        \"x\": [sA.x, sB.x],\n",
    "        \"rate\": [sA.p, sB.p],\n",
    "        \"diff_B_minus_A\": [diff_conv, diff_conv],\n",
    "        \"diff_CI95_lo\": [ci_lo_conv, ci_lo_conv],\n",
    "        \"diff_CI95_hi\": [ci_hi_conv, ci_hi_conv],\n",
    "        \"z_stat\": [z_conv, z_conv],\n",
    "        \"p_value\": [p_conv, p_conv],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa47974",
   "metadata": {},
   "source": [
    "## 3) t-test on revenue per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab865c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def welch_ttest(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Welch t-test for difference in means (two-sided).\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n1, n2 = x.size, y.size\n",
    "    if n1 < 2 or n2 < 2:\n",
    "        raise ValueError(\"Need at least 2 observations per group.\")\n",
    "    m1, m2 = float(x.mean()), float(y.mean())\n",
    "    v1, v2 = float(x.var(ddof=1)), float(y.var(ddof=1))\n",
    "    se = math.sqrt(v1 / n1 + v2 / n2)\n",
    "    if se == 0.0:\n",
    "        raise ZeroDivisionError(\"Standard error is zero; check variance.\")\n",
    "    t_stat = (m2 - m1) / se\n",
    "    p = 2.0 * (1.0 - 0.5 * (1.0 + math.erf(abs(t_stat) / math.sqrt(2.0))))\n",
    "    return float(t_stat), float(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e687b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rev_control = df.loc[df[\"group\"] == \"control\", \"revenue\"].to_numpy()\n",
    "rev_treat = df.loc[df[\"group\"] == \"treatment\", \"revenue\"].to_numpy()\n",
    "\n",
    "t_rev, p_rev = welch_ttest(rev_control, rev_treat)\n",
    "\n",
    "mean_rev_A = float(rev_control.mean())\n",
    "mean_rev_B = float(rev_treat.mean())\n",
    "diff_rev = mean_rev_B - mean_rev_A\n",
    "\n",
    "varA = float(rev_control.var(ddof=1))\n",
    "varB = float(rev_treat.var(ddof=1))\n",
    "se_diff_rev = math.sqrt(varA / rev_control.size + varB / rev_treat.size)\n",
    "ci_lo_rev = diff_rev - z_alpha * se_diff_rev\n",
    "ci_hi_rev = diff_rev + z_alpha * se_diff_rev\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"arm\": [\"control\", \"treatment\"],\n",
    "        \"mean_revenue\": [mean_rev_A, mean_rev_B],\n",
    "        \"diff_B_minus_A\": [diff_rev, diff_rev],\n",
    "        \"diff_CI95_lo\": [ci_lo_rev, ci_lo_rev],\n",
    "        \"diff_CI95_hi\": [ci_hi_rev, ci_hi_rev],\n",
    "        \"t_stat\": [t_rev, t_rev],\n",
    "        \"p_value\": [p_rev, p_rev],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45579120",
   "metadata": {},
   "source": [
    "## 4) Permutation test for conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def permutation_test_diff_mean(\n",
    "    y: np.ndarray,\n",
    "    group_labels: np.ndarray,\n",
    "    n_perm: int = 5000,\n",
    "    seed: int | None = 1,\n",
    ") -> float:\n",
    "    \"\"\"Permutation test for difference in means between two groups.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    g = np.asarray(group_labels)\n",
    "    groups = np.unique(g)\n",
    "    if groups.size != 2:\n",
    "        raise ValueError(\"Need exactly two groups.\")\n",
    "    def diff_means(vals: np.ndarray, labels: np.ndarray) -> float:\n",
    "        m0 = float(vals[labels == groups[0]].mean())\n",
    "        m1 = float(vals[labels == groups[1]].mean())\n",
    "        return m1 - m0\n",
    "    observed = diff_means(y, g)\n",
    "    diffs = np.empty(n_perm, dtype=float)\n",
    "    for i in range(n_perm):\n",
    "        perm_labels = rng.permutation(g)\n",
    "        diffs[i] = diff_means(y, perm_labels)\n",
    "    p_perm = float((np.abs(diffs) >= abs(observed)).mean())\n",
    "    plt.figure()\n",
    "    plt.hist(diffs, bins=40, density=True)\n",
    "    plt.axvline(observed, linestyle=\"--\", label=\"observed\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Permutation null distribution\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return p_perm\n",
    "\n",
    "\n",
    "p_perm_conv = permutation_test_diff_mean(\n",
    "    df[\"converted\"].to_numpy(dtype=float),\n",
    "    df[\"group\"].to_numpy(),\n",
    "    n_perm=3000,\n",
    "    seed=123,\n",
    ")\n",
    "p_perm_conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7e844",
   "metadata": {},
   "source": [
    "## 5) Variance reduction — CUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cuped_adjust(y: np.ndarray, x: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Apply CUPED adjustment Y* = Y - θ (X - mean(X)).\"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if y.shape != x.shape:\n",
    "        raise ValueError(\"y and x must have same shape.\")\n",
    "    vx = float(np.var(x, ddof=1))\n",
    "    if vx == 0.0:\n",
    "        return y.copy(), 0.0\n",
    "    cov_yx = float(np.cov(y, x, ddof=1)[0, 1])\n",
    "    theta = cov_yx / vx\n",
    "    x_centered = x - float(np.mean(x))\n",
    "    y_adj = y - theta * x_centered\n",
    "    return y_adj, theta\n",
    "\n",
    "\n",
    "y_rev = df[\"revenue\"].to_numpy(dtype=float)\n",
    "x_pre = df[\"pre_activity\"].to_numpy(dtype=float)\n",
    "y_rev_cuped, theta_hat = cuped_adjust(y_rev, x_pre)\n",
    "df[\"revenue_cuped\"] = y_rev_cuped\n",
    "theta_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2410d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cuped_summary = (\n",
    "    df.assign(revenue_raw=df[\"revenue\"].astype(float))\n",
    "      .groupby(\"group\")[[\"revenue_raw\", \"revenue_cuped\"]]\n",
    "      .agg([\"mean\", \"var\", \"count\"])\n",
    ")\n",
    "cuped_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4316894",
   "metadata": {},
   "source": [
    "## 6) Bayesian A/B — Beta–Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_posterior_lift(\n",
    "    xA: int,\n",
    "    nA: int,\n",
    "    xB: int,\n",
    "    nB: int,\n",
    "    alpha0: float = 1.0,\n",
    "    beta0: float = 1.0,\n",
    "    n_draws: int = 50_000,\n",
    "    seed: int | None = 1,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Draw from the posterior of conversion rates and their difference.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    alphaA = alpha0 + xA\n",
    "    betaA = beta0 + nA - xA\n",
    "    alphaB = alpha0 + xB\n",
    "    betaB = beta0 + nB - xB\n",
    "    pA = rng.beta(alphaA, betaA, size=n_draws)\n",
    "    pB = rng.beta(alphaB, betaB, size=n_draws)\n",
    "    lift = pB - pA\n",
    "    return pd.DataFrame({\"pA\": pA, \"pB\": pB, \"lift\": lift})\n",
    "\n",
    "\n",
    "post = sample_posterior_lift(xA, nA, xB, nB, n_draws=50_000, seed=2025)\n",
    "prob_better = float((post[\"lift\"] > 0).mean())\n",
    "ci_lo, ci_hi = np.quantile(post[\"lift\"], [0.025, 0.975])\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(post[\"lift\"], bins=50, density=True)\n",
    "plt.axvline(0.0, linestyle=\"--\")\n",
    "plt.title(\"Posterior lift distribution (p_treat - p_control)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "{\n",
    "    \"posterior_prob_treatment_better\": prob_better,\n",
    "    \"lift_cred_int_95\": (ci_lo, ci_hi),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7636b86",
   "metadata": {},
   "source": [
    "## 7) Sequential testing and peeking (A/A simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8fea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_null_stream(\n",
    "    n: int = 10_000,\n",
    "    p: float = 0.10,\n",
    "    seed: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Simulate an A/A test (no true effect) with Bernoulli outcomes.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    group_flag = rng.binomial(1, 0.5, size=n)\n",
    "    group = np.where(group_flag == 0, \"control\", \"treatment\")\n",
    "    y = rng.binomial(1, p, size=n)\n",
    "    return pd.DataFrame({\"group\": group, \"converted\": y})\n",
    "\n",
    "\n",
    "def peek_pvalues(df_stream: pd.DataFrame, look_step: int = 500) -> Tuple[list[int], list[float]]:\n",
    "    \"\"\"Compute p-values over time by looking every 'look_step' users.\"\"\"\n",
    "    look_indices: list[int] = []\n",
    "    p_values: list[float] = []\n",
    "    for n_curr in range(look_step, len(df_stream) + 1, look_step):\n",
    "        sub = df_stream.iloc[:n_curr]\n",
    "        tbl = (\n",
    "            sub.groupby(\"group\")[\"converted\"]\n",
    "               .agg([\"sum\", \"count\"])\n",
    "               .rename(columns={\"sum\": \"x\", \"count\": \"n\"})\n",
    "        )\n",
    "        xA, nA = int(tbl.loc[\"control\", \"x\"]), int(tbl.loc[\"control\", \"n\"])\n",
    "        xB, nB = int(tbl.loc[\"treatment\", \"x\"]), int(tbl.loc[\"treatment\", \"n\"])\n",
    "        _, p = two_prop_ztest(xA, nA, xB, nB, two_sided=True)\n",
    "        look_indices.append(n_curr)\n",
    "        p_values.append(p)\n",
    "    return look_indices, p_values\n",
    "\n",
    "\n",
    "def simulate_type1_inflation(\n",
    "    n_experiments: int = 200,\n",
    "    n: int = 8_000,\n",
    "    p: float = 0.10,\n",
    "    look_step: int = 400,\n",
    "    alpha: float = 0.05,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Compare Type I error under fixed-horizon vs naive sequential peeking.\"\"\"\n",
    "    rng = np.random.default_rng(2025)\n",
    "    reject_fixed = 0\n",
    "    reject_seq = 0\n",
    "    for _ in range(n_experiments):\n",
    "        df_stream = simulate_null_stream(n=n, p=p, seed=int(rng.integers(0, 10_000_000)))\n",
    "        tbl = (\n",
    "            df_stream.groupby(\"group\")[\"converted\"]\n",
    "                     .agg([\"sum\", \"count\"])\n",
    "                     .rename(columns={\"sum\": \"x\", \"count\": \"n\"})\n",
    "        )\n",
    "        xA, nA = int(tbl.loc[\"control\", \"x\"]), int(tbl.loc[\"control\", \"n\"])\n",
    "        xB, nB = int(tbl.loc[\"treatment\", \"x\"]), int(tbl.loc[\"treatment\", \"n\"])\n",
    "        _, p_fix = two_prop_ztest(xA, nA, xB, nB, two_sided=True)\n",
    "        if p_fix < alpha:\n",
    "            reject_fixed += 1\n",
    "        looks, pvals = peek_pvalues(df_stream, look_step=look_step)\n",
    "        if any(pv < alpha for pv in pvals):\n",
    "            reject_seq += 1\n",
    "    return reject_fixed / n_experiments, reject_seq / n_experiments\n",
    "\n",
    "\n",
    "fixed_alpha, seq_alpha = simulate_type1_inflation()\n",
    "{\"fixed_horizon_alpha_hat\": fixed_alpha, \"naive_seq_alpha_hat\": seq_alpha}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f225c",
   "metadata": {},
   "source": [
    "## 8) Causal / observational setting — IPW and DR (simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39954f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LogisticRegression is None:\n",
    "    print(\"sklearn not available; skipping IPW/DR demo.\")\n",
    "else:\n",
    "    def sigmoid(z: np.ndarray | float) -> np.ndarray | float:\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def simulate_confounded(\n",
    "        n: int = 20_000,\n",
    "        seed: int | None = 123,\n",
    "    ) -> pd.DataFrame:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        d = 4\n",
    "        X = rng.normal(size=(n, d))\n",
    "        w_e = np.array([0.8, -0.6, 0.4, 0.2])\n",
    "        bias_e = -0.1\n",
    "        e = sigmoid(bias_e + X @ w_e)\n",
    "        T = rng.binomial(1, e)\n",
    "        w_y = np.array([0.5, 0.3, -0.2, 0.1])\n",
    "        bias_y = -1.2\n",
    "        tau_true = 0.3\n",
    "        lin = bias_y + X @ w_y + tau_true * T\n",
    "        p = sigmoid(lin)\n",
    "        Y = rng.binomial(1, p)\n",
    "        cols = {f\"x{j+1}\": X[:, j] for j in range(d)}\n",
    "        df_sim = pd.DataFrame(cols)\n",
    "        df_sim[\"T\"] = T\n",
    "        df_sim[\"Y\"] = Y\n",
    "        df_sim[\"e_true\"] = e\n",
    "        return df_sim\n",
    "\n",
    "    def naive_ate(df_sim: pd.DataFrame) -> float:\n",
    "        m1 = float(df_sim.loc[df_sim[\"T\"] == 1, \"Y\"].mean())\n",
    "        m0 = float(df_sim.loc[df_sim[\"T\"] == 0, \"Y\"].mean())\n",
    "        return m1 - m0\n",
    "\n",
    "    def ipw_ate(df_sim: pd.DataFrame, e_col: str = \"e_true\") -> float:\n",
    "        t = df_sim[\"T\"].to_numpy()\n",
    "        y = df_sim[\"Y\"].to_numpy()\n",
    "        e = np.clip(df_sim[e_col].to_numpy(), 1e-6, 1.0 - 1e-6)\n",
    "        w1 = t / e\n",
    "        w0 = (1.0 - t) / (1.0 - e)\n",
    "        p1_hat = (w1 * y).sum() / max(w1.sum(), 1e-12)\n",
    "        p0_hat = (w0 * y).sum() / max(w0.sum(), 1e-12)\n",
    "        return float(p1_hat - p0_hat)\n",
    "\n",
    "    def dr_logistic_ate(df_sim: pd.DataFrame, e_col: str = \"e_true\") -> float:\n",
    "        feature_cols = [c for c in df_sim.columns if c.startswith(\"x\")]\n",
    "        X = df_sim[feature_cols].to_numpy()\n",
    "        t = df_sim[\"T\"].to_numpy()\n",
    "        y = df_sim[\"Y\"].to_numpy()\n",
    "        e = np.clip(df_sim[e_col].to_numpy(), 1e-6, 1.0 - 1e-6)\n",
    "        X1 = X[t == 1]\n",
    "        y1 = y[t == 1]\n",
    "        X0 = X[t == 0]\n",
    "        y0 = y[t == 0]\n",
    "        mdl1 = LogisticRegression(max_iter=1000).fit(X1, y1)\n",
    "        mdl0 = LogisticRegression(max_iter=1000).fit(X0, y0)\n",
    "        m1_hat = mdl1.predict_proba(X)[:, 1]\n",
    "        m0_hat = mdl0.predict_proba(X)[:, 1]\n",
    "        term = (m1_hat - m0_hat) + (t * (y - m1_hat) / e) - ((1.0 - t) * (y - m0_hat) / (1.0 - e))\n",
    "        return float(np.mean(term))\n",
    "\n",
    "    df_sim = simulate_confounded(n=30_000, seed=2025)\n",
    "    ate_naive = naive_ate(df_sim)\n",
    "    ate_ipw_true = ipw_ate(df_sim, e_col=\"e_true\")\n",
    "    ate_dr = dr_logistic_ate(df_sim, e_col=\"e_true\")\n",
    "    {\"ATE_naive\": ate_naive, \"ATE_IPW_true\": ate_ipw_true, \"ATE_DR_logistic\": ate_dr}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b021e17",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Treatment effect heterogeneity and uplift-style modeling\n",
    "\n",
    "So far we have mostly focused on **average treatment effects** (ATE):\n",
    "\n",
    "- A single number summarising the effect of treatment across all users.\n",
    "\n",
    "In practice, the effect of a new feature or price often **varies by user segment**. For example:\n",
    "\n",
    "- Highly engaged users might respond very differently from cold users.  \n",
    "- Discount-sensitive users might show larger uplift from a promotion.\n",
    "\n",
    "Here we build a simple **heterogeneous treatment effect (HTE)** model using:\n",
    "\n",
    "- A logistic regression for `converted`.  \n",
    "- An interaction between treatment and a covariate (`pre_activity`).  \n",
    "- A derived **uplift curve**: predicted \\(P(Y=1 \\mid T=1, X=x) - P(Y=1 \\mid T=0, X=x)\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if sm is None:\n",
    "    print(\"statsmodels is not available; skipping heterogeneity / uplift section.\")\n",
    "else:\n",
    "    # Prepare data with treatment indicator and interaction\n",
    "    df_het = df.copy()\n",
    "    df_het[\"treat_flag\"] = (df_het[\"group\"] == \"treatment\").astype(int)\n",
    "    df_het[\"interaction\"] = df_het[\"treat_flag\"] * df_het[\"pre_activity\"]\n",
    "\n",
    "    # Design matrix: intercept + main effects + interaction\n",
    "    X = df_het[[\"treat_flag\", \"pre_activity\", \"interaction\"]]\n",
    "    X_sm = sm.add_constant(X)\n",
    "    y = df_het[\"converted\"].to_numpy()\n",
    "\n",
    "    logit_het = sm.Logit(y, X_sm).fit(disp=False)\n",
    "    logit_het.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b357a0",
   "metadata": {},
   "source": [
    "\n",
    "The key coefficient here is the **interaction term** on `treat_flag × pre_activity`:\n",
    "\n",
    "- If it is **positive**, the treatment effect grows as `pre_activity` increases.  \n",
    "- If it is **negative**, the treatment effect shrinks (or even flips) for high `pre_activity`.  \n",
    "\n",
    "Logistic coefficients are on the **log-odds** scale. To make this easier to interpret, we look at\n",
    "predicted **conversion probabilities** and **uplift** as a function of `pre_activity`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103be46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if sm is not None:\n",
    "    # Build a grid of pre-activity values (central 98% range to avoid extreme tails)\n",
    "    pre_min = float(df_het[\"pre_activity\"].quantile(0.01))\n",
    "    pre_max = float(df_het[\"pre_activity\"].quantile(0.99))\n",
    "    grid = np.linspace(pre_min, pre_max, 100)\n",
    "\n",
    "    # Construct design matrices for control vs treatment at each grid point\n",
    "    X_ctrl = pd.DataFrame(\n",
    "        {\n",
    "            \"const\": 1.0,\n",
    "            \"treat_flag\": np.zeros_like(grid),\n",
    "            \"pre_activity\": grid,\n",
    "            \"interaction\": np.zeros_like(grid),  # 0 * pre_activity\n",
    "        }\n",
    "    )\n",
    "    X_treat = pd.DataFrame(\n",
    "        {\n",
    "            \"const\": 1.0,\n",
    "            \"treat_flag\": np.ones_like(grid),\n",
    "            \"pre_activity\": grid,\n",
    "            \"interaction\": grid,  # 1 * pre_activity\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Predicted conversion probabilities\n",
    "    p_ctrl = logit_het.predict(X_ctrl)\n",
    "    p_treat = logit_het.predict(X_treat)\n",
    "    uplift = p_treat - p_ctrl\n",
    "\n",
    "    uplift_df = pd.DataFrame(\n",
    "        {\n",
    "            \"pre_activity\": grid,\n",
    "            \"p_control\": p_ctrl,\n",
    "            \"p_treatment\": p_treat,\n",
    "            \"uplift\": uplift,\n",
    "        }\n",
    "    )\n",
    "    display(uplift_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if sm is not None:\n",
    "    # Plot uplift as a function of pre_activity\n",
    "    plt.figure()\n",
    "    plt.plot(uplift_df[\"pre_activity\"], uplift_df[\"uplift\"])\n",
    "    plt.axhline(0.0, linestyle=\"--\")\n",
    "    plt.xlabel(\"pre_activity\")\n",
    "    plt.ylabel(\"uplift = P(T=1) - P(T=0)\")\n",
    "    plt.title(\"Predicted treatment uplift vs pre-activity\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe94920",
   "metadata": {},
   "source": [
    "\n",
    "This curve is a simple **uplift function**: it tells you how much you expect conversion to change\n",
    "for a user with a given `pre_activity` if they receive treatment instead of control.\n",
    "\n",
    "- Regions where uplift is **large and positive** are good candidates for **targeting** the treatment.  \n",
    "- Regions where uplift is **near zero** or negative suggest that treatment is not helpful (or even harmful).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if sm is not None:\n",
    "    # Derive coarse uplift segments by pre-activity quantiles\n",
    "    df_seg = df_het.copy()\n",
    "    df_seg[\"pre_segment\"] = pd.qcut(df_seg[\"pre_activity\"], 3, labels=[\"low\", \"mid\", \"high\"])\n",
    "\n",
    "    seg_summary = (\n",
    "        df_seg.groupby([\"pre_segment\", \"group\"])[\"converted\"]\n",
    "              .agg([\"mean\", \"count\"])\n",
    "              .rename(columns={\"mean\": \"conversion_rate\"})\n",
    "              .reset_index()\n",
    "    )\n",
    "\n",
    "    # Also compute segment-level uplift\n",
    "    seg_pivot = (\n",
    "        seg_summary.pivot(index=\"pre_segment\", columns=\"group\", values=\"conversion_rate\")\n",
    "    )\n",
    "    seg_pivot[\"uplift_treat_minus_control\"] = (\n",
    "        seg_pivot[\"treatment\"] - seg_pivot[\"control\"]\n",
    "    )\n",
    "    seg_pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aebde5",
   "metadata": {},
   "source": [
    "\n",
    "### How this relates to uplift modeling\n",
    "\n",
    "The model we used is a **single logistic regression** with an interaction term:\n",
    "\n",
    "\\[\n",
    "\\log \\frac{P(Y=1 \\mid T, X)}{1 - P(Y=1 \\mid T, X)} =\n",
    "\\beta_0 + \\beta_1 T + \\beta_2 X + \\beta_3 (T \\cdot X).\n",
    "\\]\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "- One base log-odds curve in \\(X\\) (through \\(\\beta_2\\)).  \n",
    "- A treatment effect that **depends on X** (through \\(\\beta_1 + \\beta_3 X\\)).\n",
    "\n",
    "More advanced uplift approaches extend this idea:\n",
    "\n",
    "- Two-model approach: one model for treated, one for control, and subtract predictions.  \n",
    "- Direct uplift models (e.g. interaction trees, causal forests, meta-learners like T-learner, S-learner).  \n",
    "- Regularisation and non-linear features to capture complex patterns.\n",
    "\n",
    "However, even this simple logistic-with-interactions model is a very practical\n",
    "and **modern** way to explore treatment heterogeneity in many real-world A/B tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa1d73",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Uplift modeling with a T-learner\n",
    "\n",
    "In the previous section we used a **single logistic regression with an interaction term**\n",
    "to model treatment effect heterogeneity.\n",
    "\n",
    "Another popular approach is the **T-learner**:\n",
    "\n",
    "- Fit one model for the treated group: \\(m_1(x) = \\mathbb{E}[Y \\mid T=1, X=x]\\).  \n",
    "- Fit another model for the control group: \\(m_0(x) = \\mathbb{E}[Y \\mid T=0, X=x]\\).  \n",
    "- Define the **uplift function** as:\n",
    "\n",
    "\\[\n",
    "\\tau(x) = m_1(x) - m_0(x),\n",
    "\\]\n",
    "\n",
    "which estimates how much the treatment changes the outcome probability for a user with features \\(X=x\\).\n",
    "\n",
    "In a randomized A/B test this is a way to **learn heterogeneity** in a flexible manner, and the same idea\n",
    "extends to more complex learners (trees, forests, boosted models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LogisticRegression is None:\n",
    "    print(\"scikit-learn not available; skipping T-learner section.\")\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    def train_t_learner_logistic(\n",
    "        df_in: pd.DataFrame,\n",
    "        feature_cols: list[str],\n",
    "        target_col: str = \"converted\",\n",
    "        group_col: str = \"group\",\n",
    "        treat_label: str = \"treatment\",\n",
    "        test_size: float = 0.3,\n",
    "        seed: int | None = 2025,\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Train a simple T-learner with logistic regression on a train/holdout split.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_in : DataFrame\n",
    "            Input experiment data with treatment, outcome, and features.\n",
    "        feature_cols : list[str]\n",
    "            Names of columns used as features X.\n",
    "        target_col : str\n",
    "            Name of binary outcome column.\n",
    "        group_col : str\n",
    "            Name of treatment group column (e.g., \"group\").\n",
    "        treat_label : str\n",
    "            Label for treated users in `group_col`, others are considered control.\n",
    "        test_size : float\n",
    "            Fraction of data reserved for evaluation.\n",
    "        seed : int | None\n",
    "            Random seed for splitting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_train, df_test : DataFrame\n",
    "            Train and test splits with all original columns.\n",
    "        \"\"\"\n",
    "        df_local = df_in.copy()\n",
    "        # Binary treatment indicator\n",
    "        df_local[\"treat_flag\"] = (df_local[group_col] == treat_label).astype(int)\n",
    "\n",
    "        # Train/test split\n",
    "        df_train, df_test = train_test_split(\n",
    "            df_local, test_size=test_size, random_state=seed, stratify=df_local[\"treat_flag\"]\n",
    "        )\n",
    "\n",
    "        # Fit separate logistic models in train set\n",
    "        df_train_t = df_train[df_train[\"treat_flag\"] == 1]\n",
    "        df_train_c = df_train[df_train[\"treat_flag\"] == 0]\n",
    "\n",
    "        X_t = df_train_t[feature_cols].to_numpy()\n",
    "        y_t = df_train_t[target_col].to_numpy()\n",
    "\n",
    "        X_c = df_train_c[feature_cols].to_numpy()\n",
    "        y_c = df_train_c[target_col].to_numpy()\n",
    "\n",
    "        mdl_t = LogisticRegression(max_iter=1000)\n",
    "        mdl_c = LogisticRegression(max_iter=1000)\n",
    "\n",
    "        mdl_t.fit(X_t, y_t)\n",
    "        mdl_c.fit(X_c, y_c)\n",
    "\n",
    "        # Store fitted models and feature list on df_train for reference (not used directly)\n",
    "        df_train.attrs[\"t_learner_models\"] = {\n",
    "            \"treated_model\": mdl_t,\n",
    "            \"control_model\": mdl_c,\n",
    "            \"feature_cols\": feature_cols,\n",
    "        }\n",
    "\n",
    "        # Predict uplift on test set\n",
    "        X_test = df_test[feature_cols].to_numpy()\n",
    "        p1_hat = mdl_t.predict_proba(X_test)[:, 1]\n",
    "        p0_hat = mdl_c.predict_proba(X_test)[:, 1]\n",
    "        uplift_hat = p1_hat - p0_hat\n",
    "\n",
    "        df_test = df_test.copy()\n",
    "        df_test[\"p1_hat\"] = p1_hat\n",
    "        df_test[\"p0_hat\"] = p0_hat\n",
    "        df_test[\"uplift_hat\"] = uplift_hat\n",
    "\n",
    "        return df_train, df_test\n",
    "\n",
    "\n",
    "    # Use pre_activity as the feature for this example\n",
    "    feature_cols = [\"pre_activity\"]\n",
    "    df_train_tl, df_test_tl = train_t_learner_logistic(\n",
    "        df_in=df,\n",
    "        feature_cols=feature_cols,\n",
    "        target_col=\"converted\",\n",
    "        group_col=\"group\",\n",
    "        treat_label=\"treatment\",\n",
    "        test_size=0.3,\n",
    "        seed=2025,\n",
    "    )\n",
    "\n",
    "    df_test_tl.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7506d79b",
   "metadata": {},
   "source": [
    "\n",
    "The test set now has, for each user:\n",
    "\n",
    "- `p1_hat`: predicted probability of conversion **if treated**.  \n",
    "- `p0_hat`: predicted probability of conversion **if in control**.  \n",
    "- `uplift_hat = p1_hat - p0_hat`: the modelled **individual uplift** in absolute probability points.\n",
    "\n",
    "Next we examine how this uplift signal correlates with **actual** treatment effects by ranking users\n",
    "according to `uplift_hat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8453ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LogisticRegression is not None:\n",
    "    def evaluate_uplift_by_quantile(\n",
    "        df_eval: pd.DataFrame,\n",
    "        uplift_col: str = \"uplift_hat\",\n",
    "        group_col: str = \"group\",\n",
    "        target_col: str = \"converted\",\n",
    "        n_bins: int = 5,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate uplift by quantiles of predicted uplift.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_eval : DataFrame\n",
    "            Evaluation data (e.g., test set) containing uplift predictions, group, and outcome.\n",
    "        uplift_col : str\n",
    "            Column name containing uplift predictions.\n",
    "        group_col : str\n",
    "            Column name of treatment group (must contain 'control' and 'treatment').\n",
    "        target_col : str\n",
    "            Binary outcome column name.\n",
    "        n_bins : int\n",
    "            Number of bins/quantiles for ranking.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            For each bin: average predicted uplift, actual conversion by group,\n",
    "            and realized uplift (treatment - control).\n",
    "        \"\"\"\n",
    "        df_ev = df_eval.copy()\n",
    "\n",
    "        # Higher uplift_hat = more likely to benefit from treatment\n",
    "        df_ev[\"uplift_bin\"] = pd.qcut(df_ev[uplift_col], n_bins, labels=False, duplicates=\"drop\")\n",
    "\n",
    "        summaries = []\n",
    "        for b in sorted(df_ev[\"uplift_bin\"].dropna().unique()):\n",
    "            sub = df_ev[df_ev[\"uplift_bin\"] == b]\n",
    "            avg_uplift_hat = float(sub[uplift_col].mean())\n",
    "            # Actual conversion by arm\n",
    "            conv_tab = (\n",
    "                sub.groupby(group_col)[target_col]\n",
    "                   .agg([\"mean\", \"count\"])\n",
    "                   .rename(columns={\"mean\": \"conversion_rate\"})\n",
    "            )\n",
    "            # Handle possible missing arm in rare small bins\n",
    "            conv_control = float(conv_tab.loc[\"control\", \"conversion_rate\"]) if \"control\" in conv_tab.index else float(\"nan\")\n",
    "            conv_treat = float(conv_tab.loc[\"treatment\", \"conversion_rate\"]) if \"treatment\" in conv_tab.index else float(\"nan\")\n",
    "            realized_uplift = conv_treat - conv_control if (not math.isnan(conv_treat) and not math.isnan(conv_control)) else float(\"nan\")\n",
    "\n",
    "            summaries.append(\n",
    "                {\n",
    "                    \"bin\": int(b),\n",
    "                    \"n_users\": int(sub.shape[0]),\n",
    "                    \"avg_uplift_hat\": avg_uplift_hat,\n",
    "                    \"conv_control\": conv_control,\n",
    "                    \"conv_treat\": conv_treat,\n",
    "                    \"realized_uplift\": realized_uplift,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        df_bins = pd.DataFrame(summaries).sort_values(\"bin\")\n",
    "        return df_bins\n",
    "\n",
    "\n",
    "    uplift_bins = evaluate_uplift_by_quantile(df_test_tl, uplift_col=\"uplift_hat\", n_bins=5)\n",
    "    uplift_bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LogisticRegression is not None:\n",
    "    # Plot realized uplift vs predicted uplift bin\n",
    "    plt.figure()\n",
    "    plt.plot(uplift_bins[\"bin\"], uplift_bins[\"realized_uplift\"], marker=\"o\")\n",
    "    plt.xlabel(\"uplift_hat quantile bin (0 = lowest predicted uplift)\")\n",
    "    plt.ylabel(\"realized uplift (conversion_treat - conversion_control)\")\n",
    "    plt.title(\"T-learner uplift model: realized uplift by predicted uplift bin\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99398c",
   "metadata": {},
   "source": [
    "\n",
    "If the T-learner is capturing useful signal, you should see that:\n",
    "\n",
    "- Bins with **higher predicted uplift** (`uplift_hat` larger) tend to have **larger realized uplift**.  \n",
    "- Low uplift bins may have small or even negative realized uplift.\n",
    "\n",
    "This is the core idea of uplift modeling:\n",
    "\n",
    "1. Use historical randomized data to **learn a function** \\(\\tau(x)\\) that predicts who benefits most.  \n",
    "2. In future, **target** the treatment (e.g., discount, new feature) to users with high predicted uplift.  \n",
    "3. Continue to log data and periodically **retrain** the uplift model.\n",
    "\n",
    "In production systems you would typically:\n",
    "\n",
    "- Use richer feature sets (RFM variables, geography, device, etc.).  \n",
    "- Use more flexible learners (gradient boosting, random forests, neural nets).  \n",
    "- Monitor stability and drift of uplift predictions over time.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}