{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# MovieLens Recommender \u2013 Hybrid Content + Ranking Ensemble\n\nThis notebook extends the ranking ensemble by adding a **content-based\nbase model** and blending it together with classic collaborative filtering.\n\nWe will:\n\n1. Use MovieLens `ml-latest-small` (`ratings.csv`, `movies.csv`).\n2. Train 3 collaborative base models on explicit ratings:\n   - Bias model (global + user + item effects).\n   - Item-based k-NN collaborative filtering.\n   - Matrix factorisation with biases.\n3. Build a **genre-based content model** from `movies.csv`:\n   - Represent each movie by a multi-hot genre vector.\n   - Represent each user by an average of genres of movies they liked.\n   - Score `(user, item)` by cosine similarity between user and item\n     genre vectors.\n4. Define relevance as `rating \u2265 threshold` (e.g. 4.0).\n5. Evaluate all base models plus a simple average on:\n   - Hit-rate@K.\n   - Precision@K.\n   - Recall@K.\n6. Train a **logistic regression stacking model** on top of **four base\n   scores** (3 CF + 1 content).\n7. Compare ranking performance and inspect learned weights for\n   collaborative vs content model.\n\nThis gives a more realistic ensemble:\n\n- **Collaborative** signals: patterns in who rated what.\n- **Content** signals: what the items are about (genres).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration\n\nDependencies:\n\n- `pandas`, `numpy` \u2013 data.\n- `sklearn` \u2013 splitting, similarity, logistic regression.\n- `matplotlib`, `seaborn` \u2013 plots.\n\nModels are implemented in plain Python/NumPy with small, typed classes.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Set\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MultiLabelBinarizer, normalize\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_DIR: Path = Path(\"data\") / \"ml-latest-small\"\nRATINGS_PATH: Path = DATA_DIR / \"ratings.csv\"\nMOVIES_PATH: Path = DATA_DIR / \"movies.csv\"\n\nfor p in [RATINGS_PATH, MOVIES_PATH]:\n    if not p.exists():\n        raise FileNotFoundError(\n            f\"Required file not found: {p.resolve()}\\n\"\n            \"Please ensure MovieLens 'ml-latest-small' is under data/ml-latest-small/.\"\n        )\n\nratings_df = pd.read_csv(RATINGS_PATH)\nmovies_df = pd.read_csv(MOVIES_PATH)\n\nprint(\"Ratings shape:\", ratings_df.shape)\nprint(\"Movies shape: \", movies_df.shape)\nratings_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute root mean squared error (RMSE).\n\n    Args:\n        y_true: True rating values.\n        y_pred: Predicted rating values.\n\n    Returns:\n        RMSE as a float.\n    \"\"\"\n    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n\n\nsns.histplot(ratings_df[\"rating\"], bins=10)\nplt.title(\"Rating distribution\")\nplt.xlabel(\"Rating\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Train / meta / test splits\n\nWe use a 3-way split suitable for stacking:\n\n1. `train_full` (80%) vs `test` (20%).\n2. `train_full` \u2192 `base_train` (80%) + `meta_train` (20%).\n\n- Base models are trained on `base_train`.\n- Meta-model is trained on predictions on `meta_train`.\n- `test` is never seen during training.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "train_full_df, test_df = train_test_split(\n    ratings_df,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\nbase_train_df, meta_train_df = train_test_split(\n    train_full_df,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\nprint(\"Base train size:\", len(base_train_df))\nprint(\"Meta train size:\", len(meta_train_df))\nprint(\"Test size:      \", len(test_df))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Collaborative base models\n\nWe reuse three models:\n\n1. **BiasModel** \u2013 global mean + user and item biases.\n2. **ItemKNNModel** \u2013 item-based k-NN CF using cosine similarity.\n3. **MatrixFactorizationModel** \u2013 latent factor model with SGD training.\n\nAll expose a common API:\n\n- `fit(df)`\n- `predict_df(df)`\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.1 BiasModel\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class BiasModel:\n    \"\"\"Global + user + item bias recommender.\n\n    Rating estimate: mu + b_u + b_i\n    where mu is global mean and b_u / b_i are learned deviations.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.mu: float | None = None\n        self.user_bias: Dict[int, float] = {}\n        self.item_bias: Dict[int, float] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        \"\"\"Fit bias terms from a ratings DataFrame.\n\n        Args:\n            df: DataFrame with columns `userId`, `movieId`, `rating`.\n        \"\"\"\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        self.mu = float(df[\"rating\"].mean())\n\n        user_mean = df.groupby(\"userId\")[\"rating\"].mean()\n        item_mean = df.groupby(\"movieId\")[\"rating\"].mean()\n\n        # Deviations from global mean\n        self.user_bias = (user_mean - self.mu).to_dict()\n        self.item_bias = (item_mean - self.mu).to_dict()\n\n    def predict_row(self, user_id: int, movie_id: int) -> float:\n        \"\"\"Predict rating for a single user\u2013item pair.\n\n        Args:\n            user_id: User identifier.\n            movie_id: Movie identifier.\n\n        Returns:\n            Predicted rating as float.\n        \"\"\"\n        if self.mu is None:\n            raise RuntimeError(\"Model has not been fitted.\")\n\n        bu = self.user_bias.get(user_id, 0.0)\n        bi = self.item_bias.get(movie_id, 0.0)\n        return float(self.mu + bu + bi)\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Predict ratings for many user\u2013item pairs.\n\n        Args:\n            df: DataFrame with `userId`, `movieId`.\n\n        Returns:\n            Array of predictions aligned with df rows.\n        \"\"\"\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self.predict_row(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nbias_model = BiasModel()\nbias_model.fit(base_train_df)\n\nbias_meta_preds = bias_model.predict_df(meta_train_df)\nprint(\"Bias model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), bias_meta_preds))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.2 ItemKNNModel \u2013 item-based CF\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class ItemKNNModel:\n    \"\"\"Item-based k-NN collaborative filtering.\n\n    Uses cosine similarity over item rating vectors and a similarity-\n    weighted average of neighbour ratings per user.\n    \"\"\"\n\n    def __init__(self, k: int = 40, default_rating: float = 3.5) -> None:\n        self.k = k\n        self.default_rating = float(default_rating)\n\n        self.user_id_to_index: Dict[int, int] = {}\n        self.item_id_to_index: Dict[int, int] = {}\n        self.R: np.ndarray | None = None\n        self.item_sim: np.ndarray | None = None\n\n    def fit(self, df: pd.DataFrame) -> None:\n        \"\"\"Fit k-NN model from a ratings DataFrame.\n\n        Args:\n            df: DataFrame with `userId`, `movieId`, `rating`.\n        \"\"\"\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        unique_users = df[\"userId\"].unique()\n        unique_items = df[\"movieId\"].unique()\n\n        self.user_id_to_index = {uid: idx for idx, uid in enumerate(unique_users)}\n        self.item_id_to_index = {iid: idx for idx, iid in enumerate(unique_items)}\n\n        n_users = len(unique_users)\n        n_items = len(unique_items)\n\n        R = np.zeros((n_users, n_items), dtype=np.float32)\n        for row in df.itertuples(index=False):\n            u_idx = self.user_id_to_index[row.userId]\n            i_idx = self.item_id_to_index[row.movieId]\n            R[u_idx, i_idx] = row.rating\n\n        self.R = R\n        self.item_sim = cosine_similarity(R.T)\n\n    def _predict_single(self, user_id: int, movie_id: int) -> float:\n        if self.R is None or self.item_sim is None:\n            raise RuntimeError(\"Model has not been fitted.\")\n\n        u_idx = self.user_id_to_index.get(user_id)\n        i_idx = self.item_id_to_index.get(movie_id)\n        if u_idx is None or i_idx is None:\n            return self.default_rating\n\n        user_ratings = self.R[u_idx, :]\n        sims = self.item_sim[i_idx, :]\n\n        rated_mask = user_ratings > 0\n        rated_indices = np.where(rated_mask)[0]\n        if rated_indices.size == 0:\n            return self.default_rating\n\n        sims_rated = sims[rated_indices]\n        ratings_rated = user_ratings[rated_indices]\n\n        k_use = min(self.k, rated_indices.size)\n        top_idx = np.argsort(sims_rated)[-k_use:]\n\n        neigh_sims = sims_rated[top_idx]\n        neigh_ratings = ratings_rated[top_idx]\n\n        if np.all(neigh_sims == 0):\n            return float(neigh_ratings.mean())\n\n        pred = float(np.dot(neigh_sims, neigh_ratings) / np.sum(np.abs(neigh_sims)))\n        return pred\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self._predict_single(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nbias_global_mean = float(base_train_df[\"rating\"].mean())\nitem_knn_model = ItemKNNModel(k=40, default_rating=bias_global_mean)\nitem_knn_model.fit(base_train_df)\n\nitem_meta_preds = item_knn_model.predict_df(meta_train_df)\nprint(\"Item-kNN model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), item_meta_preds))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.3 MatrixFactorizationModel \u2013 latent factors\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "@dataclass\nclass MFConfig:\n    \"\"\"Configuration for matrix factorisation model.\"\"\"\n\n    n_factors: int = 30\n    n_epochs: int = 10\n    lr: float = 0.01\n    reg: float = 0.05\n\n\nclass MatrixFactorizationModel:\n    \"\"\"Matrix factorisation with biases trained via SGD.\n\n    Rating estimate: mu + b_u + b_i + p_u^T q_i\n    \"\"\"\n\n    def __init__(self, config: MFConfig, random_state: int = 42) -> None:\n        self.config = config\n        self.random_state = random_state\n\n        self.mu: float | None = None\n        self.user_bias: Dict[int, float] = {}\n        self.item_bias: Dict[int, float] = {}\n        self.P: Dict[int, np.ndarray] = {}\n        self.Q: Dict[int, np.ndarray] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        \"\"\"Fit MF model on a ratings DataFrame.\n\n        Args:\n            df: DataFrame with `userId`, `movieId`, `rating`.\n        \"\"\"\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        rng = np.random.default_rng(self.random_state)\n\n        user_ids = df[\"userId\"].unique()\n        item_ids = df[\"movieId\"].unique()\n\n        self.mu = float(df[\"rating\"].mean())\n        self.user_bias = {u: 0.0 for u in user_ids}\n        self.item_bias = {i: 0.0 for i in item_ids}\n\n        k = self.config.n_factors\n        self.P = {u: 0.1 * rng.standard_normal(k) for u in user_ids}\n        self.Q = {i: 0.1 * rng.standard_normal(k) for i in item_ids}\n\n        lr = self.config.lr\n        reg = self.config.reg\n\n        user_arr = df[\"userId\"].to_numpy()\n        item_arr = df[\"movieId\"].to_numpy()\n        rating_arr = df[\"rating\"].to_numpy()\n\n        n_obs = len(df)\n\n        for epoch in range(self.config.n_epochs):\n            idx = rng.permutation(n_obs)\n            se = 0.0\n\n            for t in idx:\n                u = int(user_arr[t])\n                i = int(item_arr[t])\n                r_ui = float(rating_arr[t])\n\n                bu = self.user_bias[u]\n                bi = self.item_bias[i]\n                pu = self.P[u]\n                qi = self.Q[i]\n\n                pred = self.mu + bu + bi + float(np.dot(pu, qi))\n                err = r_ui - pred\n                se += err * err\n\n                # Bias updates\n                self.user_bias[u] = bu + lr * (err - reg * bu)\n                self.item_bias[i] = bi + lr * (err - reg * bi)\n\n                # Latent factor updates\n                pu_new = pu + lr * (err * qi - reg * pu)\n                qi_new = qi + lr * (err * pu - reg * qi)\n\n                self.P[u] = pu_new\n                self.Q[i] = qi_new\n\n            train_rmse = float(np.sqrt(se / n_obs))\n            print(f\"Epoch {epoch+1}/{self.config.n_epochs} - train RMSE: {train_rmse:.4f}\")\n\n    def predict_single(self, user_id: int, movie_id: int) -> float:\n        if self.mu is None:\n            raise RuntimeError(\"Model has not been fitted.\")\n\n        bu = self.user_bias.get(user_id)\n        bi = self.item_bias.get(movie_id)\n        pu = self.P.get(user_id)\n        qi = self.Q.get(movie_id)\n\n        if bu is None or bi is None or pu is None or qi is None:\n            return float(self.mu)\n\n        return float(self.mu + bu + bi + float(np.dot(pu, qi)))\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self.predict_single(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nmf_config = MFConfig(n_factors=30, n_epochs=8, lr=0.01, reg=0.05)\nmf_model = MatrixFactorizationModel(config=mf_config, random_state=RANDOM_STATE)\n\nmf_model.fit(base_train_df)\n\nmf_meta_preds = mf_model.predict_df(meta_train_df)\nprint(\"MF model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), mf_meta_preds))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Genre-based content model\n\nWe now build a **content-based base model** from genres in `movies.csv`.\n\nSteps:\n\n1. Parse `genres` column into a list of genre tokens per movie.\n2. Build a multi-hot genre matrix using `MultiLabelBinarizer`.\n3. Create **movie genre vectors** (rows of that matrix).\n4. For each user, build a **user profile vector** as the average of genre\n   vectors of movies they rated above a threshold.\n5. Score `(user, item)` as the cosine similarity between the user vector\n   and the item genre vector.\n\nThis is a classic, interpretable content-based model.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def build_genre_matrix(movies: pd.DataFrame) -> Tuple[pd.DataFrame, MultiLabelBinarizer]:\n    \"\"\"Build a multi-hot genre matrix for movies.\n\n    Args:\n        movies: DataFrame with columns `movieId`, `genres`.\n\n    Returns:\n        Tuple of (genre_df, mlb) where genre_df has movieId as index and\n        one column per genre.\n    \"\"\"\n    movies = movies.copy()\n    movies[\"genres_list\"] = movies[\"genres\"].fillna(\"(no genres listed)\")\n\n    def split_genres(s: str) -> List[str]:\n        if s == \"(no genres listed)\" or s == \"No Genres Listed\":\n            return []\n        return [g.strip() for g in s.split(\"|\") if g.strip()]\n\n    movies[\"genres_list\"] = movies[\"genres_list\"].apply(split_genres)\n\n    mlb = MultiLabelBinarizer()\n    genre_mat = mlb.fit_transform(movies[\"genres_list\"])\n\n    genre_df = pd.DataFrame(\n        genre_mat,\n        columns=[f\"genre:{g}\" for g in mlb.classes_],\n        index=movies[\"movieId\"].to_numpy(),\n    )\n    return genre_df, mlb\n\n\ngenre_df, mlb = build_genre_matrix(movies_df)\nprint(\"Genre matrix shape:\", genre_df.shape)\ngenre_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class GenreContentModel:\n    \"\"\"Content-based model using movie genres.\n\n    For each user, builds a genre profile based on liked movies and\n    scores items by cosine similarity between user and item genre\n    vectors.\n    \"\"\"\n\n    def __init__(self, genre_df: pd.DataFrame, rel_threshold: float = 4.0) -> None:\n        self.genre_df = genre_df\n        self.rel_threshold = rel_threshold\n        # userId -> genre vector (np.ndarray)\n        self.user_profiles: Dict[int, np.ndarray] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        \"\"\"Fit user profiles from ratings.\n\n        Args:\n            df: Ratings DataFrame with `userId`, `movieId`, `rating`.\n        \"\"\"\n        # Filter to liked movies\n        liked = df[df[\"rating\"] >= self.rel_threshold]\n\n        # Join genres for liked items\n        joined = liked.merge(self.genre_df, left_on=\"movieId\", right_index=True, how=\"left\")\n\n        genre_cols = self.genre_df.columns\n\n        # Build profiles as average genre vector of liked movies\n        profiles = joined.groupby(\"userId\")[genre_cols].mean().fillna(0.0)\n\n        # L2-normalise for cosine similarity\n        profiles_mat = normalize(profiles.to_numpy(), norm=\"l2\")\n\n        for uid, vec in zip(profiles.index.to_numpy(), profiles_mat):\n            self.user_profiles[int(uid)] = vec\n\n    def _score_single(self, user_id: int, movie_id: int) -> float:\n        \"\"\"Score a single user\u2013item pair by cosine similarity.\n\n        Args:\n            user_id: User identifier.\n            movie_id: Movie identifier.\n\n        Returns:\n            Similarity score in [0, 1] (approximately), or 0 if no\n            profile / item vector is available.\n        \"\"\"\n        user_vec = self.user_profiles.get(user_id)\n        if user_vec is None:\n            return 0.0\n\n        if movie_id not in self.genre_df.index:\n            return 0.0\n\n        item_vec = self.genre_df.loc[movie_id].to_numpy(dtype=float)\n        if not np.any(item_vec):\n            return 0.0\n\n        # Normalise item vector to unit length\n        item_vec_norm = item_vec / (np.linalg.norm(item_vec) + 1e-12)\n        score = float(np.dot(user_vec, item_vec_norm))\n        return score\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Score multiple user\u2013item pairs.\n\n        Args:\n            df: DataFrame with `userId`, `movieId`.\n\n        Returns:\n            Array of similarity scores aligned with df rows.\n        \"\"\"\n        scores: List[float] = []\n        for row in df.itertuples(index=False):\n            scores.append(self._score_single(int(row.userId), int(row.movieId)))\n        return np.array(scores, dtype=float)\n\n\ngenre_model = GenreContentModel(genre_df=genre_df, rel_threshold=4.0)\ngenre_model.fit(base_train_df)\n\ngenre_meta_scores = genre_model.predict_df(meta_train_df)\nprint(\"Genre-content scores on meta_train: min=\", genre_meta_scores.min(), \"max=\", genre_meta_scores.max())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Relevance and ranking metrics\n\nWe now switch focus to **ranking quality**.\n\n- Define relevance: rating \u2265 `REL_THRESHOLD`.\n- For each user, rank test items by a model's scores.\n- Compute:\n  - Hit-rate@K.\n  - Precision@K.\n  - Recall@K.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "REL_THRESHOLD: float = 4.0\nK_EVAL: int = 10\n\n\ndef get_user_relevant_items(df: pd.DataFrame, user_id: int, threshold: float = REL_THRESHOLD) -> Set[int]:\n    \"\"\"Return set of relevant movieIds for a user.\n\n    Args:\n        df: Ratings DataFrame.\n        user_id: User identifier.\n        threshold: Rating threshold for relevance.\n\n    Returns:\n        Set of movieIds considered relevant.\n    \"\"\"\n    mask = (df[\"userId\"] == user_id) & (df[\"rating\"] >= threshold)\n    return set(df.loc[mask, \"movieId\"].unique())\n\n\ndef ranking_metrics_for_model_on_test(\n    test_df: pd.DataFrame,\n    scores: np.ndarray,\n    k: int = K_EVAL,\n    threshold: float = REL_THRESHOLD,\n) -> Dict[str, float]:\n    \"\"\"Compute hit-rate, precision@K, recall@K from per-row scores.\n\n    Args:\n        test_df: Test ratings DataFrame.\n        scores: Score array aligned with test_df.\n        k: Cutoff for top-K recommendations.\n        threshold: Rating threshold for relevance.\n\n    Returns:\n        Dict with hit_rate, precision_at_k, recall_at_k, n_eval_users.\n    \"\"\"\n    df_scores = test_df.copy()\n    df_scores[\"score\"] = scores\n\n    users = df_scores[\"userId\"].unique()\n\n    hits = 0\n    sum_precision = 0.0\n    sum_recall = 0.0\n    n_eval_users = 0\n\n    for u in users:\n        user_rows = df_scores[df_scores[\"userId\"] == u]\n        relevant_items = get_user_relevant_items(test_df, u, threshold=threshold)\n        if not relevant_items:\n            continue\n\n        n_eval_users += 1\n\n        user_rows_sorted = user_rows.sort_values(\"score\", ascending=False)\n        top_k = user_rows_sorted.head(k)\n\n        recommended_items = set(top_k[\"movieId\"].tolist())\n        n_rel_top = len(recommended_items & relevant_items)\n\n        if n_rel_top > 0:\n            hits += 1\n\n        precision_u = n_rel_top / min(k, len(user_rows_sorted))\n        recall_u = n_rel_top / len(relevant_items)\n\n        sum_precision += precision_u\n        sum_recall += recall_u\n\n    if n_eval_users == 0:\n        raise ValueError(\"No users with relevant items in test for ranking evaluation.\")\n\n    hit_rate = hits / n_eval_users\n    precision_at_k = sum_precision / n_eval_users\n    recall_at_k = sum_recall / n_eval_users\n\n    return {\n        \"hit_rate\": hit_rate,\n        \"precision_at_k\": precision_at_k,\n        \"recall_at_k\": recall_at_k,\n        \"n_eval_users\": float(n_eval_users),\n    }\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Base models (collaborative + content) as rankers\n\nWe now evaluate 4 base models and a simple average ensemble on the test\nset using ranking metrics.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "y_test = test_df[\"rating\"].to_numpy()\n\nbias_test_scores = bias_model.predict_df(test_df)\nitem_test_scores = item_knn_model.predict_df(test_df)\nmf_test_scores = mf_model.predict_df(test_df)\ngenre_test_scores = genre_model.predict_df(test_df)\n\nmetrics_bias = ranking_metrics_for_model_on_test(test_df, bias_test_scores, k=K_EVAL)\nmetrics_item = ranking_metrics_for_model_on_test(test_df, item_test_scores, k=K_EVAL)\nmetrics_mf = ranking_metrics_for_model_on_test(test_df, mf_test_scores, k=K_EVAL)\nmetrics_genre = ranking_metrics_for_model_on_test(test_df, genre_test_scores, k=K_EVAL)\n\n# Simple average of all four models\navg4_test_scores = (bias_test_scores + item_test_scores + mf_test_scores + genre_test_scores) / 4.0\nmetrics_avg4 = ranking_metrics_for_model_on_test(test_df, avg4_test_scores, k=K_EVAL)\n\nmetrics_bias, metrics_item, metrics_mf, metrics_genre, metrics_avg4\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "rows = []\nfor name, m in [\n    (\"BiasModel\", metrics_bias),\n    (\"ItemKNNModel\", metrics_item),\n    (\"MFModel\", metrics_mf),\n    (\"GenreContent\", metrics_genre),\n    (\"SimpleAverage4\", metrics_avg4),\n]:\n    rows.append(\n        {\n            \"model\": name,\n            \"hit_rate\": m[\"hit_rate\"],\n            \"precision_at_k\": m[\"precision_at_k\"],\n            \"recall_at_k\": m[\"recall_at_k\"],\n        }\n    )\n\nbase_rank_df = pd.DataFrame(rows)\nbase_rank_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "base_rank_melt = base_rank_df.melt(id_vars=\"model\", var_name=\"metric\", value_name=\"value\")\n\nsns.barplot(data=base_rank_melt, x=\"metric\", y=\"value\", hue=\"model\")\nplt.ylim(0, 1)\nplt.ylabel(\"Score\")\nplt.title(f\"Collaborative vs content vs simple average (K={K_EVAL})\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The genre-based content model may not beat MF, but it often helps with\ncold items or users with clear genre tastes. The goal is **complementary\nsignals**, not necessarily a standalone winner.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Stacked ensemble with collaborative + content scores\n\nWe now build a **logistic regression meta-model** for ranking, using\nfour base scores as features:\n\n- BiasModel score.\n- ItemKNNModel score.\n- MFModel score.\n- GenreContent score.\n\nMeta-training data comes from `meta_train_df`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Build meta features and labels on meta_train_df\n\nbias_meta_scores = bias_meta_preds\nitem_meta_scores = item_meta_preds\nmf_meta_scores = mf_meta_preds\ngenre_meta_scores = genre_meta_scores  # already computed above\n\nX_meta_rank = np.vstack([\n    bias_meta_scores,\n    item_meta_scores,\n    mf_meta_scores,\n    genre_meta_scores,\n]).T\n\ny_meta_rank = (meta_train_df[\"rating\"].to_numpy() >= REL_THRESHOLD).astype(int)\n\nprint(\"X_meta_rank shape:\", X_meta_rank.shape)\nprint(\"Positive rate (meta):\", y_meta_rank.mean())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "meta_rank_model = LogisticRegression(\n    penalty=\"l2\",\n    C=1.0,\n    solver=\"lbfgs\",\n    max_iter=1000,\n    random_state=RANDOM_STATE,\n)\n\nmeta_rank_model.fit(X_meta_rank, y_meta_rank)\n\nprint(\"Meta-ranking coefficients (Bias, ItemKNN, MF, Genre):\", meta_rank_model.coef_)\nprint(\"Meta-ranking intercept:\", meta_rank_model.intercept_)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 7.1 Evaluate stacked ensemble on test\n\nWe now apply the meta-model to test rows:\n\n- Build `X_test_rank` from the four base scores on test.\n- Compute `p(relevant)` for each row.\n- Rank per user by that probability.\n- Compute hit-rate@K, precision@K, recall@K.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "X_test_rank = np.vstack([\n    bias_test_scores,\n    item_test_scores,\n    mf_test_scores,\n    genre_test_scores,\n]).T\n\nstacked_test_proba = meta_rank_model.predict_proba(X_test_rank)[:, 1]\n\nmetrics_stacked = ranking_metrics_for_model_on_test(\n    test_df,\n    stacked_test_proba,\n    k=K_EVAL,\n    threshold=REL_THRESHOLD,\n)\n\nmetrics_stacked\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "rows_full = rows.copy()\nrows_full.append(\n    {\n        \"model\": \"StackedLogistic4\",\n        \"hit_rate\": metrics_stacked[\"hit_rate\"],\n        \"precision_at_k\": metrics_stacked[\"precision_at_k\"],\n        \"recall_at_k\": metrics_stacked[\"recall_at_k\"],\n    }\n)\n\nrank_compare_df = pd.DataFrame(rows_full)\nrank_compare_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "rank_compare_melt = rank_compare_df.melt(id_vars=\"model\", var_name=\"metric\", value_name=\"value\")\n\nsns.barplot(data=rank_compare_melt, x=\"metric\", y=\"value\", hue=\"model\")\nplt.ylim(0, 1)\nplt.ylabel(\"Score\")\nplt.title(f\"Ranking ensemble with collaborative + content (K={K_EVAL})\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Typically you will see that the stacked ensemble (`StackedLogistic4`)\nperforms at least as well as the best single CF model and often a bit\nbetter than the simple average. The gain may be modest on this small\ndataset, but the pattern scales when you add stronger and more diverse\nbase models.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Interpreting collaborative vs content weights\n\nWe inspect the logistic regression coefficients for the four base\nmodels to see how much each contributes to predicted relevance.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "coef_names = [\"BiasModel\", \"ItemKNNModel\", \"MFModel\", \"GenreContent\"]\ncoef_values = meta_rank_model.coef_[0]\n\ncoef_df = pd.DataFrame({\"base_model\": coef_names, \"weight\": coef_values})\ncoef_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "sns.barplot(data=coef_df, x=\"base_model\", y=\"weight\")\nplt.title(\"Meta-ranking weights: collaborative vs content\")\nplt.ylabel(\"Logistic regression coefficient\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Interpretation:\n\n- **MFModel** often gets a strong positive weight (strong CF signal).\n- **ItemKNNModel** may get smaller / moderate weight (local CF signal).\n- **BiasModel** can act as a stabiliser.\n- **GenreContent** may be smaller but still positive, indicating that\n  genre similarity helps resolve some cases (especially for cold items\n  or users with clear genre tastes).\n\nNegative weights would mean the meta-model is using a base model mostly\nas a correction term, but that is less common here.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Design summary and extensions\n\n### 9.1 Why add a content model?\n\nCollaborative models only use **who rated what**. They struggle when:\n\n- An item is new (few or no ratings).\n- A user has very few interactions.\n\nContent-based models use **what the item is** (genres, tags, text):\n\n- They can recommend items that are similar in content even if\n  interactions are sparse.\n- They make ensembles more robust and interpretable.\n\n### 9.2 Why still use logistic regression on top?\n\n- We want a **single final score** that reflects relevance.\n- Base models can have different scales and biases.\n- Logistic regression learns how to reweight and rescale them so that\n  the probability of relevance is best separated from non-relevance.\n\n### 9.3 Easy extensions\n\n1. **Richer content**\n   - Use tags (from `tags.csv`) and build TF-IDF features.\n   - Use embeddings from descriptions / plots if available.\n\n2. **More base models**\n   - Add LightFM, Surprise SVD, etc., and treat their scores as\n     additional features in `X_meta_rank`.\n\n3. **User segments**\n   - Add user-level features (e.g. number of ratings, average rating)\n     as meta-features.\n\n4. **Cross-validated stacking**\n   - Use out-of-fold base predictions instead of a single meta split for\n     more robust meta-training.\n\nThis notebook gives a full, concrete example of **hybrid collaborative\n+ content ensembles aimed at ranking metrics**, with explanations of why\neach design choice is made.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}