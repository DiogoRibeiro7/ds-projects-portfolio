{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07a56ec",
   "metadata": {},
   "source": [
    "\n",
    "# Exploration–Exploitation: Multi-Armed Bandits for Experimentation\n",
    "\n",
    "This notebook complements classical A/B testing by introducing **multi-armed bandits**.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "- Assigning a fixed share of traffic to each variant and analyzing at the end,\n",
    "\n",
    "bandits aim to:\n",
    "\n",
    "- **Exploit** arms that look good.  \n",
    "- Still **explore** enough so you do not miss better options.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Define a simple **bandit environment** with Bernoulli rewards (e.g. click / no click).  \n",
    "2. Implement several policies:\n",
    "   - Greedy (always pick the current best).  \n",
    "   - \\(\\varepsilon\\)-greedy.  \n",
    "   - UCB1 (Upper Confidence Bound).  \n",
    "   - Thompson Sampling (Beta–Bernoulli).  \n",
    "3. Compare them in terms of **cumulative reward** and **regret**.\n",
    "\n",
    "The code is written to be reusable in other notebooks or small projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bdd778",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f227f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Callable\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a5f6d",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Bandit environment\n",
    "\n",
    "We assume **K arms**, each with an unknown Bernoulli success probability:\n",
    "\n",
    "\\[\n",
    "P(R=1 \\mid A = k) = p_k.\n",
    "\\]\n",
    "\n",
    "You can think of:\n",
    "\n",
    "- Arms = different variants / creatives / prices.  \n",
    "- Reward = click / conversion / success.\n",
    "\n",
    "The environment holds the true \\(p_k\\) and generates rewards when a policy chooses arms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class BanditEnv:\n",
    "    \"\"\"Stateless K-armed bandit environment with Bernoulli rewards.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    probs : np.ndarray\n",
    "        True success probabilities for each arm (shape (K,)).\n",
    "    rng : np.random.Generator\n",
    "        Random number generator for reproducibility.\n",
    "    \"\"\"\n",
    "\n",
    "    probs: np.ndarray\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def pull(self, arm: int) -> int:\n",
    "        \"\"\"Sample a reward (0/1) from the given arm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arm : int\n",
    "            Index of the arm to pull (0 <= arm < K).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Reward (0 or 1).\n",
    "        \"\"\"\n",
    "        if arm < 0 or arm >= self.probs.shape[0]:\n",
    "            raise IndexError(\"arm index out of range\")\n",
    "        p = float(self.probs[arm])\n",
    "        return int(self.rng.binomial(1, p))\n",
    "\n",
    "\n",
    "def make_example_env(seed: int | None = 123) -> BanditEnv:\n",
    "    \"\"\"Create a simple 4-armed environment with fixed probabilities.\n",
    "\n",
    "    For example:\n",
    "    - Arm 0: 0.05\n",
    "    - Arm 1: 0.06\n",
    "    - Arm 2: 0.04\n",
    "    - Arm 3: 0.08 (best)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int | None\n",
    "        Random seed for the environment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BanditEnv\n",
    "        Environment with fixed probabilities.\n",
    "    \"\"\"\n",
    "    probs = np.array([0.05, 0.06, 0.04, 0.08], dtype=float)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return BanditEnv(probs=probs, rng=rng)\n",
    "\n",
    "\n",
    "env = make_example_env()\n",
    "env.probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21da7bf",
   "metadata": {},
   "source": [
    "\n",
    "The **optimal arm** is the one with the highest success probability. In real life,\n",
    "you do not know these probabilities and must learn them from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f86ffa",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Bandit policies\n",
    "\n",
    "We implement a tiny interface:\n",
    "\n",
    "- Each policy keeps its own internal state (counts, estimates, prior parameters).  \n",
    "- At each step:\n",
    "  1. The policy chooses an arm.  \n",
    "  2. The environment returns a reward.  \n",
    "  3. The policy updates its state.\n",
    "\n",
    "We will implement:\n",
    "\n",
    "1. **Greedy**: always pick the arm with highest empirical mean.  \n",
    "2. **\\(\\varepsilon\\)-greedy**: explore with probability \\(\\varepsilon\\), exploit otherwise.  \n",
    "3. **UCB1**: pick arm with the largest upper confidence bound.  \n",
    "4. **Thompson Sampling**: sample from Beta posteriors for each arm and pick the best sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa795fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class BanditPolicyState:\n",
    "    \"\"\"State of a bandit policy at a given time.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    counts : np.ndarray\n",
    "        Number of times each arm was pulled.\n",
    "    successes : np.ndarray\n",
    "        Number of successes observed for each arm.\n",
    "    \"\"\"\n",
    "    counts: np.ndarray\n",
    "    successes: np.ndarray\n",
    "\n",
    "\n",
    "class GreedyPolicy:\n",
    "    \"\"\"Greedy bandit policy: always pick the arm with highest empirical mean.\n",
    "\n",
    "    If some arms have not been tried yet, it explores them first (round-robin).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms: int) -> None:\n",
    "        self.n_arms = n_arms\n",
    "        self.state = BanditPolicyState(\n",
    "            counts=np.zeros(n_arms, dtype=int),\n",
    "            successes=np.zeros(n_arms, dtype=int),\n",
    "        )\n",
    "\n",
    "    def select_arm(self) -> int:\n",
    "        # Explore arms that have not been tried yet\n",
    "        unexplored = np.where(self.state.counts == 0)[0]\n",
    "        if unexplored.size > 0:\n",
    "            return int(unexplored[0])\n",
    "\n",
    "        # Exploit: choose arm with largest empirical mean\n",
    "        means = self.state.successes / np.maximum(self.state.counts, 1)\n",
    "        return int(np.argmax(means))\n",
    "\n",
    "    def update(self, arm: int, reward: int) -> None:\n",
    "        self.state.counts[arm] += 1\n",
    "        self.state.successes[arm] += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d53eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    \"\"\"Epsilon-greedy policy.\n",
    "\n",
    "    With probability epsilon, pick a random arm (exploration).\n",
    "    With probability 1 - epsilon, pick the greedy arm based on empirical means.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms: int, epsilon: float = 0.1, seed: int | None = None) -> None:\n",
    "        if not (0.0 <= epsilon <= 1.0):\n",
    "            raise ValueError(\"epsilon must be in [0, 1].\")\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = float(epsilon)\n",
    "        self.state = BanditPolicyState(\n",
    "            counts=np.zeros(n_arms, dtype=int),\n",
    "            successes=np.zeros(n_arms, dtype=int),\n",
    "        )\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def select_arm(self) -> int:\n",
    "        # Explore\n",
    "        if self.rng.uniform() < self.epsilon:\n",
    "            return int(self.rng.integers(0, self.n_arms))\n",
    "\n",
    "        # Otherwise behave like greedy\n",
    "        unexplored = np.where(self.state.counts == 0)[0]\n",
    "        if unexplored.size > 0:\n",
    "            return int(unexplored[0])\n",
    "\n",
    "        means = self.state.successes / np.maximum(self.state.counts, 1)\n",
    "        return int(np.argmax(means))\n",
    "\n",
    "    def update(self, arm: int, reward: int) -> None:\n",
    "        self.state.counts[arm] += 1\n",
    "        self.state.successes[arm] += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UCB1Policy:\n",
    "    \"\"\"UCB1 (Upper Confidence Bound) policy for Bernoulli bandits.\n",
    "\n",
    "    At time t, choose arm a maximizing:\n",
    "\n",
    "        mean_a + sqrt( 2 * log(t) / N_a )\n",
    "\n",
    "    where mean_a is empirical mean and N_a is number of pulls of arm a.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms: int) -> None:\n",
    "        self.n_arms = n_arms\n",
    "        self.state = BanditPolicyState(\n",
    "            counts=np.zeros(n_arms, dtype=int),\n",
    "            successes=np.zeros(n_arms, dtype=int),\n",
    "        )\n",
    "        self.t = 0  # total number of pulls so far\n",
    "\n",
    "    def select_arm(self) -> int:\n",
    "        self.t += 1\n",
    "\n",
    "        # Pull each arm once first\n",
    "        unexplored = np.where(self.state.counts == 0)[0]\n",
    "        if unexplored.size > 0:\n",
    "            return int(unexplored[0])\n",
    "\n",
    "        means = self.state.successes / np.maximum(self.state.counts, 1)\n",
    "        bonus = np.sqrt(2.0 * math.log(self.t) / self.state.counts)\n",
    "        ucb = means + bonus\n",
    "        return int(np.argmax(ucb))\n",
    "\n",
    "    def update(self, arm: int, reward: int) -> None:\n",
    "        self.state.counts[arm] += 1\n",
    "        self.state.successes[arm] += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ThompsonSamplingPolicy:\n",
    "    \"\"\"Thompson Sampling for Bernoulli bandits with Beta priors.\n",
    "\n",
    "    Each arm a has a Beta(alpha_a, beta_a) posterior for its success probability.\n",
    "    At each step:\n",
    "    - sample theta_a ~ Beta(alpha_a, beta_a)\n",
    "    - play arm with largest sampled theta_a\n",
    "    - update alpha/beta based on reward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms: int, alpha0: float = 1.0, beta0: float = 1.0, seed: int | None = None) -> None:\n",
    "        if alpha0 <= 0.0 or beta0 <= 0.0:\n",
    "            raise ValueError(\"alpha0 and beta0 must be positive.\")\n",
    "        self.n_arms = n_arms\n",
    "        self.alpha = np.full(n_arms, alpha0, dtype=float)\n",
    "        self.beta = np.full(n_arms, beta0, dtype=float)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def select_arm(self) -> int:\n",
    "        theta_samples = self.rng.beta(self.alpha, self.beta)\n",
    "        return int(np.argmax(theta_samples))\n",
    "\n",
    "    def update(self, arm: int, reward: int) -> None:\n",
    "        if reward not in (0, 1):\n",
    "            raise ValueError(\"reward must be 0 or 1 for Bernoulli TS.\")\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1.0\n",
    "        else:\n",
    "            self.beta[arm] += 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e3b6b",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Simulation harness: one run\n",
    "\n",
    "We now define a utility to simulate a **single bandit run**:\n",
    "\n",
    "- Run for `T` steps.  \n",
    "- At each step: policy chooses an arm, environment returns a reward.  \n",
    "- Track:\n",
    "  - reward per step,  \n",
    "  - chosen arm,  \n",
    "  - cumulative reward,  \n",
    "  - cumulative regret.\n",
    "\n",
    "Regret is defined with respect to the best arm:  \n",
    "\n",
    "\\[\n",
    "\\text{regret}_t = t \\cdot p^* - \\sum_{s=1}^t r_s,\n",
    "\\]\n",
    "\n",
    "where \\(p^* = \\max_k p_k\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29133115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class BanditRunResult:\n",
    "    rewards: np.ndarray\n",
    "    arms: np.ndarray\n",
    "    cum_reward: np.ndarray\n",
    "    cum_regret: np.ndarray\n",
    "\n",
    "\n",
    "def run_bandit(\n",
    "    env: BanditEnv,\n",
    "    policy,\n",
    "    T: int,\n",
    ") -> BanditRunResult:\n",
    "    \"\"\"Simulate a single bandit run.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : BanditEnv\n",
    "        Bandit environment with true probabilities.\n",
    "    policy : object\n",
    "        Policy object with methods `select_arm()` and `update(arm, reward)`.\n",
    "    T : int\n",
    "        Number of steps / pulls.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BanditRunResult\n",
    "        Rewards, arms, and cumulative reward/regret over time.\n",
    "    \"\"\"\n",
    "    K = env.probs.shape[0]\n",
    "    best_prob = float(np.max(env.probs))\n",
    "\n",
    "    rewards = np.zeros(T, dtype=float)\n",
    "    arms = np.zeros(T, dtype=int)\n",
    "    cum_reward = np.zeros(T, dtype=float)\n",
    "    cum_regret = np.zeros(T, dtype=float)\n",
    "\n",
    "    total = 0.0\n",
    "    for t in range(T):\n",
    "        arm = policy.select_arm()\n",
    "        r = env.pull(arm)\n",
    "        policy.update(arm, r)\n",
    "\n",
    "        total += r\n",
    "        rewards[t] = r\n",
    "        arms[t] = arm\n",
    "        cum_reward[t] = total\n",
    "        cum_regret[t] = (t + 1) * best_prob - total\n",
    "\n",
    "    return BanditRunResult(\n",
    "        rewards=rewards,\n",
    "        arms=arms,\n",
    "        cum_reward=cum_reward,\n",
    "        cum_regret=cum_regret,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef020977",
   "metadata": {},
   "source": [
    "\n",
    "### Example: single run per policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d3f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = make_example_env(seed=123)\n",
    "T = 5000\n",
    "\n",
    "policies = {\n",
    "    \"greedy\": GreedyPolicy(env.probs.shape[0]),\n",
    "    \"eps_greedy_0.1\": EpsilonGreedyPolicy(env.probs.shape[0], epsilon=0.1, seed=1),\n",
    "    \"ucb1\": UCB1Policy(env.probs.shape[0]),\n",
    "    \"thompson\": ThompsonSamplingPolicy(env.probs.shape[0], alpha0=1.0, beta0=1.0, seed=2),\n",
    "}\n",
    "\n",
    "results_single: Dict[str, BanditRunResult] = {}\n",
    "\n",
    "for name, pol in policies.items():\n",
    "    env_run = make_example_env(seed=123)  # fresh env for each policy\n",
    "    res = run_bandit(env_run, pol, T=T)\n",
    "    results_single[name] = res\n",
    "\n",
    "# Plot cumulative regret for one run\n",
    "plt.figure()\n",
    "for name, res in results_single.items():\n",
    "    plt.plot(res.cum_regret, label=name)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "plt.title(\"Single-run cumulative regret per policy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466fc86",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Multiple runs: average regret and arm allocation\n",
    "\n",
    "Single runs can be noisy. To compare policies more robustly, we average over many runs:\n",
    "\n",
    "- Repeat the simulation `n_runs` times per policy.  \n",
    "- Compute the **mean cumulative regret** across runs.  \n",
    "- Optionally analyze how often each arm was played.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf7c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_many(\n",
    "    env_probas: np.ndarray,\n",
    "    policy_factory: Callable[[], Any],\n",
    "    T: int,\n",
    "    n_runs: int,\n",
    "    seed: int | None = 999,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run a bandit policy multiple times and collect statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env_probas : np.ndarray\n",
    "        True arm probabilities for the environment.\n",
    "    policy_factory : Callable[[], Any]\n",
    "        Function that returns a fresh policy instance.\n",
    "    T : int\n",
    "        Number of steps per run.\n",
    "    n_runs : int\n",
    "        Number of independent runs.\n",
    "    seed : int | None\n",
    "        Seed for the RNG used for environments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Contains:\n",
    "        - 'mean_cum_regret': np.ndarray\n",
    "        - 'mean_cum_reward': np.ndarray\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    K = env_probas.shape[0]\n",
    "    best_prob = float(np.max(env_probas))\n",
    "\n",
    "    cum_regrets = np.zeros((n_runs, T), dtype=float)\n",
    "    cum_rewards = np.zeros((n_runs, T), dtype=float)\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        # new env RNG seed for each run\n",
    "        s = int(rng.integers(0, 1_000_000))\n",
    "        env = BanditEnv(probs=env_probas.copy(), rng=np.random.default_rng(s))\n",
    "        policy = policy_factory()\n",
    "\n",
    "        res = run_bandit(env, policy, T=T)\n",
    "        cum_regrets[i, :] = res.cum_regret\n",
    "        cum_rewards[i, :] = res.cum_reward\n",
    "\n",
    "    return {\n",
    "        \"mean_cum_regret\": np.mean(cum_regrets, axis=0),\n",
    "        \"mean_cum_reward\": np.mean(cum_rewards, axis=0),\n",
    "    }\n",
    "\n",
    "\n",
    "probs = make_example_env().probs\n",
    "T = 5000\n",
    "n_runs = 50\n",
    "\n",
    "summary_multi: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "\n",
    "summary_multi[\"greedy\"] = run_many(\n",
    "    env_probas=probs,\n",
    "    policy_factory=lambda: GreedyPolicy(probs.shape[0]),\n",
    "    T=T,\n",
    "    n_runs=n_runs,\n",
    ")\n",
    "\n",
    "summary_multi[\"eps_greedy_0.1\"] = run_many(\n",
    "    env_probas=probs,\n",
    "    policy_factory=lambda: EpsilonGreedyPolicy(probs.shape[0], epsilon=0.1, seed=None),\n",
    "    T=T,\n",
    "    n_runs=n_runs,\n",
    ")\n",
    "\n",
    "summary_multi[\"ucb1\"] = run_many(\n",
    "    env_probas=probs,\n",
    "    policy_factory=lambda: UCB1Policy(probs.shape[0]),\n",
    "    T=T,\n",
    "    n_runs=n_runs,\n",
    ")\n",
    "\n",
    "summary_multi[\"thompson\"] = run_many(\n",
    "    env_probas=probs,\n",
    "    policy_factory=lambda: ThompsonSamplingPolicy(probs.shape[0], alpha0=1.0, beta0=1.0, seed=None),\n",
    "    T=T,\n",
    "    n_runs=n_runs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fdd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot mean cumulative regret over runs\n",
    "plt.figure()\n",
    "for name, summ in summary_multi.items():\n",
    "    plt.plot(summ[\"mean_cum_regret\"], label=name)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"mean cumulative regret\")\n",
    "plt.title(f\"Average cumulative regret over {n_runs} runs\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dcf385",
   "metadata": {},
   "source": [
    "\n",
    "In many settings you should see:\n",
    "\n",
    "- **Greedy** can get stuck on suboptimal arms if early noise is unlucky.  \n",
    "- **\\(\\varepsilon\\)-greedy** keeps exploring and eventually finds the best arm, but exploration is constant.  \n",
    "- **UCB1** and **Thompson Sampling** often achieve **lower regret**, especially as T grows.\n",
    "\n",
    "This connects back to experimentation:\n",
    "- A/B tests are like **fixed, non-adaptive** assignments.  \n",
    "- Bandits are **adaptive**, assigning more traffic to better variants over time.\n",
    "\n",
    "In practice you often mix both:\n",
    "\n",
    "- Use bandits to **explore and optimize**.  \n",
    "- Use hold-out A/B tests to **get clean causal estimates** and validate big launches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbc883",
   "metadata": {},
   "source": [
    "\n",
    "## 5) How to reuse this in your projects\n",
    "\n",
    "Ideas:\n",
    "\n",
    "1. Replace the Bernoulli reward with:\n",
    "   - click-through, sign-up, or purchase indicators from a simulator.  \n",
    "   - synthetic revenue samples per arm.\n",
    "\n",
    "2. Wrap policies in a small service interface:\n",
    "   - keep `counts`, `successes`, or Beta parameters in a store.  \n",
    "   - periodically update based on new events.\n",
    "\n",
    "3. Combine with your **sequential / always-valid** tests:\n",
    "   - use bandits for daily routing decisions,  \n",
    "   - periodically run formal tests on logged data to make product decisions.\n",
    "\n",
    "This notebook is intentionally minimal, so you can use it as a starting point\n",
    "for a bandit playground or internal demos.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}