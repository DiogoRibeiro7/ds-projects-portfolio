{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de06e003",
   "metadata": {},
   "source": [
    "\n",
    "# Causal Off-Policy Evaluation for Targeting Policies (Policy Switch / Discounts)\n",
    "\n",
    "This notebook is a **playbook for off-policy evaluation (OPE)** in the context of targeting\n",
    "policies, e.g. **who should receive a discount or promo**.\n",
    "\n",
    "We assume you have **logged data** from some **logging policy** (a previous strategy) and\n",
    "you want to estimate what would happen under a **new policy** without deploying it yet.\n",
    "\n",
    "We cover:\n",
    "\n",
    "1. Simulating logged contextual bandit data for a **discount policy**.  \n",
    "2. Defining a **logging policy** and one or more **target policies**.  \n",
    "3. Off-policy estimators:\n",
    "   - Inverse propensity weighting (IPW / importance sampling).  \n",
    "   - Self-normalized IPW (SNIPW).  \n",
    "   - Doubly-robust (DR) estimator with a learned reward model \\(Q(x, a)\\).  \n",
    "4. A **policy switch** example:\n",
    "   - Current discount policy A (logging).  \n",
    "   - Candidate policy B (tighter targeting).  \n",
    "   - Use OPE to estimate **counterfactual lift** in expected reward.\n",
    "\n",
    "Everything here is structured so you can swap in your own feature matrix `X`, actions, propensities,\n",
    "and rewards from real logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f712e3",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa47676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Any, Tuple\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "try:\n",
    "    from sklearn.linear_model import LogisticRegression  # type: ignore\n",
    "except Exception as e:  # pragma: no cover\n",
    "    LogisticRegression = None\n",
    "    print(\"scikit-learn not available; DR with logistic Q will be skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec07c8d",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Simulated logged bandit data for a discount policy\n",
    "\n",
    "We simulate **contextual bandit** style logs for a simple discount decision:\n",
    "\n",
    "- Context features: `X ∈ R^d` (e.g., engagement, value, device).  \n",
    "- Action `A ∈ {0,1}`:\n",
    "  - `A = 1` ⇒ user receives a discount.  \n",
    "  - `A = 0` ⇒ no discount.  \n",
    "- Reward `R`:\n",
    "  - Here we take `R = revenue`, which depends on the user and whether they get a discount.\n",
    "\n",
    "We also log the **propensity** of the logging policy:\n",
    "\n",
    "\\[\n",
    "\\pi_b(a \\mid x) = \\mathbb{P}(A = a \\mid X = x, \\text{logging policy}).\n",
    "\\]\n",
    "\n",
    "The dataset will contain:\n",
    "\n",
    "- `x1, x2, ..., xd` — context features.  \n",
    "- `action` — 0 or 1.  \n",
    "- `propensity` — probability of the action chosen by the logging policy.  \n",
    "- `revenue` — realized reward under the taken action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b588e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class LoggedBanditData:\n",
    "    \"\"\"Container for simulated logged bandit data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Logged data with context, action, propensity, and reward.\n",
    "    true_model_params : Dict[str, Any]\n",
    "        Parameters of the generative model (for ground-truth evaluation).\n",
    "    \"\"\"\n",
    "    df: pd.DataFrame\n",
    "    true_model_params: Dict[str, Any]\n",
    "\n",
    "\n",
    "def simulate_logging_policy_data(\n",
    "    n: int = 50_000,\n",
    "    d: int = 3,\n",
    "    seed: int | None = 123,\n",
    ") -> LoggedBanditData:\n",
    "    \"\"\"Simulate logged data for a discount policy.\n",
    "\n",
    "    The generative model is:\n",
    "\n",
    "    - Context X ~ N(0, I).\n",
    "    - Logging policy probability of discount:\n",
    "        pi_b(1 | x) = sigmoid( beta0 + beta^T x ).\n",
    "    - Baseline purchase probability (no discount):\n",
    "        p0(x) = sigmoid( alpha0 + alpha^T x ).\n",
    "    - Discount uplift on purchase probability: delta > 0.\n",
    "    - Revenue:\n",
    "        - Base price = 100.\n",
    "        - If purchase:\n",
    "            - No discount: revenue = 100.\n",
    "            - Discount: revenue = (1 - discount_rate) * 100.\n",
    "        - If no purchase: revenue = 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of logged interactions.\n",
    "    d : int\n",
    "        Number of context features.\n",
    "    seed : int | None\n",
    "        Random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    LoggedBanditData\n",
    "        DataFrame with context, action, propensity, revenue, plus model params.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Context features\n",
    "    X = rng.normal(size=(n, d))\n",
    "    feature_cols = [f\"x{j+1}\" for j in range(d)]\n",
    "\n",
    "    # Logging policy parameters\n",
    "    beta0 = -0.1\n",
    "    beta = np.array([0.8, -0.4, 0.3][:d])\n",
    "\n",
    "    def sigmoid(z: np.ndarray | float) -> np.ndarray | float:\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    logit_pi = beta0 + X @ beta\n",
    "    pi_discount = sigmoid(logit_pi)  # pi_b(1 | x)\n",
    "\n",
    "    # Sample actions under logging policy\n",
    "    actions = rng.binomial(1, pi_discount)\n",
    "\n",
    "    # Outcome model parameters (true, unknown in practice)\n",
    "    alpha0 = -1.0\n",
    "    alpha = np.array([0.5, 0.3, -0.2][:d])\n",
    "    discount_uplift = 0.06  # absolute uplift in purchase prob when discounted\n",
    "    discount_rate = 0.20    # 20% off price\n",
    "\n",
    "    base_price = 100.0\n",
    "\n",
    "    # Baseline purchase probability without discount\n",
    "    logit_p0 = alpha0 + X @ alpha\n",
    "    p0 = sigmoid(logit_p0)\n",
    "\n",
    "    # Purchase probability under chosen action\n",
    "    p_purchase = np.where(actions == 1, np.clip(p0 + discount_uplift, 0.0, 1.0), p0)\n",
    "\n",
    "    # Realized purchases\n",
    "    purchases = rng.binomial(1, p_purchase)\n",
    "\n",
    "    # Revenue: 0 if no purchase; price or discounted price if purchase\n",
    "    revenue = np.where(\n",
    "        purchases == 1,\n",
    "        np.where(actions == 1, (1.0 - discount_rate) * base_price, base_price),\n",
    "        0.0,\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(X, columns=feature_cols)\n",
    "    df[\"action\"] = actions.astype(int)\n",
    "    df[\"propensity\"] = np.where(actions == 1, pi_discount, 1.0 - pi_discount)\n",
    "    df[\"purchase\"] = purchases.astype(int)\n",
    "    df[\"revenue\"] = revenue.astype(float)\n",
    "\n",
    "    params = {\n",
    "        \"alpha0\": alpha0,\n",
    "        \"alpha\": alpha,\n",
    "        \"discount_uplift\": discount_uplift,\n",
    "        \"discount_rate\": discount_rate,\n",
    "        \"base_price\": base_price,\n",
    "        \"beta0\": beta0,\n",
    "        \"beta\": beta,\n",
    "    }\n",
    "\n",
    "    return LoggedBanditData(df=df, true_model_params=params)\n",
    "\n",
    "\n",
    "logged_data = simulate_logging_policy_data()\n",
    "logged_data.df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639855a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logged_data.df[[\"action\", \"propensity\", \"purchase\", \"revenue\"]].describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e86bd7",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Logging policy vs target policies\n",
    "\n",
    "We represent a **policy** as a function \\(\\pi(a \\mid x)\\) that returns, for each action `a`,\n",
    "its probability given context `x`.\n",
    "\n",
    "For simplicity we will focus on **binary actions** `a ∈ {0,1}` (no discount / discount), and define:\n",
    "\n",
    "- Logging policy \\(\\pi_b\\) (used to generate the data): known only through the logged `propensity`.  \n",
    "- Target policies \\(\\pi_e\\) (evaluation policies):\n",
    "  - `policy_never_discount`: A = 0 always.  \n",
    "  - `policy_always_discount`: A = 1 always.  \n",
    "  - `policy_score_threshold`: discount only when a simple score is above 0.\n",
    "\n",
    "We will evaluate these target policies using the logged data from the logging policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc07e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_never_discount(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Deterministic policy: never apply discount (action=0).\n",
    "\n",
    "    Returns a 2-vector of probabilities [P(A=0), P(A=1)] for each row in x.\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    pi = np.zeros((n, 2), dtype=float)\n",
    "    pi[:, 0] = 1.0\n",
    "    return pi\n",
    "\n",
    "\n",
    "def policy_always_discount(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Deterministic policy: always apply discount (action=1).\"\"\"\n",
    "    n = x.shape[0]\n",
    "    pi = np.zeros((n, 2), dtype=float)\n",
    "    pi[:, 1] = 1.0\n",
    "    return pi\n",
    "\n",
    "\n",
    "def policy_score_threshold(x: np.ndarray, w: np.ndarray | None = None, threshold: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"Deterministic policy: discount if score = w^T x >= threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Feature matrix of shape (n, d).\n",
    "    w : np.ndarray | None\n",
    "        Weight vector; if None, use a simple default [1, -1, 0,...].\n",
    "    threshold : float\n",
    "        Score threshold for discounting.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of shape (n, 2) with [P(A=0), P(A=1)] per row.\n",
    "    \"\"\"\n",
    "    n, d = x.shape\n",
    "    if w is None:\n",
    "        w_vec = np.zeros(d)\n",
    "        w_vec[0] = 1.0\n",
    "        if d > 1:\n",
    "            w_vec[1] = -1.0\n",
    "    else:\n",
    "        w_vec = np.asarray(w, dtype=float)\n",
    "        if w_vec.shape[0] != d:\n",
    "            raise ValueError(\"w must have same dimension as x columns.\")\n",
    "\n",
    "    scores = x @ w_vec\n",
    "    discount_flag = (scores >= threshold).astype(int)\n",
    "\n",
    "    pi = np.zeros((n, 2), dtype=float)\n",
    "    pi[np.arange(n), discount_flag] = 1.0\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750d09c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Off-policy value estimators\n",
    "\n",
    "We want to estimate the **expected reward** of a target policy \\(\\pi_e\\) using data\n",
    "collected under the logging policy \\(\\pi_b\\).\n",
    "\n",
    "For each logged interaction we have:\n",
    "\n",
    "- Context: `X_i`.  \n",
    "- Action taken by logging policy: `A_i`.  \n",
    "- Reward: `R_i`.  \n",
    "- Logging propensity: `p_i = \\pi_b(A_i \\mid X_i)`.\n",
    "\n",
    "Given a target policy \\(\\pi_e\\), we define \\(\\pi_e(A_i \\mid X_i)\\) accordingly (often either 0 or 1).\n",
    "\n",
    "We implement:\n",
    "\n",
    "1. **IPW (importance sampling)** estimator:\n",
    "   \\[\n",
    "   \\hat V_{\\text{IPW}} =\n",
    "   \\frac{1}{n} \\sum_{i=1}^n w_i R_i, \\quad\n",
    "   w_i = \\frac{\\pi_e(A_i \\mid X_i)}{\\pi_b(A_i \\mid X_i)}.\n",
    "   \\]\n",
    "\n",
    "2. **Self-normalized IPW (SNIPW)**:\n",
    "   \\[\n",
    "   \\hat V_{\\text{SNIPW}} =\n",
    "   \\frac{\\sum_i w_i R_i}{\\sum_i w_i}.\n",
    "   \\]\n",
    "\n",
    "3. **Doubly-robust (DR)** estimator (requires an estimate of the conditional mean reward \\(Q(x,a)\\)):\n",
    "   \\[\n",
    "   \\hat V_{\\text{DR}} =\n",
    "   \\frac{1}{n}\\sum_i \\left(\n",
    "      \\sum_a \\pi_e(a \\mid X_i) \\hat Q(X_i, a)\n",
    "      + \\frac{\\pi_e(A_i \\mid X_i)}{\\pi_b(A_i \\mid X_i)} (R_i - \\hat Q(X_i, A_i))\n",
    "   \\right).\n",
    "   \\]\n",
    "\n",
    "If either the propensities or the Q-model is correct, DR is (asymptotically) unbiased.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f26bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_policy_value_ipw(\n",
    "    reward: np.ndarray,\n",
    "    action: np.ndarray,\n",
    "    logging_propensity: np.ndarray,\n",
    "    target_policy_prob: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"Estimate policy value via (un-normalized) IPW.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reward : np.ndarray\n",
    "        Rewards R_i.\n",
    "    action : np.ndarray\n",
    "        Actions A_i in {0,1}.\n",
    "    logging_propensity : np.ndarray\n",
    "        Logging probabilities pi_b(A_i | X_i).\n",
    "    target_policy_prob : np.ndarray\n",
    "        Target probabilities pi_e(A_i | X_i).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        IPW estimate of expected reward under pi_e.\n",
    "    \"\"\"\n",
    "    reward = np.asarray(reward, dtype=float)\n",
    "    action = np.asarray(action, dtype=int)\n",
    "    logging_propensity = np.asarray(logging_propensity, dtype=float)\n",
    "    target_policy_prob = np.asarray(target_policy_prob, dtype=float)\n",
    "\n",
    "    if reward.shape != action.shape or reward.shape != logging_propensity.shape:\n",
    "        raise ValueError(\"reward, action, logging_propensity must have same shape.\")\n",
    "    if reward.shape != target_policy_prob.shape:\n",
    "        raise ValueError(\"target_policy_prob must have same shape as reward.\")\n",
    "\n",
    "    eps = 1e-8\n",
    "    w = target_policy_prob / np.clip(logging_propensity, eps, None)\n",
    "    return float(np.mean(w * reward))\n",
    "\n",
    "\n",
    "def estimate_policy_value_snipw(\n",
    "    reward: np.ndarray,\n",
    "    action: np.ndarray,\n",
    "    logging_propensity: np.ndarray,\n",
    "    target_policy_prob: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"Estimate policy value via self-normalized IPW (SNIPW).\"\"\"\n",
    "    reward = np.asarray(reward, dtype=float)\n",
    "    logging_propensity = np.asarray(logging_propensity, dtype=float)\n",
    "    target_policy_prob = np.asarray(target_policy_prob, dtype=float)\n",
    "\n",
    "    eps = 1e-8\n",
    "    w = target_policy_prob / np.clip(logging_propensity, eps, None)\n",
    "    w_sum = np.sum(w)\n",
    "    if w_sum <= 0.0:\n",
    "        raise ValueError(\"Sum of weights is non-positive.\")\n",
    "    return float(np.sum(w * reward) / w_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_policy_value_dr(\n",
    "    reward: np.ndarray,\n",
    "    action: np.ndarray,\n",
    "    logging_propensity: np.ndarray,\n",
    "    target_pi: np.ndarray,\n",
    "    q_hat: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"Doubly-robust policy value estimator for binary actions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reward : np.ndarray\n",
    "        Rewards R_i.\n",
    "    action : np.ndarray\n",
    "        Actions A_i (0 or 1).\n",
    "    logging_propensity : np.ndarray\n",
    "        Logging probabilities pi_b(A_i | X_i).\n",
    "    target_pi : np.ndarray\n",
    "        Target policy probabilities pi_e(a | X_i) of shape (n, 2).\n",
    "    q_hat : np.ndarray\n",
    "        Estimated Q(X_i, a) for a=0,1, shape (n, 2).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        DR estimate of expected reward under pi_e.\n",
    "    \"\"\"\n",
    "    reward = np.asarray(reward, dtype=float)\n",
    "    action = np.asarray(action, dtype=int)\n",
    "    logging_propensity = np.asarray(logging_propensity, dtype=float)\n",
    "    target_pi = np.asarray(target_pi, dtype=float)\n",
    "    q_hat = np.asarray(q_hat, dtype=float)\n",
    "\n",
    "    n = reward.shape[0]\n",
    "    if target_pi.shape != (n, 2) or q_hat.shape != (n, 2):\n",
    "        raise ValueError(\"target_pi and q_hat must have shape (n, 2).\")\n",
    "\n",
    "    eps = 1e-8\n",
    "    # Q_bar_i = sum_a pi_e(a|Xi) Q_hat(Xi, a)\n",
    "    Q_bar = np.sum(target_pi * q_hat, axis=1)\n",
    "\n",
    "    # Q_hat at taken action\n",
    "    q_taken = q_hat[np.arange(n), action]\n",
    "\n",
    "    # Importance weights for taken actions\n",
    "    pi_e_taken = target_pi[np.arange(n), action]\n",
    "    w = pi_e_taken / np.clip(logging_propensity, eps, None)\n",
    "\n",
    "    dr_terms = Q_bar + w * (reward - q_taken)\n",
    "    return float(np.mean(dr_terms))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b540f",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Fitting a reward model \\(\\hat Q(x,a)\\)\n",
    "\n",
    "To use the DR estimator we need an estimate \\(\\hat Q(x,a)\\) of the conditional mean reward.\n",
    "\n",
    "Here we fit a **logistic regression** on `purchase` and then convert it into expected revenue\n",
    "under each action:\n",
    "\n",
    "\\[\n",
    "\\hat Q(x,a) = \\hat p_\\text{purchase}(x,a) \\times \\text{price}(a).\n",
    "\\]\n",
    "\n",
    "If `scikit-learn` is not available, this step is skipped and DR estimates are not computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d48cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LogisticRegression is None:\n",
    "    print(\"sklearn not available; skipping Q-model fit.\")\n",
    "else:\n",
    "    df_logs = logged_data.df.copy()\n",
    "    feature_cols = [c for c in df_logs.columns if c.startswith(\"x\")]\n",
    "\n",
    "    X = df_logs[feature_cols].to_numpy()\n",
    "    A = df_logs[\"action\"].to_numpy()\n",
    "    y = df_logs[\"purchase\"].to_numpy()\n",
    "\n",
    "    # Simple encoding: concatenate X and action as a feature\n",
    "    X_aug = np.hstack([X, A.reshape(-1, 1)])\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_aug, y)\n",
    "\n",
    "    # Extract pricing parameters from true model for Q mapping\n",
    "    base_price = logged_data.true_model_params[\"base_price\"]\n",
    "    discount_rate = logged_data.true_model_params[\"discount_rate\"]\n",
    "\n",
    "    def q_hat_fn(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute Q_hat(x,a) for a=0,1 based on the fitted purchase model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Feature matrix of shape (n, d).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Q_hat of shape (n, 2) with expected revenue for each action.\n",
    "        \"\"\"\n",
    "        n = x.shape[0]\n",
    "        # For a=0 (no discount)\n",
    "        X0 = np.hstack([x, np.zeros((n, 1))])\n",
    "        p0 = clf.predict_proba(X0)[:, 1]\n",
    "        rev0 = p0 * base_price\n",
    "\n",
    "        # For a=1 (discount)\n",
    "        X1 = np.hstack([x, np.ones((n, 1))])\n",
    "        p1 = clf.predict_proba(X1)[:, 1]\n",
    "        rev1 = p1 * ((1.0 - discount_rate) * base_price)\n",
    "\n",
    "        return np.stack([rev0, rev1], axis=1)\n",
    "\n",
    "    # Precompute Q_hat on the logged contexts for DR\n",
    "    X_logs = df_logs[feature_cols].to_numpy()\n",
    "    q_hat_logs = q_hat_fn(X_logs)\n",
    "    q_hat_logs[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac532c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Off-policy evaluation of several policies\n",
    "\n",
    "We now define a small helper that, given logged data and a target policy, computes:\n",
    "\n",
    "- IPW and SNIPW estimates.  \n",
    "- DR estimate (if the Q-model \\(\\hat Q\\) is available).\n",
    "\n",
    "We then compare several policies:\n",
    "\n",
    "- Logging policy (estimated in a trivial way for reference).  \n",
    "- Never discount.  \n",
    "- Always discount.  \n",
    "- Score-threshold policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df266228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_policy_offline(\n",
    "    df_logs: pd.DataFrame,\n",
    "    policy_name: str,\n",
    "    policy_fn: Callable[[np.ndarray], np.ndarray],\n",
    "    q_hat_logs: np.ndarray | None = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate a target policy using IPW, SNIPW, and (optionally) DR.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_logs : DataFrame\n",
    "        Logged data with features x*, action, propensity, reward.\n",
    "    policy_name : str\n",
    "        Name of the policy (for reporting only).\n",
    "    policy_fn : Callable[[np.ndarray], np.ndarray]\n",
    "        Function mapping feature matrix X to policy probabilities of shape (n, 2).\n",
    "    q_hat_logs : np.ndarray | None\n",
    "        Estimated Q(X_i, a) for a=0,1. If None, DR is skipped.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Policy name and available value estimates.\n",
    "    \"\"\"\n",
    "    feature_cols = [c for c in df_logs.columns if c.startswith(\"x\")]\n",
    "    X = df_logs[feature_cols].to_numpy()\n",
    "    A = df_logs[\"action\"].to_numpy()\n",
    "    R = df_logs[\"revenue\"].to_numpy()\n",
    "    p_b = df_logs[\"propensity\"].to_numpy()\n",
    "\n",
    "    # Target policy probabilities at taken actions\n",
    "    pi_e = policy_fn(X)\n",
    "    pi_e_taken = pi_e[np.arange(X.shape[0]), A]\n",
    "\n",
    "    est_ipw = estimate_policy_value_ipw(R, A, p_b, pi_e_taken)\n",
    "    est_snipw = estimate_policy_value_snipw(R, A, p_b, pi_e_taken)\n",
    "\n",
    "    results: Dict[str, float] = {\n",
    "        \"policy\": policy_name,\n",
    "        \"IPW\": est_ipw,\n",
    "        \"SNIPW\": est_snipw,\n",
    "    }\n",
    "\n",
    "    if q_hat_logs is not None:\n",
    "        est_dr = estimate_policy_value_dr(\n",
    "            reward=R,\n",
    "            action=A,\n",
    "            logging_propensity=p_b,\n",
    "            target_pi=pi_e,\n",
    "            q_hat=q_hat_logs,\n",
    "        )\n",
    "        results[\"DR\"] = est_dr\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "df_logs = logged_data.df\n",
    "\n",
    "# Evaluate several policies\n",
    "results = []\n",
    "\n",
    "# Approximate \"value\" of logging policy by on-policy empirical average\n",
    "logging_value = float(df_logs[\"revenue\"].mean())\n",
    "results.append({\"policy\": \"logging_empirical\", \"IPW\": logging_value, \"SNIPW\": logging_value})\n",
    "\n",
    "# Never discount\n",
    "results.append(\n",
    "    evaluate_policy_offline(df_logs, \"never_discount\", policy_never_discount, q_hat_logs if 'q_hat_logs' in globals() else None)\n",
    ")\n",
    "\n",
    "# Always discount\n",
    "results.append(\n",
    "    evaluate_policy_offline(df_logs, \"always_discount\", policy_always_discount, q_hat_logs if 'q_hat_logs' in globals() else None)\n",
    ")\n",
    "\n",
    "# Score-threshold policy\n",
    "feature_cols = [c for c in df_logs.columns if c.startswith(\"x\")]\n",
    "X_tmp = df_logs[feature_cols].to_numpy()\n",
    "d = X_tmp.shape[1]\n",
    "w_default = np.zeros(d)\n",
    "w_default[0] = 1.0\n",
    "if d > 1:\n",
    "    w_default[1] = -1.0\n",
    "\n",
    "def policy_threshold_local(x: np.ndarray) -> np.ndarray:\n",
    "    return policy_score_threshold(x, w=w_default, threshold=0.0)\n",
    "\n",
    "results.append(\n",
    "    evaluate_policy_offline(df_logs, \"score_threshold\", policy_threshold_local, q_hat_logs if 'q_hat_logs' in globals() else None)\n",
    ")\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb61607",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Ground-truth policy values via simulation\n",
    "\n",
    "In real life, we **do not know** the true outcome model. Here, because we simulated the\n",
    "data, we *do* know it and can approximate the **ground-truth value** of any policy by\n",
    "forward simulation.\n",
    "\n",
    "We use the same generative process as in `simulate_logging_policy_data`, but instead of\n",
    "choosing actions from the logging policy, we sample from the **target policy**, and then\n",
    "generate reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d742c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_policy_value_true(\n",
    "    n: int,\n",
    "    d: int,\n",
    "    target_policy: Callable[[np.ndarray], np.ndarray],\n",
    "    params: Dict[str, Any],\n",
    "    seed: int | None = 999,\n",
    ") -> float:\n",
    "    \"\"\"Approximate the true expected revenue of a policy via simulation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of simulated users.\n",
    "    d : int\n",
    "        Number of context features.\n",
    "    target_policy : Callable[[np.ndarray], np.ndarray]\n",
    "        Policy mapping X to pi_e(a | X) of shape (n, 2).\n",
    "    params : dict\n",
    "        Generative parameters from simulate_logging_policy_data (alpha, beta, etc.).\n",
    "    seed : int | None\n",
    "        Random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Monte Carlo estimate of expected revenue under the policy.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Unpack parameters\n",
    "    alpha0 = params[\"alpha0\"]\n",
    "    alpha = params[\"alpha\"]\n",
    "    discount_uplift = params[\"discount_uplift\"]\n",
    "    discount_rate = params[\"discount_rate\"]\n",
    "    base_price = params[\"base_price\"]\n",
    "\n",
    "    def sigmoid(z: np.ndarray | float) -> np.ndarray | float:\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    # Sample contexts\n",
    "    X = rng.normal(size=(n, d))\n",
    "\n",
    "    # Sample actions from target policy\n",
    "    pi_e = target_policy(X)\n",
    "    # For deterministic policies, pi_e is 0 or 1; we still sample from it (for generality)\n",
    "    actions = np.array(\n",
    "        [rng.choice([0, 1], p=pi_e[i]) for i in range(n)],\n",
    "        dtype=int,\n",
    "    )\n",
    "\n",
    "    # Baseline purchase probability (no discount)\n",
    "    logit_p0 = alpha0 + X @ alpha\n",
    "    p0 = sigmoid(logit_p0)\n",
    "\n",
    "    # Purchase probability under chosen action\n",
    "    p_purchase = np.where(actions == 1, np.clip(p0 + discount_uplift, 0.0, 1.0), p0)\n",
    "\n",
    "    purchases = rng.binomial(1, p_purchase)\n",
    "    revenue = np.where(\n",
    "        purchases == 1,\n",
    "        np.where(actions == 1, (1.0 - discount_rate) * base_price, base_price),\n",
    "        0.0,\n",
    "    )\n",
    "\n",
    "    return float(np.mean(revenue))\n",
    "\n",
    "\n",
    "# Estimate true values for the same set of policies\n",
    "d_features = len([c for c in df_logs.columns if c.startswith(\"x\")])\n",
    "params = logged_data.true_model_params\n",
    "\n",
    "true_results = []\n",
    "\n",
    "true_results.append(\n",
    "    {\n",
    "        \"policy\": \"never_discount_true\",\n",
    "        \"true_value\": simulate_policy_value_true(\n",
    "            n=50_000,\n",
    "            d=d_features,\n",
    "            target_policy=policy_never_discount,\n",
    "            params=params,\n",
    "            seed=1,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "true_results.append(\n",
    "    {\n",
    "        \"policy\": \"always_discount_true\",\n",
    "        \"true_value\": simulate_policy_value_true(\n",
    "            n=50_000,\n",
    "            d=d_features,\n",
    "            target_policy=policy_always_discount,\n",
    "            params=params,\n",
    "            seed=2,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "true_results.append(\n",
    "    {\n",
    "        \"policy\": \"score_threshold_true\",\n",
    "        \"true_value\": simulate_policy_value_true(\n",
    "            n=50_000,\n",
    "            d=d_features,\n",
    "            target_policy=policy_threshold_local,\n",
    "            params=params,\n",
    "            seed=3,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "pd.DataFrame(true_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da0cd5",
   "metadata": {},
   "source": [
    "\n",
    "By comparing OPE estimates to these **true values**, we can see:\n",
    "\n",
    "- How biased / noisy the naive empirical logging value is.  \n",
    "- How IPW / SNIPW / DR perform for different target policies.  \n",
    "- Whether the **policy ranking** is correctly recovered (e.g., score-threshold > always-discount > never-discount).\n",
    "\n",
    "In real data, you would not have access to truth, but you can still rely on the DR estimator\n",
    "for better robustness, especially when you have many features and potentially misspecified\n",
    "logging propensities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a17ea",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Policy switch example: from logging policy A to candidate B\n",
    "\n",
    "Imagine:\n",
    "\n",
    "- Policy A (current) is roughly like the **logging policy**: it tends to discount more\n",
    "  often for high-value users, but is somewhat noisy.  \n",
    "- Policy B (candidate) is the **score-threshold** policy: we discount only high-score users,\n",
    "  never low-score users.\n",
    "\n",
    "Using this notebook, you can:\n",
    "\n",
    "1. Treat the logging policy's on-policy value as the estimate for A.  \n",
    "2. Use OPE (IPW/SNIPW/DR) with the same logs to estimate the effect of switching to B.  \n",
    "3. Compare `value_B_est - value_A_est` as the **counterfactual uplift**.\n",
    "\n",
    "In real pipelines, you would:\n",
    "\n",
    "- Log `propensity`, `action`, `reward`, and rich context for all users under A.  \n",
    "- Offline, define a candidate B and run these estimators.  \n",
    "- If uplift is promising and robust, run a **smaller online A/B test** to confirm before full rollout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009db9c1",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Practical notes\n",
    "\n",
    "Key points for production use:\n",
    "\n",
    "1. **Logging everything**  \n",
    "   - You *must* log `propensity` (or enough info to reconstruct it), not only the chosen action.  \n",
    "   - Log context features you may need for future policies or Q-models.\n",
    "\n",
    "2. **Overlap / support**  \n",
    "   - OPE relies on \\(\\pi_e(a \\mid x) > 0\\) only when \\(\\pi_b(a \\mid x) > 0\\).  \n",
    "   - If the new policy takes actions rarely or never taken by the logging policy in some regions,\n",
    "     importance weights blow up and estimates become unstable.\n",
    "\n",
    "3. **DR as default**  \n",
    "   - In many real settings, DR is a good default:  \n",
    "     - use a high-quality model for \\(Q(x,a)\\),  \n",
    "     - ensure propensity estimates are reasonable.\n",
    "\n",
    "4. **Uncertainty**  \n",
    "   - For decisions, you will want **confidence intervals / bootstrap** around these estimators,\n",
    "     or Bayesian analogues.  \n",
    "   - This notebook focuses on point estimates; adding intervals is a straightforward extension.\n",
    "\n",
    "5. **Policy iteration**  \n",
    "   - Off-policy evaluation lets you iterate on policies cheaply.  \n",
    "   - But large changes should still be validated with a proper online experiment (A/B or bandit)\n",
    "     before global rollout.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}