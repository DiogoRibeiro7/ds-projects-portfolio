{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Telco Churn \u2013 Model Monitoring and Drift Analysis (Simulated)\n\nThis notebook is the **third part** of the Telco churn project.\n\nFocus:\n1. Train a solid churn model (Random Forest) on the Telco dataset.\n2. **Simulate production monitoring** by splitting the test set into time windows.\n3. Track metrics over time: accuracy, ROC-AUC, churn rate, and predicted positive rate.\n4. Compute a simple **drift indicator** (Population Stability Index, PSI) for model scores.\n5. Discuss how these ideas translate into a real production setup.\n\n> This notebook is self-contained: it reloads and preprocesses the data from scratch.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and Configuration\n\nWe import:\n\n- `pandas`, `numpy` for data handling.\n- `matplotlib`, `seaborn` for visualization.\n- `scikit-learn` for preprocessing, modelling, and metrics.\n\nWe also set a fixed random seed for reproducibility.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    roc_auc_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_PATH: Path = Path(\"data\") / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Data file not found at {DATA_PATH.resolve()}. \"\n        \"Please download the Telco churn CSV from Kaggle and place it under the 'data/' directory.\"\n    )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data Loading and Cleaning\n\nWe repeat the same cleaning steps from the first notebook:\n\n1. Load the CSV.\n2. Convert `TotalCharges` to numeric.\n3. Drop rows with missing `TotalCharges`.\n4. Drop duplicate `customerID` entries.\n\nThis gives us a clean snapshot of Telco customers.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_telco_data(path: Path) -> pd.DataFrame:\n    \"\"\"Load the Telco churn data from CSV.\n\n    Args:\n        path: Path to the CSV file.\n\n    Returns:\n        DataFrame with raw Telco data.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the loaded DataFrame is empty.\n    \"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path!s}\")\n\n    df: pd.DataFrame = pd.read_csv(path)\n    if df.empty:\n        raise ValueError(f\"Loaded DataFrame is empty: {path!s}\")\n    return df\n\n\ndef clean_telco_data(raw_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Clean Telco data (types, missing values, duplicates).\n\n    Args:\n        raw_df: Raw Telco churn DataFrame.\n\n    Returns:\n        Cleaned DataFrame.\n    \"\"\"\n    df = raw_df.copy()\n\n    # Ensure the expected column exists\n    if \"TotalCharges\" not in df.columns:\n        raise ValueError(\"Expected 'TotalCharges' column not found.\")\n\n    # Convert TotalCharges to numeric, coercing errors to NaN\n    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n\n    # Show missing values\n    missing = df.isna().sum()\n    print(\"Missing values per column (non-zero only):\")\n    display(missing[missing > 0])\n\n    # Drop rows with missing TotalCharges\n    before = df.shape[0]\n    df = df.dropna(subset=[\"TotalCharges\"])\n    after = df.shape[0]\n    print(f\"Dropped {before - after} rows with missing TotalCharges.\")\n\n    # Drop duplicate customers\n    before = df.shape[0]\n    df = df.drop_duplicates(subset=[\"customerID\"])\n    after = df.shape[0]\n    print(f\"Dropped {before - after} duplicate customerID rows.\")\n\n    df = df.reset_index(drop=True)\n    return df\n\n\nraw_df = load_telco_data(DATA_PATH)\ntelco_df = clean_telco_data(raw_df)\ndisplay(telco_df.head())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe loaded and cleaned the Telco churn dataset. The resulting DataFrame contains\none row per customer with consistent numeric and categorical features.\n\nNext, we train a reference churn model that we will later monitor.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Train a Reference Churn Model\n\nWe:\n\n1. Map the target `Churn` to 0/1.\n2. Drop `customerID` from the features.\n3. Split into train and test sets (stratified).\n4. Build a preprocessing + model pipeline:\n   - Scale numeric features.\n   - One-hot encode categorical features.\n   - Train a `RandomForestClassifier` with reasonable defaults.\n\nThis model acts as our **production model** for the rest of the notebook.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "TARGET_COL: str = \"Churn\"\n\nif TARGET_COL not in telco_df.columns:\n    raise KeyError(f\"Target column {TARGET_COL!r} not found.\")\n\nX: pd.DataFrame = telco_df.drop(columns=[TARGET_COL, \"customerID\"])\ny: pd.Series = telco_df[TARGET_COL].map({\"No\": 0, \"Yes\": 1})\n\ncategorical_cols: List[str] = [c for c in X.columns if X[c].dtype == \"O\"]\nnumeric_cols: List[str] = [c for c in X.columns if c not in categorical_cols]\n\nprint(\"Categorical columns:\", categorical_cols)\nprint(\"Numeric columns:\", numeric_cols)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    stratify=y,\n    random_state=RANDOM_STATE,\n)\n\nprint(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n\nnumeric_transformer = Pipeline(\n    steps=[(\"scaler\", StandardScaler())]\n)\ncategorical_transformer = Pipeline(\n    steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_cols),\n        (\"cat\", categorical_transformer, categorical_cols),\n    ]\n)\n\nrf_pipeline = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\n            \"clf\",\n            RandomForestClassifier(\n                n_estimators=300,\n                max_depth=None,\n                min_samples_split=4,\n                min_samples_leaf=2,\n                random_state=RANDOM_STATE,\n                n_jobs=-1,\n            ),\n        ),\n    ]\n)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def evaluate_classifier(\n    name: str,\n    model: BaseEstimator,\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n) -> Dict[str, float]:\n    \"\"\"Fit a classifier and compute basic metrics on train and test.\n\n    Args:\n        name: Model name (for printing).\n        model: Unfitted sklearn estimator or pipeline.\n        X_train: Training features.\n        X_test: Test features.\n        y_train: Training labels (0/1).\n        y_test: Test labels (0/1).\n\n    Returns:\n        Dict with accuracy and ROC-AUC on the test set.\n    \"\"\"\n    print(f\"\\n===== {name} =====\")\n    model.fit(X_train, y_train)\n\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n\n    if hasattr(model, \"predict_proba\"):\n        y_proba_test = model.predict_proba(X_test)[:, 1]\n        roc_auc = roc_auc_score(y_test, y_proba_test)\n    else:\n        y_proba_test = None\n        roc_auc = np.nan\n\n    acc_train = accuracy_score(y_train, y_pred_train)\n    acc_test = accuracy_score(y_test, y_pred_test)\n\n    print(f\"Train accuracy: {acc_train:.3f}\")\n    print(f\"Test accuracy:  {acc_test:.3f}\")\n    if not np.isnan(roc_auc):\n        print(f\"Test ROC-AUC:  {roc_auc:.3f}\")\n\n    print(\"\\nClassification report (test):\")\n    print(classification_report(y_test, y_pred_test, target_names=[\"No churn\", \"Churn\"]))\n\n    cm = confusion_matrix(y_test, y_pred_test)\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\",\n        xticklabels=[\"Pred No\", \"Pred Yes\"],\n        yticklabels=[\"True No\", \"True Yes\"],\n    )\n    plt.title(f\"Confusion matrix - {name}\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.show()\n\n    return {\n        \"model\": name,\n        \"train_accuracy\": acc_train,\n        \"test_accuracy\": acc_test,\n        \"roc_auc\": float(roc_auc) if not np.isnan(roc_auc) else np.nan,\n    }\n\n\nbaseline_metrics = evaluate_classifier(\n    \"Random Forest (reference)\", rf_pipeline, X_train, X_test, y_train, y_test\n)\nbaseline_metrics\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe trained a **Random Forest churn model** and checked that its performance on\nthe test set is reasonable (decent ROC-AUC and recall for churners).\n\nNext, we will treat this model as if it were deployed and examine how to\n**monitor** it over time, using the test set as simulated production traffic.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Simulating Production Monitoring Windows\n\nReal monitoring is usually **time-based** (daily, weekly, monthly). The Telco\ndataset has no explicit timestamp, so we simulate time windows by:\n\n1. Shuffling the test set (fixed seed).\n2. Splitting it into consecutive **batches** (e.g. 10 windows).\n3. Computing metrics per batch:\n   - Accuracy\n   - ROC-AUC\n   - Churn rate in the data\n   - Predicted positive rate\n\nThis lets us build **metrics over time**, as if each batch were a new day/week.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def create_monitoring_windows(\n    X: pd.DataFrame,\n    y: pd.Series,\n    n_windows: int = 10,\n    random_state: int = 42,\n) -> List[Dict[str, pd.DataFrame]]:\n    \"\"\"Split X and y into simulated time windows.\n\n    Args:\n        X: Feature DataFrame (test set).\n        y: Target Series (test set).\n        n_windows: Number of windows to create.\n        random_state: Seed for shuffling.\n\n    Returns:\n        List of dicts with 'X' and 'y' for each window.\n    \"\"\"\n    if len(X) != len(y):\n        raise ValueError(\"X and y must have the same length.\")\n\n    # Shuffle indices to simulate random arrival\n    rng = np.random.default_rng(random_state)\n    indices = np.arange(len(X))\n    rng.shuffle(indices)\n\n    X_shuffled = X.iloc[indices].reset_index(drop=True)\n    y_shuffled = y.iloc[indices].reset_index(drop=True)\n\n    window_size: int = max(1, len(X) // n_windows)\n    windows: List[Dict[str, pd.DataFrame]] = []\n\n    for i in range(n_windows):\n        start = i * window_size\n        end = (i + 1) * window_size if i < n_windows - 1 else len(X_shuffled)\n\n        if start >= len(X_shuffled):\n            break\n\n        X_win = X_shuffled.iloc[start:end].reset_index(drop=True)\n        y_win = y_shuffled.iloc[start:end].reset_index(drop=True)\n\n        windows.append({\"X\": X_win, \"y\": y_win})\n\n    return windows\n\n\n# Fit the RF pipeline once on the full training data\nrf_pipeline.fit(X_train, y_train)\n\n# Create simulated monitoring windows on the test set\nmonitoring_windows = create_monitoring_windows(X_test, y_test, n_windows=10, random_state=RANDOM_STATE)\nlen(monitoring_windows), monitoring_windows[0][\"X\"].shape\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def compute_window_metrics(\n    model: BaseEstimator,\n    window_idx: int,\n    X_win: pd.DataFrame,\n    y_win: pd.Series,\n) -> Dict[str, float]:\n    \"\"\"Compute metrics for a single monitoring window.\n\n    Args:\n        model: Fitted sklearn estimator or pipeline.\n        window_idx: Index of the window (0-based).\n        X_win: Features in the window.\n        y_win: Targets in the window.\n\n    Returns:\n        Dict with metrics and window index.\n    \"\"\"\n    if len(X_win) == 0:\n        raise ValueError(\"Window is empty; cannot compute metrics.\")\n\n    y_pred = model.predict(X_win)\n\n    if hasattr(model, \"predict_proba\"):\n        y_proba = model.predict_proba(X_win)[:, 1]\n        roc_auc = roc_auc_score(y_win, y_proba)\n    else:\n        y_proba = None\n        roc_auc = np.nan\n\n    accuracy = accuracy_score(y_win, y_pred)\n    churn_rate = float(y_win.mean())  # proportion of 1s\n    positive_rate = float(y_pred.mean())  # proportion predicted as churn\n\n    return {\n        \"window\": float(window_idx),\n        \"n_samples\": float(len(y_win)),\n        \"accuracy\": float(accuracy),\n        \"roc_auc\": float(roc_auc) if not np.isnan(roc_auc) else np.nan,\n        \"churn_rate\": float(churn_rate),\n        \"predicted_positive_rate\": float(positive_rate),\n    }\n\n\nwindow_metrics: List[Dict[str, float]] = []\n\nfor idx, win in enumerate(monitoring_windows):\n    metrics = compute_window_metrics(\n        model=rf_pipeline,\n        window_idx=idx,\n        X_win=win[\"X\"],\n        y_win=win[\"y\"],\n    )\n    window_metrics.append(metrics)\n\nmetrics_df = pd.DataFrame(window_metrics)\ndisplay(metrics_df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Plot metrics over windows\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\naxes[0, 0].plot(metrics_df[\"window\"], metrics_df[\"accuracy\"], marker=\"o\")\naxes[0, 0].set_title(\"Accuracy over windows\")\naxes[0, 0].set_xlabel(\"Window\")\naxes[0, 0].set_ylabel(\"Accuracy\")\n\naxes[0, 1].plot(metrics_df[\"window\"], metrics_df[\"roc_auc\"], marker=\"o\")\naxes[0, 1].set_title(\"ROC-AUC over windows\")\naxes[0, 1].set_xlabel(\"Window\")\naxes[0, 1].set_ylabel(\"ROC-AUC\")\n\naxes[1, 0].plot(metrics_df[\"window\"], metrics_df[\"churn_rate\"], marker=\"o\")\naxes[1, 0].set_title(\"Observed churn rate over windows\")\naxes[1, 0].set_xlabel(\"Window\")\naxes[1, 0].set_ylabel(\"Churn rate\")\n\naxes[1, 1].plot(metrics_df[\"window\"], metrics_df[\"predicted_positive_rate\"], marker=\"o\")\naxes[1, 1].set_title(\"Predicted positive rate over windows\")\naxes[1, 1].set_xlabel(\"Window\")\naxes[1, 1].set_ylabel(\"Predicted positive rate\")\n\nplt.tight_layout()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe simulated **monitoring windows** and computed key metrics in each one:\n\n- Performance metrics: accuracy, ROC-AUC.\n- Data properties: observed churn rate.\n- Model behaviour: predicted positive rate.\n\nIn a real system, you would log these metrics continuously and display them on\na dashboard (e.g. Grafana, Superset, internal BI tools). Thresholds or control\nlimits could trigger alerts when metrics move outside acceptable ranges.\n\nNext we add a simple **drift measure** on model scores using Population\nStability Index (PSI).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Score Distribution Drift with PSI (Population Stability Index)\n\nOne common way to monitor model drift is to compare the distribution of:\n\n- Features, or\n- Model scores (predicted probabilities)\n\nbetween a **reference period** (training data) and a **current period** (a\nmonitoring window).\n\nHere we:\n\n1. Use the **train** score distribution as the reference.\n2. For each window, compute the PSI between the train scores and the window scores.\n3. Plot PSI over windows.\n\nRule-of-thumb for PSI:\n\n- `< 0.1` \u2013 No significant drift.\n- `0.1\u20130.25` \u2013 Moderate drift; watch closely.\n- `> 0.25` \u2013 Large drift; investigate and possibly retrain.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def calculate_psi(\n    expected: np.ndarray,\n    actual: np.ndarray,\n    n_bins: int = 10,\n) -> float:\n    \"\"\"Calculate the Population Stability Index (PSI) between two score arrays.\n\n    Args:\n        expected: Reference scores (e.g. train predictions).\n        actual: Current scores (e.g. window predictions).\n        n_bins: Number of bins for the score histogram.\n\n    Returns:\n        PSI value (higher means more drift).\n    \"\"\"\n    if len(expected) == 0 or len(actual) == 0:\n        raise ValueError(\"Expected and actual arrays must be non-empty.\")\n\n    # Define common bin edges from the expected distribution\n    quantiles = np.linspace(0.0, 1.0, n_bins + 1)\n    bin_edges = np.quantile(expected, quantiles)\n\n    # Small epsilon to avoid division by zero or log of zero\n    eps = 1e-6\n\n    # Compute histogram for expected and actual\n    expected_counts, _ = np.histogram(expected, bins=bin_edges)\n    actual_counts, _ = np.histogram(actual, bins=bin_edges)\n\n    expected_perc = expected_counts / max(expected_counts.sum(), eps)\n    actual_perc = actual_counts / max(actual_counts.sum(), eps)\n\n    psi_values = (actual_perc - expected_perc) * np.log(\n        (actual_perc + eps) / (expected_perc + eps)\n    )\n\n    return float(np.sum(psi_values))\n\n\n# Score distributions: train vs each window\ntrain_scores = rf_pipeline.predict_proba(X_train)[:, 1]\n\npsi_records: List[Dict[str, float]] = []\n\nfor idx, win in enumerate(monitoring_windows):\n    window_scores = rf_pipeline.predict_proba(win[\"X\"])[:, 1]\n    psi_value = calculate_psi(expected=train_scores, actual=window_scores, n_bins=10)\n\n    psi_records.append({\"window\": float(idx), \"psi\": float(psi_value)})\n\npsi_df = pd.DataFrame(psi_records)\ndisplay(psi_df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "plt.figure(figsize=(8, 5))\nplt.plot(psi_df[\"window\"], psi_df[\"psi\"], marker=\"o\")\nplt.axhline(0.1, linestyle=\"--\")  # moderate drift threshold\nplt.axhline(0.25, linestyle=\"--\")  # high drift threshold\nplt.title(\"Score distribution drift (PSI) over windows\")\nplt.xlabel(\"Window\")\nplt.ylabel(\"PSI\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe computed **PSI** between:\n\n- The training score distribution (reference), and\n- Each monitoring window score distribution (current).\n\nThen we plotted PSI over windows, with simple thresholds at 0.1 and 0.25.\n\nIn real deployments, you might:\n\n- Compute PSI for multiple features and the score distribution.\n- Trigger alerts when PSI exceeds thresholds for several consecutive periods.\n- Investigate data-quality issues, feature distribution changes, or shifts in\n  the underlying population.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Simple Alert Logic (Example)\n\nTo make the idea concrete, we can implement a basic **alert rule**:\n\n- If PSI > 0.25 in any window, print a **high drift** warning.\n- If ROC-AUC in a window falls below a chosen threshold (e.g. 0.70),\n  print a **performance degradation** warning.\n\nThis is just a toy example; real systems would integrate with logging,\nmonitoring, and alerting infrastructure.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def check_alerts(\n    metrics_df: pd.DataFrame,\n    psi_df: pd.DataFrame,\n    min_roc_auc: float = 0.70,\n    max_psi: float = 0.25,\n) -> None:\n    \"\"\"Check simple alert conditions based on ROC-AUC and PSI.\n\n    Args:\n        metrics_df: DataFrame with window-level metrics (including 'window' and 'roc_auc').\n        psi_df: DataFrame with window-level PSI values (columns 'window' and 'psi').\n        min_roc_auc: Minimum acceptable ROC-AUC before raising an alert.\n        max_psi: Maximum acceptable PSI before raising an alert.\n    \"\"\"\n    # Merge metrics and PSI by window\n    merged = metrics_df.merge(psi_df, on=\"window\", how=\"inner\")\n\n    for _, row in merged.iterrows():\n        w = int(row[\"window\"])\n        roc_auc = float(row[\"roc_auc\"])\n        psi = float(row[\"psi\"])\n\n        if roc_auc < min_roc_auc:\n            print(f\"[ALERT] Window {w}: ROC-AUC {roc_auc:.3f} < {min_roc_auc:.2f} (performance degradation).\")\n\n        if psi > max_psi:\n            print(f\"[ALERT] Window {w}: PSI {psi:.3f} > {max_psi:.2f} (significant drift).\")\n\n\ncheck_alerts(metrics_df=metrics_df, psi_df=psi_df, min_roc_auc=0.70, max_psi=0.25)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe implemented a minimal **alerting layer** on top of our monitoring metrics.\n\nIn practice, this logic would be integrated with:\n\n- Dashboards (Grafana, Looker, internal tools).\n- Alerting systems (Slack, email, PagerDuty, etc.).\n- A retraining / revalidation pipeline when alerts persist.\n\nThe key idea is that **monitoring is not only about a single accuracy number**;\nyou track both **performance metrics** and **data/score drift** to decide when\nto investigate or intervene.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Final Thoughts and Next Steps\n\nAcross the three notebooks you now have a complete mini-project:\n\n1. **Notebook 1 \u2013 Core churn project**\n   - Business framing, data understanding, cleaning, EDA.\n   - Baseline and main models.\n   - Feature importance and interpretation.\n\n2. **Notebook 2 \u2013 Tuning and cost-sensitive decisions**\n   - Hyperparameter tuning with cross-validation.\n   - Cost-aware threshold selection.\n   - Precision\u2013recall and threshold analysis.\n\n3. **Notebook 3 \u2013 Monitoring and drift (this one)**\n   - Simulated production windows.\n   - Metrics over time (accuracy, ROC-AUC, churn and prediction rates).\n   - Score drift monitoring with PSI.\n   - Simple alert rules.\n\nPossible extensions:\n\n- Use real timestamps (if available) instead of simulated windows.\n- Persist monitoring data to a database or data lake and build dashboards on top.\n- Add more advanced drift detection methods (e.g. statistical tests, domain-specific checks).\n- Integrate monitoring and retraining into an automated MLOps pipeline.\n\nThis trilogy is a solid starting point for a **churn modelling workflow** that\nis not just about building a model, but also about **operating it responsibly\nover time**.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}