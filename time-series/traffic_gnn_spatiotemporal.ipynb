{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spatio-Temporal Traffic Forecasting with Graph Neural Networks (METR-LA / PEMS)\n",
        "\n",
        "This notebook focuses on **spatio-temporal / graph-based traffic forecasting**.\n",
        "\n",
        "Instead of a single aggregated traffic series, we model traffic at many\n",
        "sensors (nodes) connected by a road network graph, using a simple\n",
        "**Spatio-Temporal Graph Convolutional Network (ST-GCN)**-style model.\n",
        "\n",
        "We use a METR-LA / PEMS-style dataset with:\n",
        "\n",
        "- A set of **sensors / stations** (graph nodes).\n",
        "- Traffic speed or volume time series at each node.\n",
        "- A graph **adjacency matrix** describing road connections.\n",
        "\n",
        "The goal is to predict **future traffic at all sensors** given a window of\n",
        "past observations, leveraging both temporal dynamics and spatial structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. How to run this notebook\n",
        "\n",
        "1. Obtain a preprocessed METR-LA or PEMS-style dataset.\n",
        "   Many public repos (e.g. DCRNN / STGCN) provide NPZ/H5 files with\n",
        "   preprocessed data and adjacency matrices.\n",
        "\n",
        "2. For this notebook, we assume an NPZ file with at least:\n",
        "\n",
        "   - `traffic`: 3D array of shape `(T, N, 1)` or 2D `(T, N)`\n",
        "     where `T` is the number of time steps and `N` the number of sensors.\n",
        "   - `adjacency`: 2D array `(N, N)` with non-negative weights.\n",
        "\n",
        "   Save it as:\n",
        "\n",
        "   ```text\n",
        "   data/graph_traffic.npz\n",
        "   ```\n",
        "\n",
        "3. Install required packages:\n",
        "\n",
        "   ```bash\n",
        "   pip install numpy pandas matplotlib torch scikit-learn\n",
        "   ```\n",
        "\n",
        "4. Open this notebook and run it top-to-bottom. You can adjust model and\n",
        "   training hyperparameters to match your hardware.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (11, 5)\n",
        "\n",
        "RANDOM_STATE: int = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "DATA_PATH: Path = Path(\"data\") / \"graph_traffic.npz\"\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Expected dataset at {DATA_PATH.resolve()}\\n\"\n",
        "        \"It should contain 'traffic' (T,N or T,N,1) and 'adjacency' (N,N).\"\n",
        "    )\n",
        "\n",
        "npz = np.load(DATA_PATH)\n",
        "traffic = npz[\"traffic\"]  # (T,N) or (T,N,1)\n",
        "adjacency = npz[\"adjacency\"]  # (N,N)\n",
        "\n",
        "print(\"traffic shape:\", traffic.shape)\n",
        "print(\"adjacency shape:\", adjacency.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic preprocessing and EDA\n",
        "\n",
        "We ensure a standard shape and visualise traffic at a few sensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure traffic has shape (T, N)\n",
        "if traffic.ndim == 3 and traffic.shape[-1] == 1:\n",
        "    traffic = traffic[..., 0]\n",
        "elif traffic.ndim != 2:\n",
        "    raise ValueError(\"Expected traffic array with shape (T,N) or (T,N,1).\")\n",
        "\n",
        "T, N = traffic.shape\n",
        "print(f\"Using traffic array of shape T={T}, N={N} sensors.\")\n",
        "\n",
        "num_example_sensors: int = min(3, N)\n",
        "time_index = np.arange(T)\n",
        "\n",
        "for node_id in range(num_example_sensors):\n",
        "    plt.plot(time_index[:24 * 7], traffic[:24 * 7, node_id], label=f\"sensor {node_id}\")\n",
        "\n",
        "plt.title(\"Example week \u2013 traffic at a few sensors\")\n",
        "plt.xlabel(\"Time step (e.g. hour or 5-min interval)\")\n",
        "plt.ylabel(\"Traffic (e.g. speed or volume)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train/val/test split and normalization\n",
        "\n",
        "We split the time axis into train/validation/test segments and normalise\n",
        "per-node using statistics from the training segment only.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def split_time_series(\n",
        "    data: np.ndarray,\n",
        "    train_frac: float = 0.6,\n",
        "    val_frac: float = 0.2,\n",
        ") -> Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int]]:\n",
        "    \"\"\"Return index ranges (start, end) for train, val, test on time axis.\n",
        "\n",
        "    The end index is exclusive. Fractions should sum to <= 1.0.\n",
        "    \"\"\"\n",
        "    T = data.shape[0]\n",
        "    train_end = int(T * train_frac)\n",
        "    val_end = train_end + int(T * val_frac)\n",
        "    train_range = (0, train_end)\n",
        "    val_range = (train_end, val_end)\n",
        "    test_range = (val_end, T)\n",
        "    return train_range, val_range, test_range\n",
        "\n",
        "\n",
        "train_range, val_range, test_range = split_time_series(traffic)\n",
        "train_range, val_range, test_range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_normalization_stats(train_data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Compute per-node mean and std from training data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_data : np.ndarray\n",
        "        Array of shape (T_train, N).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mean : np.ndarray\n",
        "        Mean per node, shape (N,).\n",
        "    std : np.ndarray\n",
        "        Standard deviation per node, shape (N,), with minimum floor applied.\n",
        "    \"\"\"\n",
        "    mean = train_data.mean(axis=0)\n",
        "    std = train_data.std(axis=0)\n",
        "    std[std < 1e-3] = 1e-3\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "train_data = traffic[train_range[0] : train_range[1]]\n",
        "mean_nodes, std_nodes = compute_normalization_stats(train_data)\n",
        "\n",
        "traffic_norm = (traffic - mean_nodes) / std_nodes\n",
        "traffic_norm.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sequence dataset for spatio-temporal forecasting\n",
        "\n",
        "We now build a dataset of sliding windows:\n",
        "\n",
        "- Input: past `input_len` steps for all nodes.\n",
        "- Target: next `horizon` steps for all nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SpatioTemporalDataset(Dataset):\n",
        "    \"\"\"Dataset of spatio-temporal sequences for graph forecasting.\n",
        "\n",
        "    Each item is a pair `(X, Y)`:\n",
        "\n",
        "    - `X` has shape `(input_len, N, 1)` \u2013 past inputs.\n",
        "    - `Y` has shape `(horizon, N, 1)` \u2013 future targets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: np.ndarray,\n",
        "        time_range: Tuple[int, int],\n",
        "        input_len: int,\n",
        "        horizon: int,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.data = data.astype(np.float32)\n",
        "        self.start, self.end = time_range\n",
        "        self.input_len = input_len\n",
        "        self.horizon = horizon\n",
        "\n",
        "        max_start = self.end - self.start - input_len - horizon + 1\n",
        "        if max_start <= 0:\n",
        "            raise ValueError(\"Time range too small for given input_len and horizon.\")\n",
        "        self.indices = np.arange(self.start, self.start + max_start)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        start_idx = int(self.indices[idx])\n",
        "        x = self.data[start_idx : start_idx + self.input_len]\n",
        "        y = self.data[start_idx + self.input_len : start_idx + self.input_len + self.horizon]\n",
        "\n",
        "        x = x[..., None]\n",
        "        y = y[..., None]\n",
        "        return torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "\n",
        "INPUT_LEN: int = 12\n",
        "HORIZON: int = 12\n",
        "BATCH_SIZE: int = 64\n",
        "\n",
        "train_dataset = SpatioTemporalDataset(traffic_norm, train_range, INPUT_LEN, HORIZON)\n",
        "val_dataset = SpatioTemporalDataset(traffic_norm, val_range, INPUT_LEN, HORIZON)\n",
        "test_dataset = SpatioTemporalDataset(traffic_norm, test_range, INPUT_LEN, HORIZON)\n",
        "\n",
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Graph convolution and ST-GCN block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def normalize_adjacency(adj: np.ndarray, add_self_loops: bool = True) -> np.ndarray:\n",
        "    \"\"\"Compute symmetric normalised adjacency: D^{-1/2} (A + I) D^{-1/2}.\"\"\"\n",
        "    A = adj.astype(np.float32)\n",
        "    if add_self_loops:\n",
        "        A = A + np.eye(A.shape[0], dtype=np.float32)\n",
        "    d = A.sum(axis=1)\n",
        "    d_inv_sqrt = np.power(d, -0.5)\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0\n",
        "    D_inv_sqrt = np.diag(d_inv_sqrt)\n",
        "    return D_inv_sqrt @ A @ D_inv_sqrt\n",
        "\n",
        "\n",
        "A_hat_np = normalize_adjacency(adjacency)\n",
        "A_hat = torch.tensor(A_hat_np, dtype=torch.float32)\n",
        "A_hat.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class GraphConv(nn.Module):\n",
        "    \"\"\"Simple graph convolution layer using a precomputed A_hat.\n",
        "\n",
        "    Given node features X of shape (B, N, F_in), computes X' = A_hat @ X @ W.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int) -> None:\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, A_hat: torch.Tensor) -> torch.Tensor:\n",
        "        xw = self.linear(x)\n",
        "        out = torch.einsum(\"ij,bjf->bif\", A_hat, xw)\n",
        "        return out\n",
        "\n",
        "\n",
        "class STGCNBlock(nn.Module):\n",
        "    \"\"\"Spatio-temporal block combining temporal and graph convolutions.\n",
        "\n",
        "    Input:  (B, T, N, F_in)\n",
        "    Output: (B, T, N, F_out)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        spatial_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: int = 3,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.temporal1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=spatial_channels,\n",
        "            kernel_size=(kernel_size, 1),\n",
        "            padding=(kernel_size - 1, 0),\n",
        "        )\n",
        "        self.gconv = GraphConv(spatial_channels, spatial_channels)\n",
        "        self.temporal2 = nn.Conv2d(\n",
        "            in_channels=spatial_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=(kernel_size, 1),\n",
        "            padding=(kernel_size - 1, 0),\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, A_hat: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = self.temporal1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        B, T_seq, N_nodes, F_sp = x.shape\n",
        "        x_flat = x.reshape(B * T_seq, N_nodes, F_sp)\n",
        "        x_gc = self.gconv(x_flat, A_hat)\n",
        "        x = x_gc.reshape(B, T_seq, N_nodes, F_sp)\n",
        "\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = self.temporal2(x)\n",
        "        x = self.relu(x)\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Full ST-GCN model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class STGCN(nn.Module):\n",
        "    \"\"\"Spatio-Temporal GCN for multi-step traffic forecasting.\n",
        "\n",
        "    Input:  (B, input_len, N, 1)\n",
        "    Output: (B, horizon, N, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes: int,\n",
        "        input_len: int,\n",
        "        horizon: int,\n",
        "        in_channels: int = 1,\n",
        "        spatial_channels: int = 16,\n",
        "        hidden_channels: int = 32,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.input_len = input_len\n",
        "        self.horizon = horizon\n",
        "\n",
        "        self.block1 = STGCNBlock(\n",
        "            in_channels=in_channels,\n",
        "            spatial_channels=spatial_channels,\n",
        "            out_channels=hidden_channels,\n",
        "        )\n",
        "        self.block2 = STGCNBlock(\n",
        "            in_channels=hidden_channels,\n",
        "            spatial_channels=spatial_channels,\n",
        "            out_channels=hidden_channels,\n",
        "        )\n",
        "\n",
        "        self.temporal_projection = nn.Conv2d(\n",
        "            in_channels=hidden_channels,\n",
        "            out_channels=horizon,\n",
        "            kernel_size=(1, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, A_hat: torch.Tensor) -> torch.Tensor:\n",
        "        out = self.block1(x, A_hat)\n",
        "        out = self.block2(out, A_hat)\n",
        "        out = out.permute(0, 3, 1, 2)\n",
        "        out = self.temporal_projection(out)\n",
        "        out = out[:, :, -1, :]\n",
        "        out = out[..., None]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training loop and metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = STGCN(num_nodes=N, input_len=INPUT_LEN, horizon=HORIZON).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "A_hat_device = A_hat.to(device)\n",
        "\n",
        "def denormalize(data_norm: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
        "    return data_norm * std + mean\n",
        "\n",
        "\n",
        "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    return {\"mae\": float(mae), \"rmse\": rmse}\n",
        "\n",
        "\n",
        "def train_epoch(model: nn.Module, loader: DataLoader) -> float:\n",
        "    model.train()\n",
        "    losses: List[float] = []\n",
        "    for X_batch, Y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        Y_batch = Y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_batch, A_hat_device)\n",
        "        loss = criterion(preds, Y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(float(loss.item()))\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "\n",
        "def eval_epoch(model: nn.Module, loader: DataLoader) -> float:\n",
        "    model.eval()\n",
        "    losses: List[float] = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, Y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            Y_batch = Y_batch.to(device)\n",
        "            preds = model(X_batch, A_hat_device)\n",
        "            loss = criterion(preds, Y_batch)\n",
        "            losses.append(float(loss.item()))\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "\n",
        "EPOCHS: int = 10\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train_epoch(model, train_loader)\n",
        "    val_loss = eval_epoch(model, val_loader)\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Test-set evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model.eval()\n",
        "y_true_list: List[np.ndarray] = []\n",
        "y_pred_list: List[np.ndarray] = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, Y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        preds = model(X_batch, A_hat_device)\n",
        "        y_true_list.append(Y_batch.numpy())\n",
        "        y_pred_list.append(preds.cpu().numpy())\n",
        "\n",
        "y_true_arr = np.concatenate(y_true_list, axis=0)\n",
        "y_pred_arr = np.concatenate(y_pred_list, axis=0)\n",
        "\n",
        "y_true_flat = y_true_arr[..., 0].reshape(-1, N)\n",
        "y_pred_flat = y_pred_arr[..., 0].reshape(-1, N)\n",
        "\n",
        "y_true_denorm = denormalize(y_true_flat, mean_nodes, std_nodes)\n",
        "y_pred_denorm = denormalize(y_pred_flat, mean_nodes, std_nodes)\n",
        "\n",
        "stgcn_metrics = compute_metrics(y_true_denorm.ravel(), y_pred_denorm.ravel())\n",
        "print(\"STGCN test metrics:\", stgcn_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Persistence baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_true_p_list: List[np.ndarray] = []\n",
        "y_pred_p_list: List[np.ndarray] = []\n",
        "\n",
        "for X_batch, Y_batch in test_loader:\n",
        "    last_step = X_batch[:, -1, :, :]\n",
        "    B_batch = X_batch.shape[0]\n",
        "    last_rep = last_step[:, None, :, :].repeat(1, HORIZON, 1, 1)\n",
        "    y_true_p_list.append(Y_batch.numpy())\n",
        "    y_pred_p_list.append(last_rep.numpy())\n",
        "\n",
        "y_true_p = np.concatenate(y_true_p_list, axis=0)\n",
        "y_pred_p = np.concatenate(y_pred_p_list, axis=0)\n",
        "\n",
        "y_true_p_flat = y_true_p[..., 0].reshape(-1, N)\n",
        "y_pred_p_flat = y_pred_p[..., 0].reshape(-1, N)\n",
        "\n",
        "y_true_p_denorm = denormalize(y_true_p_flat, mean_nodes, std_nodes)\n",
        "y_pred_p_denorm = denormalize(y_pred_p_flat, mean_nodes, std_nodes)\n",
        "\n",
        "baseline_metrics = compute_metrics(y_true_p_denorm.ravel(), y_pred_p_denorm.ravel())\n",
        "print(\"Persistence baseline test metrics:\", baseline_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Visualising forecasts at a sample sensor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sensor_id: int = 0\n",
        "steps_to_plot: int = 200\n",
        "\n",
        "y_true_h1 = y_true_denorm[:, sensor_id]\n",
        "y_pred_h1 = y_pred_denorm[:, sensor_id]\n",
        "\n",
        "plt.plot(y_true_h1[:steps_to_plot], label=\"actual\")\n",
        "plt.plot(y_pred_h1[:steps_to_plot], label=\"STGCN\", linestyle=\"--\")\n",
        "plt.title(f\"One-step-ahead forecast at sensor {sensor_id}\")\n",
        "plt.xlabel(\"Sequence index\")\n",
        "plt.ylabel(\"Traffic (denormalised)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and extensions\n",
        "\n",
        "In this notebook we:\n",
        "\n",
        "- Loaded a **graph-based traffic dataset** (METR-LA / PEMS-style).\n",
        "- Built a **spatio-temporal supervised dataset**.\n",
        "- Implemented a simple **Spatio-Temporal GCN** model.\n",
        "- Compared it to a spatio-temporal **persistence baseline**.\n",
        "- Visualised one-step-ahead forecasts for a sample sensor.\n",
        "\n",
        "Possible extensions:\n",
        "\n",
        "- Add dilated temporal convolutions and residual connections.\n",
        "- Use multiple adjacency matrices (distance, correlation, connectivity).\n",
        "- Introduce time-of-day and day-of-week embeddings as extra inputs.\n",
        "- Explore more advanced architectures such as Graph WaveNet or MTGNN.\n"
      ]
    }
  ]
}