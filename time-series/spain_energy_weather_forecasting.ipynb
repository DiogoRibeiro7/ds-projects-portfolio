{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spanish Energy Load Forecasting with Weather\n",
        "\n",
        "This notebook focuses specifically on the Spanish **Hourly energy demand, generation and weather** dataset from Kaggle.\n",
        "\n",
        "We will:\n",
        "- Load and clean the Spanish dataset (load + weather).\n",
        "- Perform time-series EDA (decomposition, anomalies, load vs temperature).\n",
        "- Train SARIMAX (with exogenous temperature).\n",
        "- Train a Gradient Boosting model with lagged load, temperature and calendar features.\n",
        "- Build simple probabilistic forecasts (quantile GB) and a champion\u2013challenger comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup and Data Loading\n",
        "\n",
        "We expect the original Kaggle file `energy_dataset.csv` at:\n",
        "\n",
        "`data/spain/energy_dataset.csv`\n",
        "\n",
        "If it lives elsewhere, adjust `RAW_PATH` below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (11, 5)\n",
        "\n",
        "RAW_PATH = Path(\"data\") / \"spain\" / \"energy_dataset.csv\"\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "if not RAW_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Expected raw Spanish dataset at {RAW_PATH.resolve()}\\n\"\n",
        "        \"Place 'energy_dataset.csv' there or adjust RAW_PATH.\"\n",
        "    )\n",
        "\n",
        "raw_df = pd.read_csv(RAW_PATH)\n",
        "raw_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.1 Clean and standardise columns\n",
        "\n",
        "We standardise to:\n",
        "- `timestamp` (index)\n",
        "- `load` \u2013 from `total load actual`\n",
        "- `temperature` \u2013 mean across temperature-like columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_spain_energy_frame(raw: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Return cleaned hourly Spanish load + temperature.\n",
        "\n",
        "    Expects columns:\n",
        "    - 'time'\n",
        "    - 'total load actual'\n",
        "    plus one or more temperature-like columns.\n",
        "    \"\"\"\n",
        "    df = raw.copy()\n",
        "    if \"time\" not in df.columns or \"total load actual\" not in df.columns:\n",
        "        raise ValueError(\"Missing 'time' or 'total load actual' in raw data.\")\n",
        "\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
        "\n",
        "    df[\"load\"] = df[\"total load actual\"].astype(float)\n",
        "\n",
        "    temp_cols = [c for c in df.columns if (\"temp\" in c.lower() or \"temperature\" in c.lower())]\n",
        "    if temp_cols:\n",
        "        df[\"temperature\"] = df[temp_cols].astype(float).mean(axis=1)\n",
        "    else:\n",
        "        df[\"temperature\"] = np.nan\n",
        "\n",
        "    df = df.set_index(\"timestamp\").asfreq(\"H\")\n",
        "    df[\"load\"] = df[\"load\"].interpolate(limit_direction=\"both\")\n",
        "    if df[\"temperature\"].notna().any():\n",
        "        df[\"temperature\"] = df[\"temperature\"].interpolate(limit_direction=\"both\")\n",
        "\n",
        "    return df[[\"load\", \"temperature\"]]\n",
        "\n",
        "\n",
        "df = build_spain_energy_frame(raw_df)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.2 Save a reusable cleaned file (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clean_path = Path(\"data\") / \"energy.csv\"\n",
        "clean_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "df.to_csv(clean_path)\n",
        "print(\"Saved cleaned Spanish data to\", clean_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.3 Helper metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Root mean squared error.\"\"\"\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "\n",
        "def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Mean absolute percentage error (in %).\"\"\"\n",
        "    y_true_safe = np.clip(y_true, 1e-6, None)\n",
        "    return float(np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100)\n",
        "\n",
        "\n",
        "print(\"Sanity RMSE (self):\", rmse(df[\"load\"].to_numpy(), df[\"load\"].to_numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Tier 1 \u2013 EDA and Decomposition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Global plot\n",
        "df[\"load\"].plot(alpha=0.8)\n",
        "plt.title(\"Spanish energy load \u2013 full series\")\n",
        "plt.ylabel(\"Load\")\n",
        "plt.show()\n",
        "\n",
        "# Last 60 days zoom\n",
        "last_period = df.iloc[-24*60:]\n",
        "last_period[\"load\"].plot()\n",
        "plt.title(\"Spanish load \u2013 last ~60 days\")\n",
        "plt.ylabel(\"Load\")\n",
        "plt.show()\n",
        "\n",
        "# Decompose with daily seasonality\n",
        "decomp = seasonal_decompose(df[\"load\"].dropna(), model=\"additive\", period=24)\n",
        "fig = decomp.plot()\n",
        "fig.set_size_inches(11, 8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df[\"trend\"] = decomp.trend\n",
        "df[\"seasonal\"] = decomp.seasonal\n",
        "df[\"resid\"] = decomp.resid\n",
        "df[[\"load\", \"trend\", \"seasonal\", \"resid\"]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Calendar profiles and anomalies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cal_df = df.copy()\n",
        "cal_df[\"hour\"] = cal_df.index.hour\n",
        "cal_df[\"dayofweek\"] = cal_df.index.dayofweek\n",
        "\n",
        "sns.lineplot(data=cal_df, x=\"hour\", y=\"load\", estimator=\"mean\", ci=None)\n",
        "plt.title(\"Average daily load profile (Spain)\")\n",
        "plt.show()\n",
        "\n",
        "sns.lineplot(data=cal_df, x=\"dayofweek\", y=\"load\", estimator=\"mean\", ci=None)\n",
        "plt.title(\"Average weekly load profile (Spain)\")\n",
        "plt.show()\n",
        "\n",
        "# Anomaly detection\n",
        "resid = df[\"resid\"].dropna()\n",
        "resid_std = resid.std()\n",
        "stat_mask = np.abs(resid) > 3 * resid_std\n",
        "stat_anoms = resid[stat_mask]\n",
        "rule_mask = (df[\"load\"] < 0) | (df[\"load\"] > df[\"load\"].quantile(0.995))\n",
        "rule_anoms = df[\"load\"][rule_mask]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df.index, df[\"load\"], alpha=0.6, label=\"load\")\n",
        "plt.scatter(stat_anoms.index, df.loc[stat_anoms.index, \"load\"], s=10, label=\"stat\", color=\"red\")\n",
        "plt.scatter(rule_anoms.index, rule_anoms.values, s=10, label=\"rule\", color=\"orange\")\n",
        "plt.legend()\n",
        "plt.title(\"Anomalies \u2013 Spanish load\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Load vs temperature and cross-correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if df[\"temperature\"].notna().any():\n",
        "    tmp = df[[\"load\", \"temperature\"]].dropna().sample(min(8000, len(df)), random_state=RANDOM_STATE)\n",
        "    sns.scatterplot(data=tmp, x=\"temperature\", y=\"load\", alpha=0.3)\n",
        "    plt.title(\"Spanish load vs temperature (sample)\")\n",
        "    plt.show()\n",
        "\n",
        "    max_lag = 72\n",
        "    load_vals = df[\"load\"].interpolate(limit_direction=\"both\").to_numpy()\n",
        "    temp_vals = df[\"temperature\"].interpolate(limit_direction=\"both\").to_numpy()\n",
        "    lags = np.arange(-max_lag, max_lag + 1)\n",
        "    ccs: List[float] = []\n",
        "    for lag in lags:\n",
        "        if lag < 0:\n",
        "            x = temp_vals[:lag]\n",
        "            y = load_vals[-lag:]\n",
        "        elif lag > 0:\n",
        "            x = temp_vals[lag:]\n",
        "            y = load_vals[:-lag]\n",
        "        else:\n",
        "            x = temp_vals\n",
        "            y = load_vals\n",
        "        ccs.append(np.corrcoef(x, y)[0, 1])\n",
        "\n",
        "    plt.plot(lags, ccs)\n",
        "    plt.axhline(0, color=\"black\", linewidth=0.8)\n",
        "    plt.title(\"Temperature\u2013load cross-correlation (Spain)\")\n",
        "    plt.xlabel(\"Lag (hours, negative = temp leads)\")\n",
        "    plt.ylabel(\"Correlation\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No temperature data \u2013 skipping weather analysis.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tier 2 \u2013 Modelling (SARIMAX + GB with weather)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "horizon_days = 30\n",
        "horizon = 24 * horizon_days\n",
        "if len(df) <= 2 * horizon:\n",
        "    raise ValueError(\"Not enough data for 30-day test horizon.\")\n",
        "\n",
        "train_df = df.iloc[:-horizon].copy()\n",
        "test_df = df.iloc[-horizon:].copy()\n",
        "\n",
        "print(\"Train:\", train_df.index.min(), \"\u2192\", train_df.index.max())\n",
        "print(\"Test: \", test_df.index.min(), \"\u2192\", test_df.index.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def fit_sarimax(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Fit SARIMAX on Spanish load with optional temperature.\"\"\"\n",
        "    endog_train = train[\"load\"].astype(float)\n",
        "    exog_cols: List[str] = []\n",
        "    if train[\"temperature\"].notna().any():\n",
        "        exog_cols = [\"temperature\"]\n",
        "    exog_train = train[exog_cols] if exog_cols else None\n",
        "    exog_test = test[exog_cols] if exog_cols else None\n",
        "\n",
        "    model = SARIMAX(\n",
        "        endog_train,\n",
        "        exog=exog_train,\n",
        "        order=(1, 0, 2),\n",
        "        seasonal_order=(1, 1, 1, 24),\n",
        "        enforce_stationarity=False,\n",
        "        enforce_invertibility=False,\n",
        "    )\n",
        "    res = model.fit(disp=False)\n",
        "    fitted = res.fittedvalues.to_numpy()\n",
        "    forecast = res.forecast(steps=len(test), exog=exog_test).to_numpy()\n",
        "    return fitted, forecast\n",
        "\n",
        "\n",
        "sarimax_fitted, sarimax_forecast = fit_sarimax(train_df, test_df)\n",
        "print(\"SARIMAX train RMSE:\", rmse(train_df[\"load\"].to_numpy(), sarimax_fitted))\n",
        "print(\"SARIMAX test RMSE: \", rmse(test_df[\"load\"].to_numpy(), sarimax_forecast))\n",
        "\n",
        "plt.plot(train_df.index, train_df[\"load\"], label=\"train\", alpha=0.6)\n",
        "plt.plot(test_df.index, test_df[\"load\"], label=\"test\", alpha=0.8)\n",
        "plt.plot(test_df.index, sarimax_forecast, label=\"SARIMAX\", linestyle=\"--\")\n",
        "plt.title(\"Spanish load \u2013 SARIMAX forecast vs actual\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def make_lagged_features(df_in: pd.DataFrame, n_lags: int = 24) -> pd.DataFrame:\n",
        "    \"\"\"Create lag, calendar and temperature features.\"\"\"\n",
        "    df_feat = df_in.copy()\n",
        "    for lag in range(1, n_lags + 1):\n",
        "        df_feat[f\"lag_{lag}\"] = df_feat[\"load\"].shift(lag)\n",
        "    df_feat[\"hour\"] = df_feat.index.hour\n",
        "    df_feat[\"dayofweek\"] = df_feat.index.dayofweek\n",
        "    if df_feat[\"temperature\"].notna().any():\n",
        "        df_feat[\"temperature\"] = df_feat[\"temperature\"].interpolate(limit_direction=\"both\")\n",
        "    return df_feat.dropna()\n",
        "\n",
        "\n",
        "lagged_df = make_lagged_features(df, n_lags=24)\n",
        "lagged_train = lagged_df.loc[train_df.index.intersection(lagged_df.index)]\n",
        "lagged_test = lagged_df.loc[test_df.index.intersection(lagged_df.index)]\n",
        "\n",
        "feature_cols = [c for c in lagged_train.columns if c != \"load\"]\n",
        "X_train = lagged_train[feature_cols].to_numpy()\n",
        "y_train = lagged_train[\"load\"].to_numpy()\n",
        "X_test = lagged_test[feature_cols].to_numpy()\n",
        "y_test = lagged_test[\"load\"].to_numpy()\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_gb_cv(X: np.ndarray, y: np.ndarray, n_splits: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"Time series CV for Gradient Boosting.\"\"\"\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    rows: List[Dict[str, float]] = []\n",
        "    for fold, (tr, val) in enumerate(tscv.split(X), start=1):\n",
        "        X_tr, X_val = X[tr], X[val]\n",
        "        y_tr, y_val = y[tr], y[val]\n",
        "        model = GradientBoostingRegressor(\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_estimators=300,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=3,\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "        y_hat = model.predict(X_val)\n",
        "        rows.append({\n",
        "            \"fold\": fold,\n",
        "            \"rmse\": rmse(y_val, y_hat),\n",
        "            \"mae\": mean_absolute_error(y_val, y_hat),\n",
        "            \"mape\": mape(y_val, y_hat),\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "cv_metrics = evaluate_gb_cv(X_train, y_train, n_splits=5)\n",
        "cv_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.boxplot(data=cv_metrics.melt(id_vars=\"fold\", value_vars=[\"rmse\", \"mape\"]), x=\"variable\", y=\"value\")\n",
        "plt.title(\"GB \u2013 rolling-origin CV errors (Spain)\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()\n",
        "\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        ")\n",
        "gbr_model.fit(X_train, y_train)\n",
        "y_pred_test = gbr_model.predict(X_test)\n",
        "\n",
        "print(\"GBR test RMSE:\", rmse(y_test, y_pred_test))\n",
        "print(\"GBR test MAE: \", mean_absolute_error(y_test, y_pred_test))\n",
        "print(\"GBR test MAPE:\", mape(y_test, y_pred_test))\n",
        "\n",
        "plt.plot(lagged_test.index, y_test, label=\"actual\", alpha=0.8)\n",
        "plt.plot(lagged_test.index, y_pred_test, label=\"GBR\", linestyle=\"--\")\n",
        "plt.title(\"Spanish load \u2013 GBR forecast vs actual\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tier 3 \u2013 Probabilistic Forecasts and Champion\u2013Challenger\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quantile Gradient Boosting\n",
        "quantiles = [0.1, 0.5, 0.9]\n",
        "q_models: Dict[float, GradientBoostingRegressor] = {}\n",
        "for q in quantiles:\n",
        "    m = GradientBoostingRegressor(\n",
        "        loss=\"quantile\",\n",
        "        alpha=q,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=3,\n",
        "    )\n",
        "    m.fit(X_train, y_train)\n",
        "    q_models[q] = m\n",
        "\n",
        "q_preds = {q: m.predict(X_test) for q, m in q_models.items()}\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "idx = lagged_test.index\n",
        "plt.plot(idx, y_test, label=\"actual\", color=\"black\", linewidth=1)\n",
        "plt.plot(idx, q_preds[0.5], label=\"q0.5\", linestyle=\"--\")\n",
        "plt.fill_between(idx, q_preds[0.1], q_preds[0.9], alpha=0.3, label=\"q0.1\u20130.9\")\n",
        "plt.title(\"Spanish load \u2013 quantile GB forecast\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Conformal-style interval from CV residuals\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "abs_resids: List[float] = []\n",
        "for tr, val in tscv.split(X_train):\n",
        "    X_tr, X_val = X_train[tr], X_train[val]\n",
        "    y_tr, y_val = y_train[tr], y_train[val]\n",
        "    m = GradientBoostingRegressor(\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=3,\n",
        "    )\n",
        "    m.fit(X_tr, y_tr)\n",
        "    y_hat = m.predict(X_val)\n",
        "    abs_resids.extend(np.abs(y_val - y_hat))\n",
        "\n",
        "abs_resids = np.array(abs_resids)\n",
        "alpha = 0.1\n",
        "q_conf = float(np.quantile(abs_resids, 1 - alpha))\n",
        "print(\"Approx. 90% absolute error quantile:\", q_conf)\n",
        "\n",
        "lower = y_pred_test - q_conf\n",
        "upper = y_pred_test + q_conf\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(lagged_test.index, y_test, label=\"actual\", color=\"black\")\n",
        "plt.plot(lagged_test.index, y_pred_test, label=\"GBR\", linestyle=\"--\")\n",
        "plt.fill_between(lagged_test.index, lower, upper, alpha=0.3, label=\"conformal band\")\n",
        "plt.title(\"Spanish load \u2013 GBR with conformal-style band\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Champion\u2013challenger\n",
        "y_test_sarimax = test_df[\"load\"].to_numpy()\n",
        "naive = np.repeat(train_df[\"load\"].iloc[-1], len(y_test_sarimax))\n",
        "\n",
        "def model_metrics(name: str, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"rmse\": rmse(y_true, y_pred),\n",
        "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
        "        \"mape\": mape(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "rows: List[Dict[str, float]] = []\n",
        "rows.append(model_metrics(\"NaiveLast\", y_test_sarimax, naive))\n",
        "rows.append(model_metrics(\"SARIMAX\", y_test_sarimax, sarimax_forecast))\n",
        "aligned = min(len(y_test_sarimax), len(y_test))\n",
        "rows.append(model_metrics(\"GBR\", y_test[-aligned:], y_pred_test[-aligned:]))\n",
        "\n",
        "champion_df = pd.DataFrame(rows).set_index(\"model\")\n",
        "champion_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "champion_df.sort_values(\"rmse\").plot(kind=\"bar\")\n",
        "plt.title(\"Spanish load \u2013 champion\u2013challenger (lower is better)\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}