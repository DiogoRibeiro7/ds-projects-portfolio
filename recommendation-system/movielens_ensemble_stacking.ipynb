{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# MovieLens Recommender \u2013 Ensemble & Stacked Models\n\nIn this notebook we focus on **ensembles** for recommender systems on the\nMovieLens `ml-latest-small` dataset.\n\nSo far we have built single models:\n\n- Baselines (global mean, biases).\n- Item-based collaborative filtering.\n- Matrix factorisation.\n\nHere we:\n\n1. Define several **base recommenders**:\n   - Bias model (global + user + item effects).\n   - Item-based k-NN collaborative filtering.\n   - Matrix factorisation (latent factors).\n2. Split the data into:\n   - Base training set (to fit base models).\n   - Meta training set (to fit a **stacking meta-model**).\n   - Test set (final evaluation).\n3. Build a **stacked model**:\n   - Use base model predictions as features.\n   - Train a **linear regression meta-learner** to combine them.\n4. Compare:\n   - Individual base models.\n   - Simple weighted average ensembles.\n   - Stacked ensemble with a learned combiner.\n5. Visualise and interpret:\n   - RMSE comparison.\n   - True vs predicted plots.\n   - Learned weights of the meta-model.\n\nThis gives you a realistic pattern for building **blended recommenders**\nwhere multiple modelling approaches contribute to a final score.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration\n\nWe keep dependencies simple:\n\n- `pandas`, `numpy` \u2013 data wrangling.\n- `matplotlib`, `seaborn` \u2013 plots.\n- `scikit-learn` \u2013 splitting and meta-learning.\n\nAll recommender models are implemented in **plain Python / NumPy** so you\ncan fully inspect and adapt them.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.linear_model import LinearRegression\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_DIR: Path = Path(\"data\") / \"ml-latest-small\"\nRATINGS_PATH: Path = DATA_DIR / \"ratings.csv\"\nMOVIES_PATH: Path = DATA_DIR / \"movies.csv\"\n\nif not RATINGS_PATH.exists():\n    raise FileNotFoundError(\n        f\"Ratings file not found at {RATINGS_PATH.resolve()}. \"\n        \"Please ensure MovieLens 'ml-latest-small' is under data/ml-latest-small/.\"\n    )\n\nratings_df = pd.read_csv(RATINGS_PATH)\nmovies_df: pd.DataFrame | None = None\nif MOVIES_PATH.exists():\n    movies_df = pd.read_csv(MOVIES_PATH)\n\nprint(\"Ratings shape:\", ratings_df.shape)\nratings_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1.1 Utility: RMSE\n\nWe use RMSE as the primary error metric on ratings.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute root mean squared error.\n\n    Args:\n        y_true: True ratings.\n        y_pred: Predicted ratings.\n\n    Returns:\n        RMSE value.\n    \"\"\"\n    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n\n\nsns.histplot(ratings_df[\"rating\"], bins=10)\nplt.title(\"Rating distribution\")\nplt.xlabel(\"Rating\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Train / validation / test splitting for stacking\n\nWe want **three** disjoint sets:\n\n- `base_train` \u2013 used to fit base models.\n- `meta_train` \u2013 used to fit the meta-learner on base model predictions.\n- `test` \u2013 held-out set for final evaluation only.\n\nProcedure:\n\n1. First split full data into `train_full` (80%) and `test` (20%).\n2. Then split `train_full` into `base_train` (80%) and `meta_train` (20%).\n\nIn practice, you could use cross-validation or time-based splits. Here we\nkeep it simple and random.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Step 1: main train / test split\n\ntrain_full_df, test_df = train_test_split(\n    ratings_df,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\n# Step 2: split train_full into base_train and meta_train\n\nbase_train_df, meta_train_df = train_test_split(\n    train_full_df,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\nprint(\"Base train size:\", base_train_df.shape[0])\nprint(\"Meta train size:\", meta_train_df.shape[0])\nprint(\"Test size:      \", test_df.shape[0])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The intuition:\n\n- Base models never see `meta_train` or `test` when fitting.\n- Meta-model sees base predictions on `meta_train` only.\n- Test set is used only once at the end.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Base models\n\nWe implement three base recommenders:\n\n1. **BiasModel** \u2013 global + user + item biases.\n2. **ItemKNNModel** \u2013 item-based k-NN collaborative filtering.\n3. **MatrixFactorizationModel** \u2013 latent factors with biases trained via SGD.\n\nEach model follows a small, consistent interface:\n\n- `fit(df)` \u2013 train from a DataFrame with `userId`, `movieId`, `rating`.\n- `predict_df(df)` \u2013 predict ratings for a DataFrame of pairs.\n\nThis makes it easy to plug them into stacking.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.1 BiasModel\n\nThe bias model uses:\n\n\\begin{align}\n\\hat r_{ui} = \\mu + b_u + b_i\n\\end{align}\n\nWhere:\n\n- `\u03bc` is the global mean rating.\n- `b_u` is user bias.\n- `b_i` is item bias.\n\nIt is simple, fast and often a very strong baseline.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class BiasModel:\n    \"\"\"Global + user + item bias recommender.\n\n    This model estimates a baseline rating as the sum of:\n    - global mean\n    - user-specific deviation\n    - item-specific deviation\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.mu: float | None = None\n        self.user_bias: Dict[int, float] = {}\n        self.item_bias: Dict[int, float] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        \"\"\"Fit bias terms from a ratings DataFrame.\n\n        Args:\n            df: DataFrame with columns `userId`, `movieId`, `rating`.\n        \"\"\"\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        self.mu = float(df[\"rating\"].mean())\n\n        # User and item average deviations from global mean\n        user_mean = df.groupby(\"userId\")[\"rating\"].mean()\n        item_mean = df.groupby(\"movieId\")[\"rating\"].mean()\n\n        self.user_bias = (user_mean - self.mu).to_dict()\n        self.item_bias = (item_mean - self.mu).to_dict()\n\n    def predict_row(self, user_id: int, movie_id: int) -> float:\n        \"\"\"Predict rating for a single user\u2013item pair.\n\n        Args:\n            user_id: User identifier.\n            movie_id: Movie identifier.\n\n        Returns:\n            Predicted rating.\n        \"\"\"\n        if self.mu is None:\n            raise RuntimeError(\"Model has not been fitted.\")\n\n        bu = self.user_bias.get(user_id, 0.0)\n        bi = self.item_bias.get(movie_id, 0.0)\n        return float(self.mu + bu + bi)\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Predict ratings for a DataFrame of pairs.\n\n        Args:\n            df: DataFrame with columns `userId` and `movieId`.\n\n        Returns:\n            Array of predictions aligned with df rows.\n        \"\"\"\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self.predict_row(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nbias_model = BiasModel()\nbias_model.fit(base_train_df)\n\n# Quick check on meta_train\nbias_meta_preds = bias_model.predict_df(meta_train_df)\nprint(\"Bias model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), bias_meta_preds))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.2 ItemKNNModel \u2013 item-based collaborative filtering\n\nThis model:\n\n1. Builds a user\u2013item rating matrix on `base_train`.\n2. Computes cosine similarity between item vectors.\n3. Predicts a rating for `(u, i)` as a weighted average of `u`'s ratings\n   on the **k most similar items** they have rated.\n\nWe use the bias model's global mean as a fallback when no information is\navailable.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class ItemKNNModel:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    Uses cosine similarity between item rating vectors and a weighted\n    average over a user's rated neighbours.\n    \"\"\"\n\n    def __init__(self, k: int = 40, default_rating: float | None = None) -> None:\n        self.k = k\n        self.default_rating = default_rating\n\n        self.user_id_to_index: Dict[int, int] = {}\n        self.item_id_to_index: Dict[int, int] = {}\n        self.R: np.ndarray | None = None  # user-item matrix\n        self.item_sim: np.ndarray | None = None  # item-item similarity\n\n    def fit(self, df: pd.DataFrame) -> None:\n        \"\"\"Fit the k-NN model from a ratings DataFrame.\n\n        Args:\n            df: DataFrame with `userId`, `movieId`, `rating`.\n        \"\"\"\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        unique_users = df[\"userId\"].unique()\n        unique_items = df[\"movieId\"].unique()\n\n        self.user_id_to_index = {uid: idx for idx, uid in enumerate(unique_users)}\n        self.item_id_to_index = {iid: idx for idx, iid in enumerate(unique_items)}\n\n        n_users = len(unique_users)\n        n_items = len(unique_items)\n\n        R = np.zeros((n_users, n_items), dtype=np.float32)\n        for row in df.itertuples(index=False):\n            u_idx = self.user_id_to_index[row.userId]\n            i_idx = self.item_id_to_index[row.movieId]\n            R[u_idx, i_idx] = row.rating\n\n        self.R = R\n        # Item-item cosine similarity\n        self.item_sim = cosine_similarity(R.T)\n\n    def _predict_single(self, user_id: int, movie_id: int) -> float:\n        \"\"\"Predict rating for a single (user, item) pair.\n\n        Args:\n            user_id: User identifier.\n            movie_id: Movie identifier.\n\n        Returns:\n            Predicted rating.\n        \"\"\"\n        if self.R is None or self.item_sim is None:\n            raise RuntimeError(\"Model has not been fitted.\")\n\n        # Fallback default\n        default = float(self.default_rating if self.default_rating is not None else 3.5)\n\n        u_idx = self.user_id_to_index.get(user_id)\n        i_idx = self.item_id_to_index.get(movie_id)\n        if u_idx is None or i_idx is None:\n            return default\n\n        user_ratings = self.R[u_idx, :]\n        sims = self.item_sim[i_idx, :]\n\n        rated_mask = user_ratings > 0\n        rated_indices = np.where(rated_mask)[0]\n        if rated_indices.size == 0:\n            return default\n\n        sims_rated = sims[rated_indices]\n        ratings_rated = user_ratings[rated_indices]\n\n        k_use = min(self.k, rated_indices.size)\n        top_idx = np.argsort(sims_rated)[-k_use:]\n\n        neigh_sims = sims_rated[top_idx]\n        neigh_ratings = ratings_rated[top_idx]\n\n        if np.all(neigh_sims == 0):\n            return float(neigh_ratings.mean())\n\n        pred = float(np.dot(neigh_sims, neigh_ratings) / np.sum(np.abs(neigh_sims)))\n        return pred\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Predict ratings for many (userId, movieId) pairs.\n\n        Args:\n            df: DataFrame with userId, movieId.\n\n        Returns:\n            Predictions as a numpy array.\n        \"\"\"\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self._predict_single(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\n# Use bias global mean as default rating for ItemKNN\nbias_global_mean = float(base_train_df[\"rating\"].mean())\n\nitem_knn_model = ItemKNNModel(k=40, default_rating=bias_global_mean)\nitem_knn_model.fit(base_train_df)\n\nitem_meta_preds = item_knn_model.predict_df(meta_train_df)\nprint(\"Item-kNN model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), item_meta_preds))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.3 MatrixFactorizationModel \u2013 latent factors + biases\n\nWe use a simple matrix factorisation model:\n\n\\begin{align}\n\\hat r_{ui} = \\mu + b_u + b_i + p_u^T q_i\n\\end{align}\n\n- `\u03bc` \u2013 global mean.\n- `b_u`, `b_i` \u2013 biases.\n- `p_u`, `q_i` \u2013 latent vectors of dimension `k`.\n\nTraining is done with **stochastic gradient descent** on observed ratings.\nWe keep the implementation simple and reasonably small so it is easy to\nread.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "@dataclass\nclass MFConfig:\n    n_factors: int = 30\n    n_epochs: int = 15\n    lr: float = 0.01\n    reg: float = 0.05\n\n\nclass MatrixFactorizationModel:\n    \"\"\"Matrix factorisation with biases trained via SGD.\n\n    This is a simple reference implementation suitable for small datasets.\n    \"\"\"\n\n    def __init__(self, config: MFConfig, random_state: int = 42) -> None:\n        self.config = config\n        self.random_state = random_state\n\n        self.mu: float | None = None\n        self.user_bias: Dict[int, float] = {}\n        self.item_bias: Dict[int, float] = {}\n        self.P: Dict[int, np.ndarray] = {}\n        self.Q: Dict[int, np.ndarray] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        \"\"\"Fit the MF model on a ratings DataFrame.\n\n        Args:\n            df: DataFrame with userId, movieId, rating.\n        \"\"\"\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        rng = np.random.default_rng(self.random_state)\n\n        user_ids = df[\"userId\"].unique()\n        item_ids = df[\"movieId\"].unique()\n\n        self.mu = float(df[\"rating\"].mean())\n\n        self.user_bias = {u: 0.0 for u in user_ids}\n        self.item_bias = {i: 0.0 for i in item_ids}\n\n        k = self.config.n_factors\n\n        self.P = {u: 0.1 * rng.standard_normal(k) for u in user_ids}\n        self.Q = {i: 0.1 * rng.standard_normal(k) for i in item_ids}\n\n        lr = self.config.lr\n        reg = self.config.reg\n\n        user_arr = df[\"userId\"].to_numpy()\n        item_arr = df[\"movieId\"].to_numpy()\n        rating_arr = df[\"rating\"].to_numpy()\n\n        n_obs = len(df)\n\n        for epoch in range(self.config.n_epochs):\n            idx = rng.permutation(n_obs)\n            se = 0.0\n\n            for t in idx:\n                u = int(user_arr[t])\n                i = int(item_arr[t])\n                r_ui = float(rating_arr[t])\n\n                bu = self.user_bias[u]\n                bi = self.item_bias[i]\n                pu = self.P[u]\n                qi = self.Q[i]\n\n                pred = self.mu + bu + bi + float(np.dot(pu, qi))\n                err = r_ui - pred\n\n                se += err * err\n\n                # Update biases\n                self.user_bias[u] = bu + lr * (err - reg * bu)\n                self.item_bias[i] = bi + lr * (err - reg * bi)\n\n                # Update latent factors\n                pu_new = pu + lr * (err * qi - reg * pu)\n                qi_new = qi + lr * (err * pu - reg * qi)\n\n                self.P[u] = pu_new\n                self.Q[i] = qi_new\n\n            train_rmse = float(np.sqrt(se / n_obs))\n            print(f\"Epoch {epoch+1}/{self.config.n_epochs} - train RMSE: {train_rmse:.4f}\")\n\n    def predict_single(self, user_id: int, movie_id: int) -> float:\n        \"\"\"Predict rating for a single user\u2013item pair.\n\n        Args:\n            user_id: User identifier.\n            movie_id: Movie identifier.\n\n        Returns:\n            Predicted rating.\n        \"\"\"\n        if self.mu is None:\n            raise RuntimeError(\"Model has not been fitted.\")\n\n        bu = self.user_bias.get(user_id)\n        bi = self.item_bias.get(movie_id)\n        pu = self.P.get(user_id)\n        qi = self.Q.get(movie_id)\n\n        if bu is None or bi is None or pu is None or qi is None:\n            return float(self.mu)\n\n        return float(self.mu + bu + bi + float(np.dot(pu, qi)))\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Predict ratings for a DataFrame of pairs.\n\n        Args:\n            df: DataFrame with userId and movieId.\n\n        Returns:\n            Predictions as a numpy array.\n        \"\"\"\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self.predict_single(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nmf_config = MFConfig(n_factors=30, n_epochs=12, lr=0.01, reg=0.05)\nmf_model = MatrixFactorizationModel(config=mf_config, random_state=RANDOM_STATE)\n\nmf_model.fit(base_train_df)\n\nmf_meta_preds = mf_model.predict_df(meta_train_df)\nprint(\"MF model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), mf_meta_preds))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We now have three base models, all trained on `base_train_df`, and all\nproducing predictions on `meta_train_df`.\n\nNext step: **stacking**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Stacking setup \u2013 meta-features and meta-target\n\nFor stacking we build a new supervised learning problem:\n\n- **Input features**: predictions from base models.\n- **Target**: the true rating.\n\nFor each row `(userId, movieId, rating)` in `meta_train_df` we compute:\n\n```text\nx = [bias_pred, item_knn_pred, mf_pred]\ny = rating\n```\n\nWe then train a **regression model** that learns how to combine the base\nmodels. Here we start with a simple **LinearRegression** (a learned\nweighted average).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Construct meta-training matrix X_meta and target y_meta\n\nX_meta = np.vstack([\n    bias_meta_preds,\n    item_meta_preds,\n    mf_meta_preds,\n]).T\n\ny_meta = meta_train_df[\"rating\"].to_numpy()\n\nprint(\"Meta feature matrix shape:\", X_meta.shape)\nprint(\"First row (bias, itemKNN, MF):\", X_meta[0])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 4.1 Meta-learner: LinearRegression\n\nWe choose `LinearRegression` as a simple, interpretable meta-learner:\n\n- It learns a combination:\n\n\\begin{align}\n\\hat r_{ui}^{\\text{ensemble}} = w_0 + w_1 \\hat r_{ui}^{\\text{bias}} + w_2 \\hat r_{ui}^{\\text{item}} + w_3 \\hat r_{ui}^{\\text{mf}}\n\\end{align}\n\n- Coefficients tell us how much each base model contributes.\n\nYou could also use more complex meta-models (e.g. GradientBoostingRegressor)\nif you suspect non-linear interactions between base models.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "meta_model = LinearRegression()\nmeta_model.fit(X_meta, y_meta)\n\nprint(\"Meta-model coefficients (w1, w2, w3):\", meta_model.coef_)\nprint(\"Meta-model intercept (w0):\", meta_model.intercept_)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The coefficients can be interpreted as **weights** assigned to each base\nmodel. Larger magnitude means a stronger influence.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Evaluation on the test set\n\nWe now evaluate:\n\n1. The three base models separately.\n2. A simple **uniform average** ensemble.\n3. The **stacked ensemble** using the learned meta-model.\n\nAll of these are evaluated on **the same test set**, which was not used\nin fitting either base models or the meta-model.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Base model predictions on test set\n\ny_test = test_df[\"rating\"].to_numpy()\n\nbias_test_preds = bias_model.predict_df(test_df)\nitem_test_preds = item_knn_model.predict_df(test_df)\nmf_test_preds = mf_model.predict_df(test_df)\n\n# Simple uniform average ensemble\nsimple_ensemble_test = (bias_test_preds + item_test_preds + mf_test_preds) / 3.0\n\n# Stacked ensemble predictions on test set\nX_test_meta = np.vstack([\n    bias_test_preds,\n    item_test_preds,\n    mf_test_preds,\n]).T\n\nstacked_test_preds = meta_model.predict(X_test_meta)\n\n# Compute RMSE for all\nresults = {\n    \"BiasModel\": rmse(y_test, bias_test_preds),\n    \"ItemKNNModel\": rmse(y_test, item_test_preds),\n    \"MFModel\": rmse(y_test, mf_test_preds),\n    \"SimpleAverageEnsemble\": rmse(y_test, simple_ensemble_test),\n    \"StackedEnsemble\": rmse(y_test, stacked_test_preds),\n}\n\nresults\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "results_df = pd.DataFrame(\n    {\n        \"model\": list(results.keys()),\n        \"rmse\": list(results.values()),\n    }\n)\n\nresults_df.sort_values(\"rmse\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "sns.barplot(data=results_df, x=\"model\", y=\"rmse\")\nplt.xticks(rotation=20)\nplt.ylabel(\"RMSE (lower is better)\")\nplt.title(\"Base models vs ensembles on test set\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Typically, you will see:\n\n- The best **single model** outperforms the others.\n- The **simple average ensemble** is often competitive or slightly better\n  than many single models.\n- The **stacked ensemble** is at least as good as the best base model,\n  sometimes noticeably better.\n\nThe exact numbers depend on random seeds and hyperparameters.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Visual diagnostics\n\nWe visualise how the stacked ensemble compares to a strong base model\n(usually matrix factorisation) in rating space:\n\n- True vs predicted scatter.\n- Error distribution.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Choose MF as a strong base model for comparison\n\nplt.scatter(y_test, mf_test_preds, alpha=0.2, label=\"MF\")\nplt.scatter(y_test, stacked_test_preds, alpha=0.2, label=\"Stacked\", marker=\"x\")\nplt.plot([0.5, 5], [0.5, 5], linestyle=\"--\", color=\"black\")\nplt.xlabel(\"True rating\")\nplt.ylabel(\"Predicted rating\")\nplt.xlim(0.5, 5.0)\nplt.ylim(0.5, 5.0)\nplt.title(\"True vs predicted: MF vs Stacked Ensemble\")\nplt.legend()\nplt.show()\n\n# Error distributions\nmf_errors = y_test - mf_test_preds\nstacked_errors = y_test - stacked_test_preds\n\nsns.histplot(mf_errors, color=\"tab:blue\", label=\"MF\", kde=False, bins=30, alpha=0.6)\nsns.histplot(stacked_errors, color=\"tab:orange\", label=\"Stacked\", kde=False, bins=30, alpha=0.6)\nplt.title(\"Prediction error distribution (MF vs Stacked)\")\nplt.xlabel(\"True - predicted\")\nplt.legend()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You should see that the stacked ensemble often has slightly smaller\nabsolute errors and a narrower error distribution.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Interpreting the meta-model weights\n\nSince we used `LinearRegression`, we can inspect the learned coefficients\nas a **data-driven weighting scheme** over base models.\n\nExample interpretation:\n\n- If `w2` (item-kNN) is small, the meta-model is effectively ignoring it.\n- If `w3` (MF) is large, the stack heavily trusts the MF model.\n- If `w1` (bias model) is non-zero, it acts as a stabiliser / shrinkage\n  term for cold or uncertain regions.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "coef_names = [\"BiasModel\", \"ItemKNNModel\", \"MFModel\"]\ncoef_values = meta_model.coef_\n\ncoef_df = pd.DataFrame({\"base_model\": coef_names, \"weight\": coef_values})\ncoef_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "sns.barplot(data=coef_df, x=\"base_model\", y=\"weight\")\nplt.title(\"Meta-model weights for base models\")\nplt.ylabel(\"Linear coefficient\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "These weights are not constrained to sum to 1 or be positive, but they\nstill give a good sense of relative importance.\n\nIf you want a **convex combination** (non-negative weights that sum to 1),\ncommon choices include:\n\n- Non-negative least squares.\n- Simple grid search over weight triplets.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Design choices \u2013 why this ensemble setup?\n\n### 8.1 Why these base models?\n\nWe chose three base recommenders with **different biases**:\n\n1. **BiasModel**\n   - Very stable, low-variance, captures global structure.\n   - Handles sparse users/items gracefully.\n\n2. **ItemKNNModel**\n   - Local, memory-based model.\n   - Good at finding highly similar items based on overlapping ratings.\n   - Can exploit neighbourhood structure that MF might smooth over.\n\n3. **MatrixFactorizationModel**\n   - Latent-factor model that captures global interaction patterns.\n   - Often best single model in practice on explicit feedback.\n\nBy combining them, the ensemble can:\n\n- Lean on MF when enough data is available.\n- Use k-NN when a user has strong local neighbourhood signals.\n- Fall back to the bias model in sparse regions.\n\n### 8.2 Why stacking instead of just averaging?\n\nAveraging assumes that **each base model is equally good everywhere**.\nStacking lets the meta-model learn:\n\n- Different relative importance across the rating range.\n- How to correct systematic biases of base models.\n\nFor example, the meta-model may learn that:\n\n- MF is slightly over-confident on very high ratings.\n- Item-kNN is noisy for certain sparse items.\n\n### 8.3 Why a linear meta-model?\n\n- Simple, fast, easy to inspect.\n- Coefficients are directly interpretable as weights.\n- Often a good baseline; you can replace it with non-linear models later.\n\nIf you want more power, you can swap `LinearRegression` for e.g. a\n`GradientBoostingRegressor` while keeping the rest of the pipeline.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Extensions and production considerations\n\nSome natural extensions of this notebook:\n\n1. **More base models**\n   - Add a content-based model (e.g. using genres or tags).\n   - Add external models such as Surprise SVD or LightFM.\n   - Treat their predictions as additional meta-features.\n\n2. **Cross-validated stacking**\n   - Instead of a single `meta_train` split, use K-fold CV:\n     - For each fold, get out-of-fold predictions from base models.\n     - Train meta-model on all out-of-fold predictions.\n\n3. **Ranking-focused ensembles**\n   - Here we optimised RMSE (rating prediction).\n   - For top-N recommendation, you could:\n     - Compute ranking metrics (precision@K, recall@K) per model.\n     - Learn a meta-model over ranking losses or per-user calibration.\n\n4. **Serving ensembles**\n   - In production, you may:\n     - Precompute base-model scores offline.\n     - Store them as features in a feature store.\n     - Use a lightweight online model (e.g. linear layer) to combine them\n       at request time.\n\nThis notebook provides a concrete, end-to-end template for **stacked\nrecommender ensembles**, showing the full path from design to training,\ncombination and evaluation.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}