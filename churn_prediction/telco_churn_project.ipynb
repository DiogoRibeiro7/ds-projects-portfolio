{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# End-to-End Customer Churn Project (Telco Dataset)\n\nIn this notebook we develop an end-to-end customer churn prediction project using\nthe IBM Telco Customer Churn dataset.\n\nWe will follow a typical data science workflow:\n\n1. **Business understanding** \u2013 What is churn and why do we care?\n2. **Data understanding** \u2013 Explore the dataset and basic patterns.\n3. **Data cleaning & preprocessing** \u2013 Handle data types, missing values, and encodings.\n4. **Exploratory data analysis (EDA)** \u2013 Understand how features relate to churn.\n5. **Feature engineering & modelling** \u2013 Build models to predict churn.\n6. **Evaluation & model comparison** \u2013 Compare models with appropriate metrics.\n7. **Interpretation & business insights** \u2013 Translate model results into actions.\n8. **(Optional) Model export** \u2013 Save the best model for later use.\n\nThe target variable is **Churn**, which tells us whether a customer left the\ncompany (Yes) or stayed (No). Our goal is to build a model that predicts\nwhich customers are at high risk of churn, and to understand the key drivers\nbehind churn.\n\n---\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Imports and Configuration\n\nIn this section we import all the libraries that will be used throughout the\nproject and set some basic configuration options.\n\nWe will use:\n\n- `pandas` and `numpy` for data manipulation.\n- `matplotlib` and `seaborn` for visualization.\n- `scikit-learn` for preprocessing, modelling, and evaluation.\n- `typing` for explicit type hints.\n\nWe also set a random seed to make results reproducible.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    roc_auc_score,\n    RocCurveDisplay,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Set plotting style and global options\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_PATH: Path = Path(\"data\") / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n\n# Quick sanity check: fail early if the file is not present.\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Data file not found at {DATA_PATH.resolve()}. \"\n        \"Please download the Telco churn CSV from Kaggle and place it under the 'data/' directory.\"\n    )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Business Understanding\n\n### What is churn?\n\n**Customer churn** (or attrition) happens when a customer stops using a service.\nIn telecom, churn often means cancelling a subscription, which directly impacts\nrecurring revenue.\n\n### Why predict churn?\n\n- Acquiring new customers is usually more expensive than retaining existing ones.\n- If we can **identify high-risk customers early**, we can:\n  - Offer discounts or special plans.\n  - Improve service quality for specific segments.\n  - Prioritize customer support resources.\n\n### Our goal\n\nGiven customer information (contract type, tenure, payment method, monthly\ncharges, etc.), we want to:\n\n1. Predict whether a customer is likely to churn in the near future.\n2. Understand which factors are associated with higher churn risk.\n\nThis allows the business to design targeted **retention campaigns** instead of\ngeneric, expensive actions.\n\n---\n\n**Section summary**\n\nWe framed churn as a business problem: predicting and understanding churn helps\nthe company protect recurring revenue by targeting retention efforts effectively.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Data Loading and Initial Inspection\n\nIn this section we:\n\n1. Load the Telco churn dataset from CSV.\n2. Inspect the first rows to understand the structure.\n3. Check column types, missing values, and basic statistics.\n\nWe are mainly interested in:\n\n- The **target variable**: `Churn`.\n- The **features**: customer demographics, services, contract type, tenure,\n  monthly and total charges, etc.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_telco_data(path: Path) -> pd.DataFrame:\n    \"\"\"Load the Telco customer churn dataset from a CSV file.\n\n    Args:\n        path: Path to the CSV file.\n\n    Returns:\n        pandas DataFrame containing the Telco churn data.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the resulting DataFrame is empty.\n    \"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path!s}\")\n\n    df: pd.DataFrame = pd.read_csv(path)\n\n    if df.empty:\n        raise ValueError(f\"Loaded DataFrame is empty: {path!s}\")\n\n    return df\n\n\ntelco_df: pd.DataFrame = load_telco_data(DATA_PATH)\n\n# Basic inspection\ndisplay(telco_df.head())\ndisplay(telco_df.info())\ndisplay(telco_df.describe(include=\"all\").T)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe successfully loaded the Telco churn dataset and inspected its structure.\nWe now know:\n\n- The dataset has one row per customer.\n- The target `Churn` indicates whether the customer left (Yes) or stayed (No).\n- Many features are categorical (e.g. `gender`, `Contract`, `PaymentMethod`).\n- A few are numeric (e.g. `tenure`, `MonthlyCharges`, `TotalCharges`).\n\nNext we will clean and preprocess the data to make it suitable for modelling.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Data Cleaning and Basic Preprocessing\n\nThe main goals in this step:\n\n1. **Handle data types** \u2013 some numeric columns may be stored as strings.\n2. **Identify and handle missing values**.\n3. **Remove obvious data issues**, like duplicate customers if any.\n\nFor the IBM Telco dataset, a known issue is that `TotalCharges` can contain\nspaces for customers with very low tenure, which makes it a non-numeric column.\nWe will:\n\n- Convert `TotalCharges` to numeric.\n- Coerce errors to `NaN`, then decide how to handle them.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def clean_telco_data(raw_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Clean the Telco dataset: fix data types, handle missing values, and drop duplicates.\n\n    Args:\n        raw_df: Original Telco churn DataFrame.\n\n    Returns:\n        Cleaned DataFrame.\n\n    Raises:\n        ValueError: If critical columns are missing.\n    \"\"\"\n    df = raw_df.copy()\n\n    required_columns: List[str] = [\"customerID\", \"Churn\", \"TotalCharges\"]\n    missing_required: List[str] = [c for c in required_columns if c not in df.columns]\n    if missing_required:\n        raise ValueError(f\"Missing required columns: {missing_required}\")\n\n    # Convert TotalCharges to numeric (spaces or non-numeric values -> NaN)\n    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n\n\n\n    # Count missing values per column\n    missing_counts: pd.Series = df.isna().sum()\n    print(\"Missing values per column:\")\n    display(missing_counts[missing_counts > 0])\n\n    # In this dataset, missing TotalCharges correspond to very new customers.\n    # We will drop rows with missing TotalCharges to simplify the modelling.\n    before_rows: int = df.shape[0]\n    df = df.dropna(subset=[\"TotalCharges\"])\n    after_rows: int = df.shape[0]\n    print(f\"Dropped {before_rows - after_rows} rows with missing TotalCharges.\")\n\n    # Drop duplicate customer IDs if any (should be very rare)\n    before_rows = df.shape[0]\n    df = df.drop_duplicates(subset=[\"customerID\"])\n    after_rows = df.shape[0]\n    print(f\"Dropped {before_rows - after_rows} duplicate customerID rows.\")\n\n    # Optional: reset index\n    df = df.reset_index(drop=True)\n\n    return df\n\n\ntelco_df_clean: pd.DataFrame = clean_telco_data(telco_df)\ndisplay(telco_df_clean.head())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe cleaned the dataset by:\n\n- Converting `TotalCharges` to numeric and dropping rows with missing values.\n- Removing any duplicate `customerID` entries.\n- Resetting the index.\n\nThe data is now in a more consistent state. Next we will perform exploratory\ndata analysis to understand the churn patterns.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Exploratory Data Analysis (EDA)\n\n### 6.1 Churn distribution (class balance)\n\nWe start by understanding the distribution of the target variable `Churn`:\n\n- How many customers churned vs. stayed?\n- Is the dataset imbalanced?\n\nThis will guide our choice of metrics and model evaluation strategy.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def plot_churn_distribution(df: pd.DataFrame, target_col: str = \"Churn\") -> None:\n    \"\"\"Plot the distribution of the churn target variable.\n\n    Args:\n        df: Clean Telco DataFrame.\n        target_col: Name of the churn column.\n\n    Raises:\n        KeyError: If the target column is not in the DataFrame.\n    \"\"\"\n    if target_col not in df.columns:\n        raise KeyError(f\"Column {target_col!r} not found in DataFrame.\")\n\n    churn_counts: pd.Series = df[target_col].value_counts()\n    churn_rate: float = (churn_counts.get(\"Yes\", 0) / churn_counts.sum()) * 100.0\n\n    print(f\"Churn rate: {churn_rate:.2f}%\")\n    ax = sns.countplot(data=df, x=target_col)\n    ax.set_title(\"Churn distribution\")\n    ax.bar_label(ax.containers[0])\n    plt.show()\n\n\nplot_churn_distribution(telco_df_clean)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe observed the number of churned vs. non-churned customers and computed the\noverall churn rate.\n\nTypically, churn is around 25\u201330% for this dataset, which means:\n- The classes are **somewhat imbalanced**, but not extremely.\n- We should pay attention to metrics like **recall** for the churned class,\n  not only overall accuracy.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 6.2 Numerical features vs churn\n\nWe now explore how numeric features such as:\n\n- `tenure` (months as a customer),\n- `MonthlyCharges`,\n- `TotalCharges`\n\nrelate to churn. We look at distributions split by churn status and compare\ntypical values between churned and non-churned customers.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "numeric_cols: List[str] = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n\n# Check that these columns exist\nfor col in numeric_cols:\n    if col not in telco_df_clean.columns:\n        raise KeyError(f\"Expected numeric column {col!r} not found in DataFrame.\")\n\nfig, axes = plt.subplots(1, len(numeric_cols), figsize=(18, 4))\n\nfor ax, col in zip(axes, numeric_cols):\n    sns.kdeplot(\n        data=telco_df_clean,\n        x=col,\n        hue=\"Churn\",\n        common_norm=False,\n        fill=True,\n        alpha=0.5,\n        ax=ax,\n    )\n    ax.set_title(f\"{col} distribution by churn\")\n\n\nplt.tight_layout()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nFrom the numeric feature distributions we usually see patterns like:\n\n- Churned customers often have **shorter tenure** (they leave earlier).\n- Churned customers may have **higher monthly charges**.\n- Total charges can be lower for churned customers (they leave early, so they\n  have not paid for many months).\n\nThese patterns support the business intuition: new or high-cost customers may\nbe more fragile and at higher risk.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 6.3 Categorical features vs churn\n\nWe now look at selected categorical features and their relationship with churn:\n\n- `Contract` \u2013 Month-to-month vs. one or two year contracts.\n- `InternetService` \u2013 DSL, Fiber optic, or no internet service.\n- `PaymentMethod` \u2013 e.g. electronic check, credit card, bank transfer.\n\nWe compute churn rates per category to see where churn is higher.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def compute_churn_rate_by_category(\n    df: pd.DataFrame, category_col: str, target_col: str = \"Churn\"\n) -> pd.DataFrame:\n    \"\"\"Compute churn rate for each category in a given column.\n\n    Args:\n        df: Clean Telco DataFrame.\n        category_col: Name of the categorical column.\n        target_col: Churn column, expected values 'Yes'/'No'.\n\n    Returns:\n        DataFrame with counts and churn rate per category.\n\n    Raises:\n        KeyError: If requested columns are not present.\n    \"\"\"\n    for col in (category_col, target_col):\n        if col not in df.columns:\n            raise KeyError(f\"Column {col!r} not found in DataFrame.\")\n\n    grouped = (\n        df.groupby(category_col)[target_col]\n        .value_counts()\n        .unstack(fill_value=0)\n        .rename(columns={\"Yes\": \"ChurnYes\", \"No\": \"ChurnNo\"})\n    )\n    grouped[\"Total\"] = grouped[\"ChurnYes\"] + grouped[\"ChurnNo\"]\n    grouped[\"ChurnRate\"] = grouped[\"ChurnYes\"] / grouped[\"Total\"]\n    return grouped.sort_values(\"ChurnRate\", ascending=False)\n\n\ncategorical_to_inspect: List[str] = [\"Contract\", \"InternetService\", \"PaymentMethod\"]\n\nfor col in categorical_to_inspect:\n    print(f\"\\n=== Churn rate by {col} ===\")\n    summary_df = compute_churn_rate_by_category(telco_df_clean, col)\n    display(summary_df)\n\n    ax = sns.barplot(\n        data=summary_df.reset_index(),\n        x=col,\n        y=\"ChurnRate\",\n        order=summary_df.index,\n    )\n    ax.set_title(f\"Churn rate by {col}\")\n    ax.set_ylabel(\"Churn rate\")\n    ax.set_xlabel(col)\n    ax.set_ylim(0, 1)\n    ax.bar_label(ax.containers[0], fmt=\"%.2f\")\n    plt.xticks(rotation=30)\n    plt.tight_layout()\n    plt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe inspected churn rates for key categorical variables.\n\nTypical findings:\n\n- **Month-to-month contracts** often have much higher churn than longer-term\n  contracts.\n- Certain internet types (e.g. fiber) may be associated with higher churn.\n- Some payment methods (e.g. electronic check) can correlate with higher churn.\n\nThese insights hint at potential **intervention points**, such as:\n- Encouraging customers to move from month-to-month to longer contracts.\n- Investigating service quality for high-churn segments.\n- Adjusting payment experience for risky payment methods.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Train\u2013Test Split and Feature Preprocessing\n\nBefore building models, we need to:\n\n1. Separate **features (X)** and **target (y)**.\n2. Split the data into **training** and **test** sets.\n3. Define preprocessing:\n   - One-hot encode categorical variables.\n   - Scale numerical variables (for models like logistic regression).\n\nWe will:\n\n- Drop `customerID` (identifier, not useful for prediction).\n- Use `ColumnTransformer` + `Pipeline` from scikit-learn to keep preprocessing\n  and modelling together in a clean way.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Separate target and features\nTARGET_COL: str = \"Churn\"\n\nif TARGET_COL not in telco_df_clean.columns:\n    raise KeyError(f\"Target column {TARGET_COL!r} not found in DataFrame.\")\n\nX: pd.DataFrame = telco_df_clean.drop(columns=[TARGET_COL, \"customerID\"])\ny: pd.Series = telco_df_clean[TARGET_COL].map({\"No\": 0, \"Yes\": 1})  # binary 0/1\n\n# Identify categorical and numerical columns\ncategorical_cols: List[str] = [\n    col for col in X.columns if X[col].dtype == \"O\"\n]\nnumeric_cols: List[str] = [\n    col for col in X.columns if col not in categorical_cols\n]\n\nprint(\"Categorical columns:\", categorical_cols)\nprint(\"Numeric columns:\", numeric_cols)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    stratify=y,\n    random_state=RANDOM_STATE,\n)\n\nprint(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n\n# Define preprocessing transformer\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"scaler\", StandardScaler()),\n    ]\n)\n\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n    ]\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_cols),\n        (\"cat\", categorical_transformer, categorical_cols),\n    ]\n)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe:\n\n- Prepared features and target, mapping `Churn` to 0/1.\n- Performed a stratified train\u2013test split (preserving the churn ratio).\n- Defined a preprocessing pipeline that:\n  - Scales numerical features.\n  - One-hot encodes categorical features.\n\nNext we will build baseline and more advanced models using this preprocessing.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Modelling\n\nWe will compare three models:\n\n1. **Baseline** \u2013 `DummyClassifier` that predicts the majority class.\n2. **Logistic Regression** \u2013 simple, interpretable linear model.\n3. **Random Forest** \u2013 non-linear, tree-based ensemble.\n\nFirst we define a helper function that:\n\n- Fits a model pipeline.\n- Computes metrics on train and test sets.\n- Prints a classification report and ROC-AUC.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from sklearn.base import BaseEstimator\n\n\ndef evaluate_classifier(\n    name: str,\n    model: BaseEstimator,\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n) -> Dict[str, float]:\n    \"\"\"Fit a classifier and evaluate it on train and test data.\n\n    Args:\n        name: Model name (for printing).\n        model: Unfitted sklearn estimator or pipeline.\n        X_train: Training features.\n        X_test: Test features.\n        y_train: Training labels (0/1).\n        y_test: Test labels (0/1).\n\n    Returns:\n        Dictionary with key metrics on the test set.\n\n    Raises:\n        ValueError: If y_train or y_test contain values outside {0, 1}.\n    \"\"\"\n    # Type / value checks\n    unique_y_train = set(y_train.unique())\n    unique_y_test = set(y_test.unique())\n    allowed_values = {0, 1}\n    if not unique_y_train.issubset(allowed_values) or not unique_y_test.issubset(\n        allowed_values\n    ):\n        raise ValueError(\"y_train and y_test should contain only 0/1 values.\")\n\n    print(f\"\\n===== {name} =====\")\n    model.fit(X_train, y_train)\n\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n\n    # Some models may not support predict_proba; handle gracefully\n    if hasattr(model, \"predict_proba\"):\n        y_proba_test = model.predict_proba(X_test)[:, 1]\n        roc_auc = roc_auc_score(y_test, y_proba_test)\n    else:\n        y_proba_test = None\n        roc_auc = np.nan\n\n    acc_train = accuracy_score(y_train, y_pred_train)\n    acc_test = accuracy_score(y_test, y_pred_test)\n\n    print(f\"Train accuracy: {acc_train:.3f}\")\n    print(f\"Test accuracy:  {acc_test:.3f}\")\n    if not np.isnan(roc_auc):\n        print(f\"Test ROC-AUC:  {roc_auc:.3f}\")\n\n    print(\"\\nClassification report (test):\")\n    print(classification_report(y_test, y_pred_test, target_names=[\"No churn\", \"Churn\"]))\n\n    # Confusion matrix\n    cm = confusion_matrix(y_test, y_pred_test)\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\",\n        xticklabels=[\"Pred No\", \"Pred Yes\"],\n        yticklabels=[\"True No\", \"True Yes\"],\n    )\n    plt.title(f\"Confusion matrix - {name}\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.show()\n\n    # ROC curve if probabilities available\n    if y_proba_test is not None:\n        RocCurveDisplay.from_predictions(y_test, y_proba_test)\n        plt.title(f\"ROC curve - {name}\")\n        plt.show()\n\n    return {\n        \"model\": name,\n        \"train_accuracy\": acc_train,\n        \"test_accuracy\": acc_test,\n        \"roc_auc\": float(roc_auc) if not np.isnan(roc_auc) else np.nan,\n    }\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 8.2 Baseline model: Dummy classifier\n\nA baseline helps us assess whether our complex models are actually useful.\n\nWe use a `DummyClassifier` that always predicts the most frequent class\n(usually \"No churn\"). Any reasonable model should clearly outperform this\nbaseline, especially in recall for the churn class.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "baseline_clf = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\"clf\", DummyClassifier(strategy=\"most_frequent\", random_state=RANDOM_STATE)),\n    ]\n)\n\nbaseline_metrics = evaluate_classifier(\n    name=\"Baseline (Most Frequent)\",  # type: ignore[arg-type]\n    model=baseline_clf,\n    X_train=X_train,\n    X_test=X_test,\n    y_train=y_train,\n    y_test=y_test,\n)\nbaseline_metrics\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 8.3 Logistic Regression\n\nLogistic regression is:\n\n- A linear model for binary classification.\n- Interpretable via its coefficients.\n- A good first non-trivial model.\n\nWe train it on the preprocessed features and evaluate it on the test set.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "log_reg_clf = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\n            \"clf\",\n            LogisticRegression(\n                max_iter=1000,\n                random_state=RANDOM_STATE,\n                n_jobs=-1,\n            ),\n        ),\n    ]\n)\n\nlog_reg_metrics = evaluate_classifier(\n    name=\"Logistic Regression\",  # type: ignore[arg-type]\n    model=log_reg_clf,\n    X_train=X_train,\n    X_test=X_test,\n    y_train=y_train,\n    y_test=y_test,\n)\nlog_reg_metrics\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 8.4 Random Forest\n\nRandom Forest is an ensemble of decision trees:\n\n- Captures non-linear relationships and interactions.\n- Provides feature importance measures.\n- Often a strong baseline for tabular data.\n\nWe train a reasonably sized forest (not heavily tuned) and compare its\nperformance against logistic regression and the dummy baseline.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "rf_clf = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\n            \"clf\",\n            RandomForestClassifier(\n                n_estimators=200,\n                max_depth=None,\n                min_samples_split=4,\n                min_samples_leaf=2,\n                random_state=RANDOM_STATE,\n                n_jobs=-1,\n            ),\n        ),\n    ]\n)\n\nrf_metrics = evaluate_classifier(\n    name=\"Random Forest\",  # type: ignore[arg-type]\n    model=rf_clf,\n    X_train=X_train,\n    X_test=X_test,\n    y_train=y_train,\n    y_test=y_test,\n)\nrf_metrics\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 8.5 Model comparison\n\nWe now put the main metrics side-by-side for:\n\n- Baseline dummy model\n- Logistic Regression\n- Random Forest\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "results_df = pd.DataFrame([baseline_metrics, log_reg_metrics, rf_metrics])\ndisplay(results_df)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe trained and evaluated three models:\n\n- **Dummy baseline** \u2013 low recall and low ROC-AUC, as expected.\n- **Logistic Regression** \u2013 improved accuracy and recall; provides interpretable coefficients.\n- **Random Forest** \u2013 usually highest ROC-AUC and better recall for churn at the cost\n  of interpretability.\n\nThe exact numbers may differ slightly due to randomness, but a good model should\nsubstantially outperform the dummy classifier, especially on the churn (positive) class.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Feature Importance and Interpretation\n\nUnderstanding *why* the model predicts churn is as important as the predictive\nperformance. Here we:\n\n1. Extract feature importances from the Random Forest.\n2. Map them back to the original feature names (after one-hot encoding).\n3. Interpret the most influential drivers of churn.\n\nThis helps translate model results into actionable business insights.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def get_feature_names_from_preprocessor(\n    preprocessor: ColumnTransformer,\n) -> List[str]:\n    \"\"\"Extract the final feature names after the ColumnTransformer.\n\n    Args:\n        preprocessor: Fitted ColumnTransformer.\n\n    Returns:\n        List of feature names corresponding to the transformed columns.\n    \"\"\"\n    feature_names: List[str] = []\n\n    # Numeric features (passthrough -> just column names after scaler)\n    num_features: List[str] = preprocessor.named_transformers_[\"num\"].named_steps[\n        \"scaler\"\n    ].get_feature_names_out(numeric_cols).tolist()\n    feature_names.extend(num_features)\n\n    # Categorical features (OneHotEncoder produces multiple columns)\n    ohe: OneHotEncoder = preprocessor.named_transformers_[\"cat\"].named_steps[\"encoder\"]\n    cat_features: List[str] = ohe.get_feature_names_out(categorical_cols).tolist()\n    feature_names.extend(cat_features)\n\n    return feature_names\n\n\n# Fit the RF pipeline to the full training data (if not already fitted)\nrf_clf.fit(X_train, y_train)\n\n# Extract the underlying RF model and preprocessor\nfitted_preprocessor: ColumnTransformer = rf_clf.named_steps[\"preprocess\"]  # type: ignore[assignment]\nfitted_rf: RandomForestClassifier = rf_clf.named_steps[\"clf\"]  # type: ignore[assignment]\n\nrf_feature_importances: np.ndarray = fitted_rf.feature_importances_\nrf_feature_names: List[str] = get_feature_names_from_preprocessor(fitted_preprocessor)\n\n# Build a DataFrame of importances\nimportance_df = (\n    pd.DataFrame(\n        {\n            \"feature\": rf_feature_names,\n            \"importance\": rf_feature_importances,\n        }\n    )\n    .sort_values(\"importance\", ascending=False)\n    .head(20)\n)\n\ndisplay(importance_df)\n\nplt.figure(figsize=(8, 6))\nsns.barplot(data=importance_df, x=\"importance\", y=\"feature\")\nplt.title(\"Top 20 feature importances (Random Forest)\")\nplt.tight_layout()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nThe feature importance plot highlights which variables most influence the\nRandom Forest predictions. Typical important drivers include:\n\n- Contract type (e.g. month-to-month vs. long-term).\n- Tenure.\n- Monthly and total charges.\n- Internet-related services and additional options.\n\nThese results align with our EDA and business intuition. They can guide:\n\n- Targeted offers (e.g. discounts for high-risk segments).\n- Product changes (e.g. improving problematic services).\n- Contract design (e.g. incentives to move away from high-risk contract types).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Business Insights and Next Steps\n\n### Key findings\n\nFrom our analysis and models, we learned that:\n\n1. **Churn rate** is non-trivial (around a quarter to a third of customers), so\n   retention strategies are important.\n2. **Short-tenure, month-to-month customers** are at much higher risk.\n3. Customers with **higher monthly charges** also show increased churn, which may\n   reflect price sensitivity or perceived value issues.\n4. Certain **services and payment methods** correlate with higher churn.\n\n### Possible actions\n\nBased on these findings, the company could consider:\n\n- **Onboarding and early-life programs** for new customers to increase engagement\n  and satisfaction.\n- **Targeted offers** (discounts, service bundles) for high-risk segments indicated\n  by the model.\n- **Reviewing pricing and plan structure** for customers with very high monthly\n  charges.\n- **Monitoring service quality** for configurations with high churn rates.\n\n### Model improvements and future work\n\nTo make this churn model production-ready, we could:\n\n- Tune hyperparameters using cross-validation (GridSearchCV/RandomizedSearchCV).\n- Try additional algorithms (Gradient Boosting, XGBoost, etc.).\n- Use **calibrated probabilities** and choose thresholds based on business costs\n  (e.g. cost of contacting a customer vs. cost of losing them).\n- Implement regular retraining and monitoring (data drift, performance decay).\n- Combine with time-based or survival models to predict **time-to-churn**.\n\n---\n\n**Overall summary**\n\nWe built a complete churn prediction workflow:\n\n- Framed the business problem.\n- Explored and cleaned the Telco churn dataset.\n- Performed EDA to understand patterns of churn.\n- Built and compared several models.\n- Interpreted model outputs to derive business-oriented insights.\n\nThis notebook can serve as a solid template for churn analysis in other\nindustries (banking, subscriptions, SaaS, etc.) by swapping in the relevant\ndataset and adjusting the features accordingly.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}