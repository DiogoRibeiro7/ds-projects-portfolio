{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modern Traffic Volume Forecasting (Metro Interstate)\n",
        "\n",
        "This notebook builds a **modern traffic forecasting project** on top of the\n",
        "widely used **Metro Interstate Traffic Volume** dataset (Kaggle).\n",
        "\n",
        "We focus on short-term traffic volume prediction with:\n",
        "\n",
        "- Solid time-series EDA.\n",
        "- Feature engineering with calendar, weather and lags.\n",
        "- Baseline and tree-based models (sklearn).\n",
        "- A **sequence deep learning model (LSTM)** for traffic volume.\n",
        "- An optional section sketching a **Temporal Fusion Transformer** setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. How to run this notebook\n",
        "\n",
        "1. Download the **Metro Interstate Traffic Volume** dataset from Kaggle.\n",
        "2. Save it as:\n",
        "\n",
        "   ```text\n",
        "   data/traffic_volume.csv\n",
        "   ```\n",
        "\n",
        "3. Install required packages:\n",
        "\n",
        "   ```bash\n",
        "   pip install numpy pandas matplotlib scikit-learn torch\n",
        "   ```\n",
        "\n",
        "   For the optional Temporal Fusion Transformer section:\n",
        "\n",
        "   ```bash\n",
        "   pip install pytorch-lightning pytorch-forecasting\n",
        "   ```\n",
        "\n",
        "4. Run this notebook top-to-bottom in Jupyter / VS Code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and basic config\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (11, 5)\n",
        "RANDOM_STATE: int = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "DATA_PATH: Path = Path(\"data\") / \"traffic_volume.csv\"\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Expected dataset at {DATA_PATH.resolve()}\\n\"\n",
        "        \"Download 'Metro Interstate Traffic Volume' and save as 'data/traffic_volume.csv'.\"\n",
        "    )\n",
        "\n",
        "raw_df: pd.DataFrame = pd.read_csv(DATA_PATH)\n",
        "raw_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Basic cleaning and timestamp handling\n",
        "\n",
        "We parse the `date_time` column, sort by time and keep the main fields we\n",
        "need for forecasting traffic volume.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_traffic_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean the raw Metro traffic dataframe.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Raw dataframe as loaded from the Kaggle CSV.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Cleaned dataframe indexed by timestamp, with a `traffic_volume` column\n",
        "        and helper columns for weather and calendar.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    if \"date_time\" not in df.columns:\n",
        "        raise ValueError(\"Expected a 'date_time' column in traffic dataset.\")\n",
        "\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"date_time\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
        "\n",
        "    # Set index and ensure regular hourly frequency (dataset is hourly)\n",
        "    df = df.set_index(\"timestamp\")\n",
        "    df = df.asfreq(\"H\")\n",
        "\n",
        "    # Basic numeric conversions\n",
        "    num_cols: List[str] = [\n",
        "        \"traffic_volume\",\n",
        "        \"temp\",\n",
        "        \"rain_1h\",\n",
        "        \"snow_1h\",\n",
        "        \"clouds_all\",\n",
        "    ]\n",
        "    for col in num_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    df = df.dropna(subset=[\"traffic_volume\"])\n",
        "    return df\n",
        "\n",
        "\n",
        "df: pd.DataFrame = clean_traffic_df(raw_df)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Quick EDA: traffic volume over time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[\"traffic_volume\"].plot(alpha=0.7)\n",
        "plt.title(\"Traffic volume over time\")\n",
        "plt.ylabel(\"Vehicles / hour\")\n",
        "plt.show()\n",
        "\n",
        "# Daily pattern example\n",
        "sample_start: pd.Timestamp = df.index.min() + pd.Timedelta(days=7)\n",
        "sample_end: pd.Timestamp = sample_start + pd.Timedelta(days=7)\n",
        "sample = df.loc[sample_start:sample_end]\n",
        "sample[\"traffic_volume\"].plot()\n",
        "plt.title(\"Example week \u2013 hourly traffic volume\")\n",
        "plt.ylabel(\"Vehicles / hour\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature engineering\n",
        "\n",
        "We build a supervised learning table with:\n",
        "\n",
        "- Calendar features: hour of day, day of week, weekend/holiday flags.\n",
        "- Weather features: temperature, rain, snow, clouds.\n",
        "- Lag features: previous 1, 2, 3, 24, 24*7 hours of traffic volume.\n",
        "- Target: next hour's traffic volume.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def add_calendar_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Add basic calendar features to the traffic dataframe.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Dataframe indexed by datetime.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Same dataframe with added calendar columns.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"hour\"] = df.index.hour\n",
        "    df[\"dayofweek\"] = df.index.dayofweek\n",
        "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n",
        "    df[\"month\"] = df.index.month\n",
        "    return df\n",
        "\n",
        "\n",
        "def add_lagged_target(df: pd.DataFrame, target_col: str, lags: List[int]) -> pd.DataFrame:\n",
        "    \"\"\"Add lagged versions of a target column.\n",
        "\n",
        "    Each lag `k` creates a column `f\"{target_col}_lag_{k}\"`.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    for lag in lags:\n",
        "        df[f\"{target_col}_lag_{lag}\"] = df[target_col].shift(lag)\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_supervised_frame(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Build a supervised learning table for next-hour traffic forecasting.\n",
        "\n",
        "    The target is the traffic volume one step ahead.\n",
        "    \"\"\"\n",
        "    df_feat = add_calendar_features(df)\n",
        "    df_feat = add_lagged_target(df_feat, \"traffic_volume\", [1, 2, 3, 24, 24 * 7])\n",
        "\n",
        "    # Target: next hour's traffic volume\n",
        "    df_feat[\"target\"] = df_feat[\"traffic_volume\"].shift(-1)\n",
        "\n",
        "    df_feat = df_feat.dropna()\n",
        "    return df_feat\n",
        "\n",
        "\n",
        "sup_df: pd.DataFrame = build_supervised_frame(df)\n",
        "sup_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Train / validation / test split\n",
        "\n",
        "We keep the last 20% of observations as test, the previous 20% as validation,\n",
        "and the rest as training data (time-ordered).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def time_series_train_val_test_split(df: pd.DataFrame, val_frac: float = 0.2, test_frac: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Time-ordered train/val/test split.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Supervised dataframe sorted by time.\n",
        "    val_frac : float\n",
        "        Fraction of data to allocate to validation.\n",
        "    test_frac : float\n",
        "        Fraction of data to allocate to test.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (train_df, val_df, test_df)\n",
        "    \"\"\"\n",
        "    n: int = len(df)\n",
        "    n_test: int = int(n * test_frac)\n",
        "    n_val: int = int(n * val_frac)\n",
        "\n",
        "    test_df = df.iloc[-n_test:]\n",
        "    val_df = df.iloc[-(n_test + n_val) : -n_test]\n",
        "    train_df = df.iloc[: -(n_test + n_val)]\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "train_df, val_df, test_df = time_series_train_val_test_split(sup_df)\n",
        "len(train_df), len(val_df), len(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Feature/target matrices and scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "feature_cols: List[str] = [\n",
        "    c\n",
        "    for c in sup_df.columns\n",
        "    if c\n",
        "    not in [\n",
        "        \"target\",\n",
        "        \"traffic_volume\",\n",
        "        \"date_time\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "X_train = train_df[feature_cols].to_numpy()\n",
        "y_train = train_df[\"target\"].to_numpy()\n",
        "X_val = val_df[feature_cols].to_numpy()\n",
        "y_val = val_df[\"target\"].to_numpy()\n",
        "X_test = test_df[feature_cols].to_numpy()\n",
        "y_test = test_df[\"target\"].to_numpy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_scaled.shape, X_val_scaled.shape, X_test_scaled.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Metrics helper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Root mean squared error.\"\"\"\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "\n",
        "def evaluate_regression(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"Compute common regression metrics.\"\"\"\n",
        "    return {\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"rmse\": rmse(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "\n",
        "def print_metrics(name: str, y_true: np.ndarray, y_pred: np.ndarray) -> None:\n",
        "    \"\"\"Print metrics for a model with a short label.\"\"\"\n",
        "    metrics = evaluate_regression(y_true, y_pred)\n",
        "    print(f\"{name}: MAE={metrics['mae']:.2f}, RMSE={metrics['rmse']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Baseline models\n",
        "\n",
        "We use simple baselines based on the target itself:\n",
        "\n",
        "- **Na\u00efve**: predict the last observed traffic volume.\n",
        "- **Daily seasonal na\u00efve**: predict the value from the same hour of the\n",
        "  previous day (lag 24).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Align baselines with the supervised table\n",
        "naive_pred = test_df[\"traffic_volume\"].to_numpy()  # predict current hour as next hour\n",
        "daily_naive_pred = test_df[\"traffic_volume_lag_24\"].to_numpy()\n",
        "\n",
        "print_metrics(\"Naive\", y_test, naive_pred)\n",
        "print_metrics(\"DailyNaive\", y_test, daily_naive_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. HistGradientBoostingRegressor (tree-based, modern sklearn)\n",
        "\n",
        "We now train a **HistGradientBoostingRegressor**, which is a strong baseline\n",
        "for tabular time-series forecasting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hgb = HistGradientBoostingRegressor(\n",
        "    max_depth=8,\n",
        "    learning_rate=0.05,\n",
        "    max_iter=500,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "hgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_val_pred_hgb = hgb.predict(X_val_scaled)\n",
        "y_test_pred_hgb = hgb.predict(X_test_scaled)\n",
        "\n",
        "print_metrics(\"HGB (val)\", y_val, y_val_pred_hgb)\n",
        "print_metrics(\"HGB (test)\", y_test, y_test_pred_hgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Visual check \u2013 predictions vs actuals (test set)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_plot: int = 7 * 24  # one week\n",
        "plt.plot(test_df.index[:n_plot], y_test[:n_plot], label=\"actual\")\n",
        "plt.plot(test_df.index[:n_plot], y_test_pred_hgb[:n_plot], label=\"HGB\", linestyle=\"--\")\n",
        "plt.title(\"Test set \u2013 actual vs HGB predictions (first week)\")\n",
        "plt.ylabel(\"Traffic volume\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Sequence deep learning model (LSTM)\n",
        "\n",
        "Next we build a simple **LSTM-based sequence model**:\n",
        "\n",
        "- Input: sliding windows of past features of length `seq_len`.\n",
        "- Output: next-hour traffic volume.\n",
        "- Trained with MSE loss and Adam.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TrafficSequenceDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for sliding window sequences.\n",
        "\n",
        "    Each item is a pair (X_seq, y), where X_seq has shape\n",
        "    (seq_len, n_features) and y is a scalar target.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        features: np.ndarray,\n",
        "        targets: np.ndarray,\n",
        "        seq_len: int,\n",
        "    ) -> None:\n",
        "        assert len(features) == len(targets), \"Features and targets must align.\"\n",
        "        self.features = features.astype(np.float32)\n",
        "        self.targets = targets.astype(np.float32)\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.features) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        x_seq = self.features[idx : idx + self.seq_len]\n",
        "        y = self.targets[idx + self.seq_len]\n",
        "        return torch.from_numpy(x_seq), torch.tensor(y)\n",
        "\n",
        "\n",
        "class LSTMRegressor(nn.Module):\n",
        "    \"\"\"Simple many-to-one LSTM regressor for time series.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_features: int,\n",
        "        hidden_size: int = 64,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=n_features,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        output, _ = self.lstm(x)\n",
        "        last_hidden = output[:, -1, :]\n",
        "        return self.fc(last_hidden).squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "seq_len: int = 24  # use past 24 hours\n",
        "batch_size: int = 64\n",
        "n_features: int = X_train_scaled.shape[1]\n",
        "\n",
        "train_dataset = TrafficSequenceDataset(X_train_scaled, y_train, seq_len)\n",
        "val_dataset = TrafficSequenceDataset(X_val_scaled, y_val, seq_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMRegressor(n_features=n_features, hidden_size=64, num_layers=2, dropout=0.2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    n_epochs: int = 10,\n",
        ") -> None:\n",
        "    \"\"\"Train the LSTM model with basic logging.\"\"\"\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        model.train()\n",
        "        train_losses: List[float] = []\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(float(loss.item()))\n",
        "\n",
        "        model.eval()\n",
        "        val_losses: List[float] = []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch = X_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                preds = model(X_batch)\n",
        "                loss = criterion(preds, y_batch)\n",
        "                val_losses.append(float(loss.item()))\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"train_loss={np.mean(train_losses):.4f} | val_loss={np.mean(val_losses):.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "# Training the LSTM may take a few minutes depending on hardware.\n",
        "train_lstm(model, train_loader, val_loader, n_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Evaluate the LSTM on the test set\n",
        "\n",
        "We construct test sequences and compare against the HGB baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test_dataset = TrafficSequenceDataset(X_test_scaled, y_test, seq_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "y_test_lstm: List[float] = []\n",
        "y_true_lstm: List[float] = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        preds = model(X_batch).cpu().numpy()\n",
        "        y_test_lstm.append(preds)\n",
        "        y_true_lstm.append(y_batch.numpy())\n",
        "\n",
        "y_test_lstm_arr = np.concatenate(y_test_lstm)\n",
        "y_true_lstm_arr = np.concatenate(y_true_lstm)\n",
        "\n",
        "print_metrics(\"LSTM (test)\", y_true_lstm_arr, y_test_lstm_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Visual comparison: HGB vs LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_plot_seq: int = 7 * 24\n",
        "plt.plot(test_df.index[seq_len : seq_len + n_plot_seq], y_true_lstm_arr[:n_plot_seq], label=\"actual\")\n",
        "plt.plot(test_df.index[seq_len : seq_len + n_plot_seq], y_test_pred_hgb[seq_len : seq_len + n_plot_seq], label=\"HGB\")\n",
        "plt.plot(test_df.index[seq_len : seq_len + n_plot_seq], y_test_lstm_arr[:n_plot_seq], label=\"LSTM\", linestyle=\"--\")\n",
        "plt.title(\"Test set \u2013 actual vs HGB vs LSTM (first week with sequences)\")\n",
        "plt.ylabel(\"Traffic volume\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Optional: Temporal Fusion Transformer (TFT) sketch\n",
        "\n",
        "Below is a **sketch** of how you could set up a Temporal Fusion Transformer\n",
        "using `pytorch-forecasting`. It is not runnable without installing\n",
        "`pytorch-forecasting` and adapting the data pipeline, but it shows the core\n",
        "ideas and configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This cell is illustrative and may require adaptation to run.\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "    from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "    from pytorch_forecasting.metrics import QuantileLoss\n",
        "\n",
        "    has_tft = True\n",
        "except ImportError:\n",
        "    has_tft = False\n",
        "    print(\"pytorch-forecasting not installed; skip TFT example or install it first.\")\n",
        "\n",
        "if has_tft:\n",
        "    # Example max encoder/decoder lengths\n",
        "    max_encoder_length = 24 * 7\n",
        "    max_prediction_length = 24\n",
        "\n",
        "    df_tft = sup_df.copy()\n",
        "    df_tft[\"time_idx\"] = np.arange(len(df_tft))\n",
        "    df_tft[\"series\"] = 0  # single series id\n",
        "\n",
        "    training_cutoff = df_tft[\"time_idx\"].max() - max_prediction_length * 2\n",
        "\n",
        "    tft_dataset = TimeSeriesDataSet(\n",
        "        df_tft,\n",
        "        time_idx=\"time_idx\",\n",
        "        target=\"target\",\n",
        "        group_ids=[\"series\"],\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        max_prediction_length=max_prediction_length,\n",
        "        time_varying_unknown_reals=[\"target\"],\n",
        "        time_varying_known_reals=[\"hour\", \"dayofweek\", \"is_weekend\", \"month\"],\n",
        "        static_categoricals=None,\n",
        "        target_normalizer=None,\n",
        "        allow_missing_timesteps=True,\n",
        "    )\n",
        "\n",
        "    train_tft, val_tft = tft_dataset.split_before(training_cutoff)\n",
        "    train_loader_tft = train_tft.to_dataloader(train=True, batch_size=64)\n",
        "    val_loader_tft = val_tft.to_dataloader(train=False, batch_size=64)\n",
        "\n",
        "    tft = TemporalFusionTransformer.from_dataset(\n",
        "        train_tft,\n",
        "        learning_rate=1e-3,\n",
        "        hidden_size=32,\n",
        "        attention_head_size=4,\n",
        "        dropout=0.1,\n",
        "        loss=QuantileLoss(),\n",
        "    )\n",
        "\n",
        "    trainer = pl.Trainer(max_epochs=10, accelerator=\"auto\")\n",
        "    trainer.fit(tft, train_dataloaders=train_loader_tft, val_dataloaders=val_loader_tft)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary\n",
        "\n",
        "In this notebook we:\n",
        "\n",
        "- Built a **modern traffic forecasting** pipeline on the Metro dataset.\n",
        "- Engineered calendar and lag features and used a **time-aware split**.\n",
        "- Compared simple baselines with a strong tree-based model\n",
        "  (**HistGradientBoostingRegressor**).\n",
        "- Implemented a **sequence LSTM model** and compared it against HGB.\n",
        "- Sketched how to plug in a **Temporal Fusion Transformer** using\n",
        "  `pytorch-forecasting` for probabilistic, attention-based forecasting.\n",
        "\n",
        "From here you can:\n",
        "\n",
        "- Extend to multi-step (24h) forecasting.\n",
        "- Add richer weather sources or incident flags.\n",
        "- Move the LSTM / TFT into a proper training loop with GPU and logging.\n"
      ]
    }
  ]
}