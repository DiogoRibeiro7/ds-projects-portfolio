{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Bank Customer Churn \u2013 Lifetime Value and Retention Prioritisation\n\nThis notebook is a **new churn project** focused on combining:\n\n- **Churn modelling** (probability a customer exits), and\n- **Customer Lifetime Value (CLV)** approximation,\n\nto build **priority segments for retention**.\n\nWe use the common **Bank Customer Churn** dataset (`Churn_Modelling.csv`) and:\n\n1. Build a supervised churn model (classification) to estimate `P(Exited = 1)`.\n2. Approximate customer value using simple proxies from the dataset.\n3. Combine churn risk and value into **actionable segments**.\n4. Discuss which customers should be prioritised for retention campaigns.\n\nThe goal is to move from *\"Who will churn?\"* to *\"Which churners matter most?\"*.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration\n\nWe use:\n\n- `pandas`, `numpy` for data handling.\n- `matplotlib`, `seaborn` for visualisation.\n- `scikit-learn` for modelling and evaluation.\n\nWe assume the dataset is available at:\n\n```text\ndata/Churn_Modelling.csv\n```\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, List\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    roc_auc_score,\n    RocCurveDisplay,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.base import BaseEstimator\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_PATH: Path = Path(\"data\") / \"Churn_Modelling.csv\"\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Data file not found at {DATA_PATH.resolve()}. \"\n        \"Please download the Bank Customer Churn CSV and place it under the 'data/' directory.\"\n    )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Load and inspect the data\n\nWe load the bank churn dataset and take a first look.\n\nTypical columns include:\n\n- IDs: `RowNumber`, `CustomerId`, `Surname` (not predictive).\n- Features: `CreditScore`, `Geography`, `Gender`, `Age`, `Tenure`, `Balance`,\n  `NumOfProducts`, `HasCrCard`, `IsActiveMember`, `EstimatedSalary`.\n- Target: `Exited` (1 = churned, 0 = stayed).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_bank_churn_data(path: Path) -> pd.DataFrame:\n    \"\"\"Load the bank customer churn dataset from a CSV file.\n\n    Args:\n        path: Path to the CSV file.\n\n    Returns:\n        DataFrame containing the bank churn data.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the loaded DataFrame is empty.\n    \"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path!s}\")\n\n    df: pd.DataFrame = pd.read_csv(path)\n    if df.empty:\n        raise ValueError(f\"Loaded DataFrame is empty: {path!s}\")\n\n    return df\n\n\nraw_df: pd.DataFrame = load_bank_churn_data(DATA_PATH)\n\ndisplay(raw_df.head())\nprint(\"\\nDataframe shape:\", raw_df.shape)\nprint(\"Columns:\", list(raw_df.columns))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Basic cleaning\n\nWe:\n\n1. Drop identifier columns that do not carry predictive signal.\n2. Confirm that `Exited` is present and binary.\n3. Check missing values.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def clean_bank_churn_data(raw_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Clean the bank customer churn dataset.\n\n    - Drop identifier columns.\n    - Check for missing values.\n    - Ensure `Exited` exists.\n\n    Args:\n        raw_df: Raw bank churn DataFrame.\n\n    Returns:\n        Cleaned DataFrame.\n    \"\"\"\n    df = raw_df.copy()\n\n    id_cols: List[str] = [\"RowNumber\", \"CustomerId\", \"Surname\"]\n    drop_cols: List[str] = [c for c in id_cols if c in df.columns]\n    if drop_cols:\n        df = df.drop(columns=drop_cols)\n        print(f\"Dropped identifier columns: {drop_cols}\")\n\n    # Check missing values\n    missing = df.isna().sum()\n    print(\"Missing values per column (non-zero only):\")\n    display(missing[missing > 0])\n\n    if \"Exited\" not in df.columns:\n        raise ValueError(\"Target column 'Exited' not found in DataFrame.\")\n\n    # Ensure target is integer 0/1\n    df[\"Exited\"] = df[\"Exited\"].astype(int)\n\n    return df\n\n\ndf: pd.DataFrame = clean_bank_churn_data(raw_df)\n\ndisplay(df.head())\nprint(\"\\nClass distribution (Exited):\")\nprint(df[\"Exited\"].value_counts(normalize=True).rename(\"proportion\"))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe now have a clean dataset with predictive features and a binary\n`Exited` column. The churn rate is typically around 20% in this dataset.\n\nNext we perform a short EDA focused on **value-related** features.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Short EDA: churn and value proxies\n\nWe are particularly interested in:\n\n- `EstimatedSalary` \u2013 rough proxy for potential revenue.\n- `NumOfProducts` \u2013 engagement with the bank.\n- `Balance` \u2013 deposit volume.\n\nWe quickly explore these features by churn status.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "value_cols: List[str] = [\"EstimatedSalary\", \"NumOfProducts\", \"Balance\"]\n\nfor col in value_cols:\n    if col not in df.columns:\n        print(f\"Skipping {col!r} \u2013 not found in DataFrame.\")\n        continue\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\n    # Distribution by churn status\n    sns.kdeplot(data=df, x=col, hue=\"Exited\", common_norm=False, fill=True, alpha=0.4, ax=ax[0])\n    ax[0].set_title(f\"{col} distribution by Exited\")\n\n    # Boxplot\n    sns.boxplot(data=df, x=\"Exited\", y=col, ax=ax[1])\n    ax[1].set_title(f\"{col} by Exited (boxplot)\")\n\n    plt.tight_layout()\n    plt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "These plots suggest how churn relates to potential value. Our next step is to\nbuild a **churn model** that outputs `P(Exited = 1)` for each customer.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Train\u2013test split and preprocessing\n\nWe separate:\n\n- Features `X` (all columns except `Exited`).\n- Target `y` (`Exited`).\n\nThen we define a preprocessing pipeline:\n\n- Numeric features \u2192 `StandardScaler`.\n- Categorical features (`Geography`, `Gender`) \u2192 `OneHotEncoder`.\n\nWe use `ColumnTransformer` inside a `Pipeline` to keep things clean and\nreproducible.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "TARGET_COL: str = \"Exited\"\n\nif TARGET_COL not in df.columns:\n    raise KeyError(f\"Target column {TARGET_COL!r} not found in DataFrame.\")\n\nX: pd.DataFrame = df.drop(columns=[TARGET_COL])\ny: pd.Series = df[TARGET_COL]\n\ncategorical_cols: List[str] = [c for c in [\"Geography\", \"Gender\"] if c in X.columns]\nnumeric_cols: List[str] = [c for c in X.columns if c not in categorical_cols]\n\nprint(\"Categorical columns:\", categorical_cols)\nprint(\"Numeric columns:\", numeric_cols)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    stratify=y,\n    random_state=RANDOM_STATE,\n)\n\nprint(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n\nnumeric_transformer = Pipeline(\n    steps=[(\"scaler\", StandardScaler())]\n)\n\ncategorical_transformer = Pipeline(\n    steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_cols),\n        (\"cat\", categorical_transformer, categorical_cols),\n    ]\n)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Churn model: Random Forest\n\nWe train a **Random Forest classifier** as our churn model.\n\nIt is not the only choice (we could use LightGBM, XGBoost, etc.), but it is:\n\n- Strong for tabular data.\n- Robust and easy to use.\n\nWe define a small evaluation helper and inspect accuracy and ROC-AUC.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def evaluate_classifier(\n    name: str,\n    model: BaseEstimator,\n    X_train: pd.DataFrame,\n    X_test: pd.DataFrame,\n    y_train: pd.Series,\n    y_test: pd.Series,\n) -> Dict[str, float]:\n    \"\"\"Fit and evaluate a classifier.\n\n    Args:\n        name: Model name.\n        model: scikit-learn estimator or pipeline.\n        X_train: Training features.\n        X_test: Test features.\n        y_train: Training labels.\n        y_test: Test labels.\n\n    Returns:\n        Dictionary with key metrics on the test set.\n    \"\"\"\n    print(f\"\\n===== {name} =====\")\n\n    model.fit(X_train, y_train)\n\n    y_pred_test = model.predict(X_test)\n    y_proba_test = model.predict_proba(X_test)[:, 1]\n\n    acc = accuracy_score(y_test, y_pred_test)\n    roc_auc = roc_auc_score(y_test, y_proba_test)\n\n    print(f\"Test accuracy: {acc:.3f}\")\n    print(f\"Test ROC-AUC:  {roc_auc:.3f}\")\n\n    print(\"\\nClassification report (test):\")\n    print(classification_report(y_test, y_pred_test, target_names=[\"Stayed\", \"Exited\"]))\n\n    cm = confusion_matrix(y_test, y_pred_test)\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt=\"d\",\n        cmap=\"Blues\",\n        xticklabels=[\"Pred stayed\", \"Pred exited\"],\n        yticklabels=[\"True stayed\", \"True exited\"],\n    )\n    plt.title(f\"Confusion matrix - {name}\")\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.show()\n\n    RocCurveDisplay.from_predictions(y_test, y_proba_test)\n    plt.title(f\"ROC curve - {name}\")\n    plt.show()\n\n    return {\"model\": name, \"test_accuracy\": acc, \"test_roc_auc\": roc_auc}\n\n\nrf_clf = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\n            \"clf\",\n            RandomForestClassifier(\n                n_estimators=300,\n                max_depth=None,\n                min_samples_split=4,\n                min_samples_leaf=2,\n                random_state=RANDOM_STATE,\n                n_jobs=-1,\n            ),\n        ),\n    ]\n)\n\nmetrics_rf = evaluate_classifier(\"Random Forest\", rf_clf, X_train, X_test, y_train, y_test)\nmetrics_rf\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This Random Forest is our **churn probability engine**. Next, we will use it to\ncompute predicted churn probabilities for **all customers**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Predict churn probability for all customers\n\nWe refit the model on the **full dataset** (all rows) to obtain steady-state\nchurn probabilities for each customer.\n\nThese probabilities will feed our CLV approximation.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Fit on full data for scoring purposes\nrf_clf_full = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\n            \"clf\",\n            RandomForestClassifier(\n                n_estimators=300,\n                max_depth=None,\n                min_samples_split=4,\n                min_samples_leaf=2,\n                random_state=RANDOM_STATE,\n                n_jobs=-1,\n            ),\n        ),\n    ]\n)\n\nrf_clf_full.fit(X, y)\n\nchurn_proba_all = rf_clf_full.predict_proba(X)[:, 1]\n\nscored_df = df.copy()\nscored_df[\"churn_proba\"] = churn_proba_all\n\ndisplay(scored_df[[\"Exited\", \"churn_proba\"]].head())\n\nprint(\"Churn probability summary:\")\nprint(scored_df[\"churn_proba\"].describe())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The column `churn_proba` is our estimate of **short-term exit probability**\n(e.g., within the next period). We now combine this with a **value proxy**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Approximate customer value (simple CLV proxy)\n\nWe do **not** have actual revenue or margin per customer, so we create a\nreasonable proxy using:\n\n- `EstimatedSalary` \u2013 proxy for potential revenue.\n- `NumOfProducts` \u2013 number of products held.\n\nThis is intentionally simple. In a real bank you would use:\n\n- Net interest margin.\n- Fees.\n- Product-specific contribution.\n- Time-varying behaviour.\n\nHere we construct:\n\n```text\nannual_margin \u2248 base_margin\n                + w_products * NumOfProducts\n                + w_salary * (EstimatedSalary_normalised)\n```\n\nThen:\n\n```text\nCLV_proxy \u2248 annual_margin * expected_lifetime_years\n```\n\nWe approximate expected lifetime using a geometric survival idea:\n\n```text\nexpected_lifetime_years \u2248 1 / max(churn_proba, epsilon)\n```\n\nThis is a simplification but enough for segmentation.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def compute_annual_margin(row: pd.Series) -> float:\n    \"\"\"Compute a simple annual margin proxy for a customer.\n\n    Uses NumOfProducts and EstimatedSalary (normalised).\n\n    Args:\n        row: Row from the scored DataFrame.\n\n    Returns:\n        Approximate annual margin value.\n    \"\"\"\n    num_products: float = float(row.get(\"NumOfProducts\", 0.0))\n    est_salary: float = float(row.get(\"EstimatedSalary\", 0.0))\n\n    # Normalise salary roughly to [0, 1] scale using a simple heuristic\n    salary_norm: float = est_salary / 200_000.0  # assumes salaries around 0\u2013200k\n\n    base_margin: float = 100.0  # baseline yearly margin\n    w_products: float = 80.0    # extra margin per product\n    w_salary: float = 200.0     # extra margin scaled by salary_norm\n\n    annual_margin: float = base_margin + w_products * num_products + w_salary * salary_norm\n    return float(annual_margin)\n\n\nscored_df[\"annual_margin\"] = scored_df.apply(compute_annual_margin, axis=1)\n\nprint(\"Annual margin summary:\")\nprint(scored_df[\"annual_margin\"].describe())\n\nsns.histplot(scored_df[\"annual_margin\"], bins=30, kde=True)\nplt.title(\"Approximate annual margin distribution\")\nplt.xlabel(\"Annual margin (proxy)\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def compute_expected_lifetime_years(churn_p: float, max_years: float = 10.0) -> float:\n    \"\"\"Approximate expected remaining lifetime (years) from churn probability.\n\n    We assume a simple geometric-like model with constant churn probability\n    per year:\n\n    E[L] \u2248 1 / p, capped at `max_years`.\n\n    Args:\n        churn_p: Estimated churn probability (0\u20131).\n        max_years: Upper cap on expected lifetime.\n\n    Returns:\n        Expected lifetime in years.\n    \"\"\"\n    eps: float = 1e-4\n    p = max(float(churn_p), eps)\n    expected_lifetime: float = 1.0 / p\n    return float(min(expected_lifetime, max_years))\n\n\nscored_df[\"expected_lifetime_years\"] = scored_df[\"churn_proba\"].apply(compute_expected_lifetime_years)\n\nprint(\"Expected lifetime (years) summary:\")\nprint(scored_df[\"expected_lifetime_years\"].describe())\n\nsns.histplot(scored_df[\"expected_lifetime_years\"], bins=30, kde=True)\nplt.title(\"Approximate expected lifetime (years)\")\nplt.xlabel(\"Years\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# CLV proxy: annual margin * expected lifetime\nscored_df[\"clv_proxy\"] = scored_df[\"annual_margin\"] * scored_df[\"expected_lifetime_years\"]\n\nprint(\"CLV proxy summary:\")\nprint(scored_df[\"clv_proxy\"].describe())\n\nsns.histplot(scored_df[\"clv_proxy\"], bins=30, kde=True)\nplt.title(\"CLV proxy distribution\")\nplt.xlabel(\"CLV proxy (arbitrary units)\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe now have, for every customer:\n\n- `churn_proba` \u2013 model-based churn risk.\n- `annual_margin` \u2013 rough yearly value proxy.\n- `expected_lifetime_years` \u2013 rough time horizon.\n- `clv_proxy` \u2013 product of the two (value \u00d7 time).\n\nNext we build **priority segments** based on churn risk and CLV.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Risk\u2013value segmentation\n\nA common prioritisation scheme is to divide customers into **four quadrants**:\n\n- **High CLV, high churn risk** \u2192 top retention priority.\n- **High CLV, low churn risk** \u2192 protect and grow.\n- **Low CLV, high churn risk** \u2192 selective retention.\n- **Low CLV, low churn risk** \u2192 monitor, limited investment.\n\nWe implement this by splitting:\n\n- Churn probability at its median.\n- CLV proxy at its median.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Compute medians as cutpoints\nchurn_median: float = float(scored_df[\"churn_proba\"].median())\nclv_median: float = float(scored_df[\"clv_proxy\"].median())\n\nprint(f\"Median churn probability: {churn_median:.3f}\")\nprint(f\"Median CLV proxy:        {clv_median:.1f}\")\n\n\ndef assign_risk_value_segment(row: pd.Series) -> str:\n    \"\"\"Assign a segment label based on churn probability and CLV proxy.\n\n    Segments:\n    - 'high_value_high_risk'\n    - 'high_value_low_risk'\n    - 'low_value_high_risk'\n    - 'low_value_low_risk'\n\n    Args:\n        row: Row with 'churn_proba' and 'clv_proxy'.\n\n    Returns:\n        Segment label string.\n    \"\"\"\n    churn_p: float = float(row[\"churn_proba\"])\n    clv: float = float(row[\"clv_proxy\"])\n\n    value_label = \"high_value\" if clv >= clv_median else \"low_value\"\n    risk_label = \"high_risk\" if churn_p >= churn_median else \"low_risk\"\n\n    return f\"{value_label}_{risk_label}\"\n\n\nscored_df[\"segment\"] = scored_df.apply(assign_risk_value_segment, axis=1)\n\nsegment_counts = scored_df[\"segment\"].value_counts().rename(\"n_customers\")\nsegment_mean_churn = scored_df.groupby(\"segment\")[\"churn_proba\"].mean().rename(\"avg_churn_proba\")\nsegment_mean_clv = scored_df.groupby(\"segment\")[\"clv_proxy\"].mean().rename(\"avg_clv_proxy\")\n\nsegment_summary = pd.concat([segment_counts, segment_mean_churn, segment_mean_clv], axis=1)\nsegment_summary = segment_summary.sort_index()\n\ndisplay(segment_summary)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Visualise segments on risk vs value plane\nplt.figure(figsize=(7, 6))\n\nsns.scatterplot(\n    data=scored_df.sample(min(2000, len(scored_df)), random_state=RANDOM_STATE),\n    x=\"churn_proba\",\n    y=\"clv_proxy\",\n    hue=\"segment\",\n    alpha=0.6,\n)\n\nplt.axvline(churn_median, linestyle=\"--\", color=\"black\")\nplt.axhline(clv_median, linestyle=\"--\", color=\"black\")\nplt.xlabel(\"Churn probability\")\nplt.ylabel(\"CLV proxy\")\nplt.title(\"Risk\u2013value segmentation\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The scatter plot shows customers coloured by segment in the\n**churn probability vs CLV proxy** plane.\n\n- Top-right quadrant \u2192 **high_value_high_risk**.\n- Top-left quadrant \u2192 **high_value_low_risk**.\n- Bottom-right quadrant \u2192 **low_value_high_risk**.\n- Bottom-left quadrant \u2192 **low_value_low_risk**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Profiling key segments\n\nWe now inspect the **high_value_high_risk** segment more closely:\n\n- Size and share of total CLV proxy.\n- Average age, products, balance, activity.\n\nThis is typically the group you most want to retain.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Focus on high_value_high_risk segment\nhv_hr_mask = scored_df[\"segment\"] == \"high_value_high_risk\"\n\nhv_hr_df = scored_df.loc[hv_hr_mask].copy()\n\nprint(\"High-value, high-risk customers:\")\nprint(\"N:\", hv_hr_df.shape[0])\n\nshare_of_customers = hv_hr_df.shape[0] / scored_df.shape[0]\nshare_of_clv = hv_hr_df[\"clv_proxy\"].sum() / scored_df[\"clv_proxy\"].sum()\n\nprint(f\"Share of customers: {share_of_customers:.2%}\")\nprint(f\"Share of total CLV proxy: {share_of_clv:.2%}\")\n\nprofile_cols: List[str] = [\"Age\", \"NumOfProducts\", \"Balance\", \"EstimatedSalary\", \"IsActiveMember\"]\n\nsummary_profile = hv_hr_df[profile_cols].describe().T if all(c in hv_hr_df.columns for c in profile_cols) else hv_hr_df.describe().T\n\ndisplay(summary_profile)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Compare activity and geography across segments\nif \"IsActiveMember\" in scored_df.columns:\n    sns.barplot(\n        data=scored_df,\n        x=\"segment\",\n        y=\"IsActiveMember\",\n        estimator=np.mean,\n    )\n    plt.xticks(rotation=30)\n    plt.ylabel(\"Mean IsActiveMember\")\n    plt.title(\"Average activity by segment\")\n    plt.tight_layout()\n    plt.show()\n\nif \"Geography\" in scored_df.columns:\n    geo_segment = (\n        scored_df.groupby([\"segment\", \"Geography\"])[\"clv_proxy\"]\n        .mean()\n        .reset_index()\n    )\n    sns.catplot(\n        data=geo_segment,\n        x=\"segment\",\n        y=\"clv_proxy\",\n        hue=\"Geography\",\n        kind=\"bar\",\n        height=5,\n        aspect=1.6,\n    )\n    plt.xticks(rotation=30)\n    plt.ylabel(\"Average CLV proxy\")\n    plt.title(\"Average CLV proxy by segment and geography\")\n    plt.tight_layout()\n    plt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "These profiles help answer questions such as:\n\n- Which **geographies** concentrate high-value at-risk customers?\n- Are high-value, high-risk customers less active (`IsActiveMember`)?\n- How many products do they hold on average?\n\nFrom here, the business can design **targeted strategies**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11. Business interpretation and strategies\n\nGiven this segmentation, a bank might:\n\n### High-value, high-risk\n\n- **Goal:** prevent churn for these customers.\n- Possible actions:\n  - Personalised outreach (relationship managers, calls).\n  - Tailored offers: fee waivers, better terms, bundled products.\n  - Service recovery for customers with low activity or recent issues.\n\n### High-value, low-risk\n\n- **Goal:** maintain satisfaction and grow value.\n- Possible actions:\n  - Cross-sell and up-sell (new products that fit profile).\n  - Loyalty programmes and VIP treatment.\n\n### Low-value, high-risk\n\n- **Goal:** selective retention based on campaign cost.\n- Possible actions:\n  - Lower-cost digital campaigns.\n  - A/B tests to see if certain offers have acceptable ROI.\n\n### Low-value, low-risk\n\n- **Goal:** efficient maintenance.\n- Possible actions:\n  - Minimal proactive contact.\n  - Automated nudges and monitoring.\n\nThe exact actions depend on:\n\n- Operational constraints (call centre capacity, budget).\n- Regulatory context.\n- Product portfolio and pricing.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12. Limitations and extensions\n\nThis notebook deliberately uses **simple approximations**:\n\n- Churn probability is modelled as a one-period risk.\n- Expected lifetime uses a geometric approximation `1 / p`.\n- CLV proxy uses `NumOfProducts` and `EstimatedSalary` with arbitrary weights.\n\nIn a more advanced project you could:\n\n1. Use **survival analysis** or hazard models for richer lifetime estimates.\n2. Replace CLV proxy with a proper **LTV model** using:\n   - Product-level margins.\n   - Time-varying behaviour.\n   - Discounting of future cash flows.\n3. Optimise **thresholds and segment definitions** based on:\n   - Campaign costs.\n   - Expected uplift.\n   - Capacity constraints.\n4. Run **what-if simulations**:\n   - \"What if we reduce churn probability by X% for this segment?\"\n   - \"What is the incremental CLV gained?\"\n\nEven with simple assumptions, the combination of **churn risk** and **value**\nprovides a more realistic view of **which customers to focus on**.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}