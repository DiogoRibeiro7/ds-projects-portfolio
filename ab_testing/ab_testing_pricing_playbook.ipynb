{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27850d43",
   "metadata": {},
   "source": [
    "\n",
    "# Pricing A/B Testing Playbook — High vs Low Price Variant\n",
    "\n",
    "This notebook is a **complete, end-to-end analysis** of a pricing A/B test for an e-commerce or subscription product.\n",
    "\n",
    "- The company experimented with **two price points** (e.g. 9.99 vs 7.99).  \n",
    "- Each user was randomly assigned to either **control (old price)** or **treatment (new price)**.  \n",
    "- The main question: *Should we change the price, given the impact on conversion and revenue?*\n",
    "\n",
    "All Markdown is in **English**, and the code is written to be clear, typed, and re-usable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b18daa",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Data and experiment setup\n",
    "\n",
    "We assume a pricing A/B dataset called `pricing_ab.csv` with the following columns:\n",
    "\n",
    "- `user_id` — unique user identifier (string or integer).  \n",
    "- `group` — `\"control\"` or `\"treatment\"` (pricing buckets).  \n",
    "- `price` — price shown to the user (float).  \n",
    "- `purchased` — 1 if the user purchased, 0 otherwise (binary).  \n",
    "- `revenue` — total revenue from the user during the experiment window (float; 0 for non-buyers).  \n",
    "- *(Optional)* `segment` — user segment (e.g., `\"new\"`, `\"returning\"`, `\"high_value\"`).  \n",
    "- *(Optional)* `date` or `timestamp` — when the user saw the price (for day-level checks).\n",
    "\n",
    "> This structure is common across many public pricing A/B datasets.  \n",
    "> If your file uses different column names, you can adapt the small parts where they are referenced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593924a",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Setup\n",
    "\n",
    "We import the usual scientific Python stack, then define typed helper functions for proportions,\n",
    "t-tests, SRM checks, and power/MDE calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38141c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# Optional: used later for GLM (logit)\n",
    "try:\n",
    "    import statsmodels.api as sm  # type: ignore\n",
    "except Exception as e:  # pragma: no cover\n",
    "    sm = None\n",
    "    print(\"statsmodels not available; GLM cells will be skipped.\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8640458",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Proportion helpers (conversion) and SRM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e223f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class PropSummary:\n",
    "    \"\"\"Summary of a Bernoulli proportion.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    p : float\n",
    "        Sample proportion x / n.\n",
    "    n : int\n",
    "        Sample size.\n",
    "    x : int\n",
    "        Number of successes.\n",
    "    \"\"\"\n",
    "    p: float\n",
    "    n: int\n",
    "    x: int\n",
    "\n",
    "\n",
    "def summarize_prop(x: int, n: int) -> PropSummary:\n",
    "    \"\"\"Validate and summarize a proportion sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : int\n",
    "        Number of successes, in [0, n].\n",
    "    n : int\n",
    "        Sample size, must be positive.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PropSummary\n",
    "        Dataclass with p, n, x.\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive.\")\n",
    "    if not (0 <= x <= n):\n",
    "        raise ValueError(\"x must satisfy 0 <= x <= n.\")\n",
    "    return PropSummary(p=x / n, n=n, x=x)\n",
    "\n",
    "\n",
    "def invPhi(u: float) -> float:\n",
    "    \"\"\"Inverse standard normal CDF using erfcinv.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : float\n",
    "        Probability in (0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        z such that Phi(z) = u.\n",
    "    \"\"\"\n",
    "    if not 0.0 < u < 1.0:\n",
    "        raise ValueError(\"u must be in (0,1).\")\n",
    "    return math.sqrt(2.0) * math.erfcinv(2.0 * (1.0 - u))\n",
    "\n",
    "\n",
    "def two_prop_ztest(\n",
    "    x1: int,\n",
    "    n1: int,\n",
    "    x2: int,\n",
    "    n2: int,\n",
    "    two_sided: bool = True,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Two-sample z-test for proportions with pooled variance.\n",
    "\n",
    "    Tests H0: p1 = p2 vs H1: p1 != p2 (two-sided by default).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x1, n1, x2, n2 : int\n",
    "        Success counts and sample sizes for groups 1 and 2.\n",
    "    two_sided : bool\n",
    "        If True, compute a two-sided p-value. If False, right-sided (p2 > p1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    z : float\n",
    "        z-statistic (signed).\n",
    "    p_value : float\n",
    "        Corresponding p-value.\n",
    "    \"\"\"\n",
    "    s1, s2 = summarize_prop(x1, n1), summarize_prop(x2, n2)\n",
    "    p_pool = (s1.x + s2.x) / (s1.n + s2.n)\n",
    "    se = math.sqrt(p_pool * (1.0 - p_pool) * (1.0 / s1.n + 1.0 / s2.n))\n",
    "    if se == 0.0:\n",
    "        raise ZeroDivisionError(\"Standard error is zero; check inputs.\")\n",
    "    z = (s2.p - s1.p) / se\n",
    "    # standard normal tail via erf\n",
    "    if two_sided:\n",
    "        p = 2.0 * (1.0 - 0.5 * (1.0 + math.erf(abs(z) / math.sqrt(2.0))))\n",
    "    else:\n",
    "        p = 1.0 - 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
    "    return float(z), float(p)\n",
    "\n",
    "\n",
    "def chisq_srm(nA: int, nB: int) -> float:\n",
    "    \"\"\"Chi-square SRM (sample ratio mismatch) test for 50/50 split.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nA, nB : int\n",
    "        Sample sizes for arms A and B.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Approximate two-sided p-value for chi-square(1).\n",
    "    \"\"\"\n",
    "    n = nA + nB\n",
    "    exp_A = exp_B = n / 2.0\n",
    "    chi2 = (nA - exp_A) ** 2 / exp_A + (nB - exp_B) ** 2 / exp_B\n",
    "    z = math.sqrt(chi2)\n",
    "    # Tail of chi-square(1) via normal on sqrt(chi2)\n",
    "    p = 2.0 * (1.0 - 0.5 * (1.0 + math.erf(z / math.sqrt(2.0))))\n",
    "    return float(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a8d0b",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 T-test helpers (revenue) and power/MDE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bebda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def welch_ttest(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Welch t-test for difference in means (two-sided).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : np.ndarray\n",
    "        Samples from two groups (e.g., revenue per user in control vs treatment).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    t_stat : float\n",
    "        Welch t-statistic.\n",
    "    p_value : float\n",
    "        Two-sided p-value under approximate normality.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n1, n2 = x.size, y.size\n",
    "    if n1 < 2 or n2 < 2:\n",
    "        raise ValueError(\"Need at least 2 observations per group.\")\n",
    "    m1, m2 = float(x.mean()), float(y.mean())\n",
    "    v1, v2 = float(x.var(ddof=1)), float(y.var(ddof=1))\n",
    "    se = math.sqrt(v1 / n1 + v2 / n2)\n",
    "    if se == 0.0:\n",
    "        raise ZeroDivisionError(\"Standard error is zero; check variance.\")\n",
    "    t_stat = (m2 - m1) / se\n",
    "    # Use normal approximation for p-value (large n)\n",
    "    p = 2.0 * (1.0 - 0.5 * (1.0 + math.erf(abs(t_stat) / math.sqrt(2.0))))\n",
    "    return float(t_stat), float(p)\n",
    "\n",
    "\n",
    "def mde_for_n(\n",
    "    p_baseline: float,\n",
    "    n_per_arm: int,\n",
    "    alpha: float = 0.05,\n",
    "    power: float = 0.8,\n",
    "    two_sided: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"Compute absolute MDE on a proportion metric for given n per arm.\n",
    "\n",
    "    Uses normal approximation for a two-sample proportion test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_baseline : float\n",
    "        Baseline conversion rate (in (0, 1)).\n",
    "    n_per_arm : int\n",
    "        Sample size per arm.\n",
    "    alpha : float\n",
    "        Significance level.\n",
    "    power : float\n",
    "        Desired power (1 - beta).\n",
    "    two_sided : bool\n",
    "        If True, uses two-sided z_{alpha/2}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Approximate minimal detectable effect (absolute difference in p).\n",
    "    \"\"\"\n",
    "    if not 0.0 < p_baseline < 1.0:\n",
    "        raise ValueError(\"p_baseline must be in (0,1).\")\n",
    "    if n_per_arm <= 0:\n",
    "        raise ValueError(\"n_per_arm must be positive.\")\n",
    "\n",
    "    z_alpha = abs(invPhi(1.0 - alpha / 2.0)) if two_sided else abs(invPhi(1.0 - alpha))\n",
    "    z_beta = abs(invPhi(power))\n",
    "    se = math.sqrt(2.0 * p_baseline * (1.0 - p_baseline))\n",
    "    return float((z_alpha + z_beta) * se / math.sqrt(n_per_arm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeacfa4a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load and inspect the pricing A/B dataset\n",
    "\n",
    "We expect `pricing_ab.csv` in the same directory as this notebook. If the file name or path differ,\n",
    "you can adjust `DATA_PATH` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce024156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"pricing_ab.csv\")\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"pricing_ab.csv not found in the current directory.\\n\"\n",
    "        \"Place your pricing A/B dataset here or update DATA_PATH.\"\n",
    "    )\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f24956",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Basic cleaning and SRM check\n",
    "\n",
    "We:\n",
    "\n",
    "1. Keep only `group` in {`control`, `treatment`}.  \n",
    "2. Drop duplicate `user_id` rows.  \n",
    "3. Confirm that prices align with expectations (e.g., one main price per group or a consistent pattern).  \n",
    "4. Run an SRM check on the group sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d656513",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Filter to the two main groups\n",
    "df = df[df[\"group\"].isin([\"control\", \"treatment\"])].copy()\n",
    "\n",
    "# Drop duplicate users, if any\n",
    "df = df.drop_duplicates(subset=[\"user_id\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# Basic sanity checks\n",
    "print(\"Groups:\", df[\"group\"].value_counts())\n",
    "print(\"Price per group (describe):\")\n",
    "print(df.groupby(\"group\")[\"price\"].describe().round(4))\n",
    "\n",
    "n_control = (df[\"group\"] == \"control\").sum()\n",
    "n_treat = (df[\"group\"] == \"treatment\").sum()\n",
    "p_srm = chisq_srm(n_control, n_treat)\n",
    "\n",
    "n_control, n_treat, p_srm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1045fd",
   "metadata": {},
   "source": [
    "\n",
    "**Reading SRM**\n",
    "\n",
    "- Very small SRM p-values (e.g., < 0.01) can indicate randomization or logging issues.  \n",
    "- A healthy A/B test for pricing often targets either a 50/50 split or another pre-defined ratio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c640ee3",
   "metadata": {},
   "source": [
    "\n",
    "## 3) EDA: price and metrics overview\n",
    "\n",
    "We explore:\n",
    "\n",
    "- Price distributions by group.  \n",
    "- Basic conversion and revenue metrics.  \n",
    "- Optional segment breakdown if a `segment` column exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Price distribution by group\n",
    "plt.figure()\n",
    "for g in [\"control\", \"treatment\"]:\n",
    "    vals = df.loc[df[\"group\"] == g, \"price\"].values\n",
    "    plt.hist(vals, bins=40, alpha=0.5, label=g)\n",
    "plt.title(\"Price distribution by group\")\n",
    "plt.xlabel(\"price\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic metrics by group\n",
    "metrics = (\n",
    "    df.assign(\n",
    "        revenue_per_user=df[\"revenue\"].astype(float),\n",
    "        conversion=df[\"purchased\"].astype(int),\n",
    "    )\n",
    "    .groupby(\"group\")[[\"conversion\", \"revenue_per_user\"]]\n",
    "    .agg([\"mean\", \"median\", \"std\", \"count\"])\n",
    ")\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233ace74",
   "metadata": {},
   "source": [
    "\n",
    "If a `segment` column exists, we can quickly inspect conversion by segment and group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"segment\" in df.columns:\n",
    "    seg_summary = (\n",
    "        df.groupby([\"segment\", \"group\"])[\"purchased\"]\n",
    "        .agg([\"mean\", \"count\"])\n",
    "        .rename(columns={\"mean\": \"conversion_rate\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    seg_summary.head(20)\n",
    "else:\n",
    "    print(\"No 'segment' column found; skipping segment breakdown.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ea823",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Primary inference: conversion rate\n",
    "\n",
    "We treat `purchased` as a Bernoulli metric and compare **conversion rates**:\n",
    "\n",
    "\\[\n",
    "\\text{CR}_g = \\mathbb{P}(\\text{purchased}=1 \\mid group=g).\n",
    "\\]\n",
    "\n",
    "We run a two-proportion z-test and compute a 95% CI for the difference in conversion rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conv_by_group = (\n",
    "    df.groupby(\"group\")[\"purchased\"]\n",
    "    .agg([\"sum\", \"count\", \"mean\"])\n",
    "    .rename(columns={\"sum\": \"x\", \"count\": \"n\", \"mean\": \"rate\"})\n",
    ")\n",
    "conv_by_group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12122809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xA = int(conv_by_group.loc[\"control\", \"x\"])\n",
    "nA = int(conv_by_group.loc[\"control\", \"n\"])\n",
    "xB = int(conv_by_group.loc[\"treatment\", \"x\"])\n",
    "nB = int(conv_by_group.loc[\"treatment\", \"n\"])\n",
    "\n",
    "sA = summarize_prop(xA, nA)\n",
    "sB = summarize_prop(xB, nB)\n",
    "\n",
    "z_conv, p_conv = two_prop_ztest(xA, nA, xB, nB, two_sided=True)\n",
    "\n",
    "# Normal-approximation CI for the difference (B - A)\n",
    "diff_conv = sB.p - sA.p\n",
    "alpha = 0.05\n",
    "z_alpha = abs(invPhi(1.0 - alpha / 2.0))\n",
    "se_diff_conv = math.sqrt(\n",
    "    (sA.p * (1.0 - sA.p)) / sA.n + (sB.p * (1.0 - sB.p)) / sB.n\n",
    ")\n",
    "ci_lo_conv = diff_conv - z_alpha * se_diff_conv\n",
    "ci_hi_conv = diff_conv + z_alpha * se_diff_conv\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"arm\": [\"control\", \"treatment\"],\n",
    "        \"n\": [sA.n, sB.n],\n",
    "        \"x\": [sA.x, sB.x],\n",
    "        \"rate\": [sA.p, sB.p],\n",
    "        \"diff_B_minus_A\": [diff_conv, diff_conv],\n",
    "        \"diff_CI95_lo\": [ci_lo_conv, ci_lo_conv],\n",
    "        \"diff_CI95_hi\": [ci_hi_conv, ci_hi_conv],\n",
    "        \"z_stat\": [z_conv, z_conv],\n",
    "        \"p_value\": [p_conv, p_conv],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d13448",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation (conversion)**\n",
    "\n",
    "- `diff_B_minus_A` is the absolute lift in conversion when showing the **treatment price**.  \n",
    "- The 95% CI shows which lifts are compatible with the data.  \n",
    "- The p-value tests the null hypothesis of **no difference** in conversion rate.\n",
    "\n",
    "Next we look at **revenue per user**, which is the more direct business objective in pricing tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5887b7b",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Revenue per user: Welch t-test\n",
    "\n",
    "Our key business metric is often **revenue per eligible user** (RPU), defined as:\n",
    "\n",
    "\\[\n",
    "\\text{RPU}_g = \\mathbb{E}[\\text{revenue} \\mid group = g].\n",
    "\\]\n",
    "\n",
    "We use a **Welch t-test** on revenue per user to compare groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37848910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"revenue\"] = df[\"revenue\"].astype(float)\n",
    "revenue_control = df.loc[df[\"group\"] == \"control\", \"revenue\"].to_numpy()\n",
    "revenue_treat = df.loc[df[\"group\"] == \"treatment\", \"revenue\"].to_numpy()\n",
    "\n",
    "t_rev, p_rev = welch_ttest(revenue_control, revenue_treat)\n",
    "\n",
    "mean_rev_A = float(revenue_control.mean())\n",
    "mean_rev_B = float(revenue_treat.mean())\n",
    "\n",
    "# Approximate CI using normal approximation around mean difference\n",
    "diff_rev = mean_rev_B - mean_rev_A\n",
    "varA = float(revenue_control.var(ddof=1))\n",
    "varB = float(revenue_treat.var(ddof=1))\n",
    "se_diff_rev = math.sqrt(varA / revenue_control.size + varB / revenue_treat.size)\n",
    "ci_lo_rev = diff_rev - z_alpha * se_diff_rev\n",
    "ci_hi_rev = diff_rev + z_alpha * se_diff_rev\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"arm\": [\"control\", \"treatment\"],\n",
    "        \"mean_revenue\": [mean_rev_A, mean_rev_B],\n",
    "        \"diff_B_minus_A\": [diff_rev, diff_rev],\n",
    "        \"diff_CI95_lo\": [ci_lo_rev, ci_lo_rev],\n",
    "        \"diff_CI95_hi\": [ci_hi_rev, ci_hi_rev],\n",
    "        \"t_stat\": [t_rev, t_rev],\n",
    "        \"p_value\": [p_rev, p_rev],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38501e71",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation (revenue)**\n",
    "\n",
    "- If conversion goes down but revenue per user goes up, a **higher price** can still be beneficial.  \n",
    "- The decision should focus on RPU (and downstream margin), not only on conversion.\n",
    "\n",
    "Next, we connect price differences with demand differences via a simple **elasticity** calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8a51e",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Price elasticity (coarse estimate)\n",
    "\n",
    "A very rough **price elasticity of demand** can be approximated using the two price points and conversion rates:\n",
    "\n",
    "\\[\n",
    "\\epsilon \\approx \\frac{\\Delta Q / Q}{\\Delta P / P},\n",
    "\\]\n",
    "\n",
    "where:\n",
    "\n",
    "- \\(Q\\) is demand proxy (e.g., conversion rate),  \n",
    "- \\(P\\) is price.\n",
    "\n",
    "We use control as baseline and treatment as the alternative price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "price_control = float(df.loc[df[\"group\"] == \"control\", \"price\"].mean())\n",
    "price_treat = float(df.loc[df[\"group\"] == \"treatment\", \"price\"].mean())\n",
    "\n",
    "Q_control = float(conv_by_group.loc[\"control\", \"rate\"])\n",
    "Q_treat = float(conv_by_group.loc[\"treatment\", \"rate\"])\n",
    "\n",
    "dP_over_P = (price_treat - price_control) / price_control if price_control != 0 else float(\"nan\")\n",
    "dQ_over_Q = (Q_treat - Q_control) / Q_control if Q_control != 0 else float(\"nan\")\n",
    "\n",
    "elasticity = dQ_over_Q / dP_over_P if dP_over_P not in (0.0, float(\"nan\")) else float(\"nan\")\n",
    "\n",
    "{\n",
    "    \"price_control\": price_control,\n",
    "    \"price_treat\": price_treat,\n",
    "    \"conv_control\": Q_control,\n",
    "    \"conv_treat\": Q_treat,\n",
    "    \"dP_over_P\": dP_over_P,\n",
    "    \"dQ_over_Q\": dQ_over_Q,\n",
    "    \"elasticity\": elasticity,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dbacb0",
   "metadata": {},
   "source": [
    "\n",
    "**Reading elasticity**\n",
    "\n",
    "- A negative elasticity (common) means that increasing price reduces demand.  \n",
    "- Values with larger absolute magnitude indicate **more sensitive demand**.  \n",
    "- This is a **very coarse** estimate; a full pricing project often uses more price points and regression models.\n",
    "\n",
    "Next we check whether the experiment was capable of detecting a meaningful conversion change (MDE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80baa98c",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Power and MDE for conversion\n",
    "\n",
    "Using the baseline conversion rate and the smallest group size, we compute the **MDE** at 80% power and 5% alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d30620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_baseline = sA.p  # control conversion\n",
    "n_per_arm = min(sA.n, sB.n)\n",
    "mde_80 = mde_for_n(p_baseline, n_per_arm, alpha=0.05, power=0.8, two_sided=True)\n",
    "\n",
    "{\n",
    "    \"baseline_conversion_control\": p_baseline,\n",
    "    \"n_per_arm\": n_per_arm,\n",
    "    \"MDE_abs_at_80pct_power\": mde_80,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04910219",
   "metadata": {},
   "source": [
    "\n",
    "If your observed conversion lift is much smaller (in absolute value) than this MDE, the test might be\n",
    "**underpowered** to detect such subtle effects. In pricing, this is common: small price changes can have\n",
    "small but important effects that require large sample sizes to pick up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171c61f",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Executive summary template\n",
    "\n",
    "When you run this notebook on your real `pricing_ab.csv`, you can use the following structure for a decision memo:\n",
    "\n",
    "1. **Sanity checks**\n",
    "   - SRM p-value and any obvious data-quality issues.\n",
    "\n",
    "2. **Conversion and revenue results**\n",
    "   - Conversion rate per group and absolute lift (with 95% CI and p-value).  \n",
    "   - Revenue per user per group and absolute lift (with 95% CI and p-value).  \n",
    "   - Simple elasticity estimate and whether it is plausible from a business standpoint.\n",
    "\n",
    "3. **Risk and upside**\n",
    "   - If price increases: trade-off between lower conversion and higher RPU.  \n",
    "   - If price decreases: higher conversion vs lower per-order revenue, and overall RPU impact.\n",
    "\n",
    "4. **Power / MDE context**\n",
    "   - Whether the test could realistically detect the effect sizes you care about.\n",
    "\n",
    "5. **Decision and rollout**\n",
    "   - Ship / hold / roll back and why.  \n",
    "   - If ship: ramp strategy (e.g. 20% → 50% → 100%) and monitoring.  \n",
    "   - If hold: whether you need more data, more segments, or a different price grid.\n",
    "\n",
    "This keeps the pricing experiment focused on **business outcomes** rather than only statistical significance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15802916",
   "metadata": {},
   "source": [
    "\n",
    "## 8a) CUPED with a pre-period metric (if available)\n",
    "\n",
    "In pricing experiments it is common to have a **pre-period** metric per user, for example:\n",
    "\n",
    "- `pre_revenue`: revenue for this user in the 30 days before the test, or  \n",
    "- `past_purchases`: number of past purchases before the test.\n",
    "\n",
    "If this covariate is predictive of **revenue during the experiment**, we can use **CUPED** to reduce\n",
    "variance on our revenue per user metric.\n",
    "\n",
    "The CUPED transform is:\n",
    "\n",
    "\\[\n",
    "Y_i^{*} = Y_i - \\theta (X_i - \\bar X), \\quad \n",
    "\\theta = \\frac{\\mathrm{Cov}(Y, X)}{\\mathrm{Var}(X)},\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(Y_i\\) is the outcome (here: revenue per user during the test),  \n",
    "- \\(X_i\\) is a pre-period covariate,  \n",
    "- \\(\\bar X\\) is the sample mean of \\(X\\).\n",
    "\n",
    "We then compare groups on \\(Y^{*}\\) instead of \\(Y\\). When \\(X\\) is correlated with \\(Y\\), the variance\n",
    "of group means (and their difference) shrinks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def cuped_adjust(y: np.ndarray, x: np.ndarray) -> tuple[np.ndarray, float]:\n",
    "    \"\"\"Apply CUPED adjustment Y* = Y - θ (X - mean(X)).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray\n",
    "        Outcome vector (shape (n,)).\n",
    "    x : np.ndarray\n",
    "        Covariate vector (shape (n,)), ideally pre-experiment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_adj : np.ndarray\n",
    "        CUPED-adjusted outcome.\n",
    "    theta : float\n",
    "        Estimated CUPED coefficient Cov(Y,X)/Var(X).\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if y.shape != x.shape:\n",
    "        raise ValueError(\"y and x must have the same shape.\")\n",
    "    if y.ndim != 1:\n",
    "        raise ValueError(\"y and x must be 1D arrays.\")\n",
    "    vx = float(np.var(x, ddof=1))\n",
    "    if vx == 0.0:\n",
    "        # No variation in X: no adjustment\n",
    "        return y.copy(), 0.0\n",
    "    cov_yx = float(np.cov(y, x, ddof=1)[0, 1])\n",
    "    theta = cov_yx / vx\n",
    "    x_centered = x - float(np.mean(x))\n",
    "    y_adj = y - theta * x_centered\n",
    "    return y_adj, theta\n",
    "\n",
    "\n",
    "# Try to find a reasonable pre-period covariate\n",
    "pre_candidates: list[str] = [\n",
    "    c for c in [\"pre_revenue\", \"past_purchases\"] if c in df.columns\n",
    "]\n",
    "pre_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not pre_candidates:\n",
    "    print(\n",
    "        \"No pre-period covariate found (expected 'pre_revenue' or 'past_purchases').\\n\"\n",
    "        \"To use CUPED here, add one of these columns to pricing_ab.csv and rerun.\"\n",
    "    )\n",
    "else:\n",
    "    pre_col = pre_candidates[0]\n",
    "    print(f\"Using pre-period covariate: {pre_col!r}\")\n",
    "\n",
    "    y = df[\"revenue\"].to_numpy(dtype=float)\n",
    "    x = df[pre_col].to_numpy(dtype=float)\n",
    "\n",
    "    y_adj, theta_hat = cuped_adjust(y, x)\n",
    "    df[\"revenue_cuped\"] = y_adj\n",
    "\n",
    "    theta_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f1f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if \"revenue_cuped\" in df.columns:\n",
    "    # Compare group-level stats before and after CUPED\n",
    "    cuped_summary = (\n",
    "        df.assign(revenue_raw=df[\"revenue\"].astype(float))\n",
    "          .groupby(\"group\")[[\"revenue_raw\", \"revenue_cuped\"]]\n",
    "          .agg([\"mean\", \"var\", \"count\"])\n",
    "    )\n",
    "    display(cuped_summary)\n",
    "\n",
    "    # A/B comparison on CUPED-adjusted revenue (difference in means, Welch t-test)\n",
    "    rev_A_c = df.loc[df[\"group\"] == \"control\", \"revenue_cuped\"].to_numpy(dtype=float)\n",
    "    rev_B_c = df.loc[df[\"group\"] == \"treatment\", \"revenue_cuped\"].to_numpy(dtype=float)\n",
    "\n",
    "    t_cuped, p_cuped = welch_ttest(rev_A_c, rev_B_c)\n",
    "\n",
    "    mean_A_c = float(rev_A_c.mean())\n",
    "    mean_B_c = float(rev_B_c.mean())\n",
    "    diff_c = mean_B_c - mean_A_c\n",
    "\n",
    "    varA_c = float(rev_A_c.var(ddof=1))\n",
    "    varB_c = float(rev_B_c.var(ddof=1))\n",
    "    se_diff_c = math.sqrt(varA_c / rev_A_c.size + varB_c / rev_B_c.size)\n",
    "    z_alpha_local = abs(invPhi(1.0 - 0.05 / 2.0))\n",
    "    ci_lo_c = diff_c - z_alpha_local * se_diff_c\n",
    "    ci_hi_c = diff_c + z_alpha_local * se_diff_c\n",
    "\n",
    "    {\n",
    "        \"theta_hat\": theta_hat,\n",
    "        \"diff_raw_revenue\": float(\n",
    "            df.loc[df[\"group\"] == \"treatment\", \"revenue\"].mean()\n",
    "            - df.loc[df[\"group\"] == \"control\", \"revenue\"].mean()\n",
    "        ),\n",
    "        \"diff_cuped_revenue\": diff_c,\n",
    "        \"cuped_CI95\": (ci_lo_c, ci_hi_c),\n",
    "        \"t_cuped\": t_cuped,\n",
    "        \"p_cuped\": p_cuped,\n",
    "    }\n",
    "else:\n",
    "    print(\"No 'revenue_cuped' column — CUPED not applied.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce93a91",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation (CUPED)**\n",
    "\n",
    "- When a true pre-period metric is available and correlated with experiment revenue, CUPED can\n",
    "  reduce the variance of revenue per user, often cutting required sample size by 10–30% or more.  \n",
    "- In this notebook we only apply CUPED if a column like `pre_revenue` or `past_purchases` exists.\n",
    "  You can plug in any numeric pre-period feature you trust.  \n",
    "- The logic is identical for other metrics (e.g., conversion) — simply change `y` accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e3acfb",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Bayesian pricing view — posterior over RPU difference\n",
    "\n",
    "For pricing, the key quantity is often **revenue per user (RPU)** in each arm:\n",
    "\n",
    "\\[\n",
    "\\mu_A = \\mathbb{E}[\\text{revenue} \\mid \\text{control}], \\quad\n",
    "\\mu_B = \\mathbb{E}[\\text{revenue} \\mid \\text{treatment}].\n",
    "\\]\n",
    "\n",
    "We want a distribution for the **difference**:\n",
    "\n",
    "\\[\n",
    "\\Delta_\\mu = \\mu_B - \\mu_A.\n",
    "\\]\n",
    "\n",
    "Assuming revenue per user is approximately **normal** in each arm, we can use a simple Bayesian\n",
    "approximation:\n",
    "\n",
    "- Prior: vague / flat for each mean \\(\\mu_g\\).  \n",
    "- Likelihood: Normal with unknown variance, approximated using the sample variance.  \n",
    "- Posterior (approximate):\n",
    "\n",
    "\\[\n",
    "\\mu_g \\mid \\text{data} \\approx \\mathcal{N}\\left(\\bar y_g,\\ \\frac{s_g^2}{n_g}\\right),\n",
    "\\]\n",
    "\n",
    "where \\(\\bar y_g\\) and \\(s_g^2\\) are the sample mean and variance for arm \\(g\\).\n",
    "\n",
    "We then draw Monte Carlo samples of \\(\\mu_A, \\mu_B\\) and derive a posterior distribution for \\(\\Delta_\\mu\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ff88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_posterior_rpu(\n",
    "    revenue_A: np.ndarray,\n",
    "    revenue_B: np.ndarray,\n",
    "    n_draws: int = 50_000,\n",
    "    seed: int | None = 123,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Approximate posterior for mean revenue per user in each arm.\n",
    "\n",
    "    Assumes a normal model with unknown variance approximated by the sample variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    revenue_A, revenue_B : np.ndarray\n",
    "        Revenue per user in control and treatment arms.\n",
    "    n_draws : int\n",
    "        Number of Monte Carlo draws.\n",
    "    seed : int | None\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Columns: mu_A, mu_B, lift (mu_B - mu_A).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    revenue_A = np.asarray(revenue_A, dtype=float)\n",
    "    revenue_B = np.asarray(revenue_B, dtype=float)\n",
    "\n",
    "    nA = revenue_A.size\n",
    "    nB = revenue_B.size\n",
    "    if nA < 2 or nB < 2:\n",
    "        raise ValueError(\"Need at least 2 observations per group.\")\n",
    "\n",
    "    mean_A = float(revenue_A.mean())\n",
    "    mean_B = float(revenue_B.mean())\n",
    "    var_A = float(revenue_A.var(ddof=1))\n",
    "    var_B = float(revenue_B.var(ddof=1))\n",
    "\n",
    "    # Posterior approx: Normal(mean_g, var_g / n_g)\n",
    "    mu_A_draws = rng.normal(loc=mean_A, scale=math.sqrt(var_A / nA), size=n_draws)\n",
    "    mu_B_draws = rng.normal(loc=mean_B, scale=math.sqrt(var_B / nB), size=n_draws)\n",
    "    lift = mu_B_draws - mu_A_draws\n",
    "\n",
    "    return pd.DataFrame({\"mu_A\": mu_A_draws, \"mu_B\": mu_B_draws, \"lift\": lift})\n",
    "\n",
    "\n",
    "post_rpu = sample_posterior_rpu(revenue_control, revenue_treat, n_draws=60_000, seed=2025)\n",
    "post_rpu.describe(percentiles=[0.025, 0.5, 0.975])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Posterior probability that treatment RPU is higher\n",
    "prob_rpu_better: float = float((post_rpu[\"lift\"] > 0).mean())\n",
    "ci_lo_rpu, ci_hi_rpu = np.quantile(post_rpu[\"lift\"], [0.025, 0.975])\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(post_rpu[\"lift\"], bins=60, density=True)\n",
    "plt.axvline(0.0, linestyle=\"--\")\n",
    "plt.title(\"Posterior distribution of RPU lift (μ_B - μ_A)\")\n",
    "plt.xlabel(\"RPU lift\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "{\n",
    "    \"posterior_prob_RPU_treatment_higher\": prob_rpu_better,\n",
    "    \"RPU_lift_cred_int_95\": (ci_lo_rpu, ci_hi_rpu),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d1d91",
   "metadata": {},
   "source": [
    "\n",
    "**Reading the Bayesian RPU view**\n",
    "\n",
    "- `posterior_prob_RPU_treatment_higher` is the probability (under this model) that **treatment pricing**\n",
    "  yields higher revenue per user than control.  \n",
    "- The credible interval for the RPU lift tells you which **revenue deltas** are plausible\n",
    "  given your data and assumptions.  \n",
    "- For a more detailed demand view, you can combine this with a **Beta–Binomial** posterior on conversion,\n",
    "  as in the e-commerce notebook.\n",
    "\n",
    "This Bayesian view is useful when stakeholders want answers like\n",
    "“**How likely is it that the new price is better, in revenue terms?**”\n",
    "instead of only a p-value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621bfe4",
   "metadata": {},
   "source": [
    "\n",
    "## 10) DR / causal block — non-random discount assignment (simulation)\n",
    "\n",
    "In many real systems, **pricing** or **discounts** are **not assigned randomly**:\n",
    "\n",
    "- A model decides who sees a discount based on features (segment, RFM, device, etc.).  \n",
    "- High-value users may be shown higher prices or fewer coupons.  \n",
    "- New users may automatically see a discount banner.\n",
    "\n",
    "In such cases, **naive comparisons** of discounted vs non-discounted users are biased:\n",
    "there is **confounding** by user features.\n",
    "\n",
    "Here we simulate a simple scenario where:\n",
    "\n",
    "- User features \\(X\\) influence both **discount assignment** \\(D\\) and **purchase outcome** \\(Y\\).  \n",
    "- We then compare:\n",
    "  - **Naive difference in means**,\n",
    "  - **IPW** using the true propensities,\n",
    "  - **DR (doubly-robust)** estimator with logistic outcome models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09788905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def sigmoid(z: np.ndarray | float) -> np.ndarray | float:\n",
    "    \"\"\"Numerically stable logistic function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def simulate_discount_confounded(\n",
    "    n: int = 20_000,\n",
    "    seed: int | None = 123,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Simulate discount assignment and purchase outcome with confounding via X.\n",
    "\n",
    "    Data generating process\n",
    "    -----------------------\n",
    "    - Features X ~ N(0, I_d) with d = 4.\n",
    "    - Discount propensity e(x) = sigmoid(bias_e + w_e^T x).\n",
    "    - Discount D ~ Bernoulli(e(x)).\n",
    "    - Outcome logit: logit P(Y=1 | X, D) = bias_y + w_y^T x + tau_true * D.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of samples.\n",
    "    seed : int | None\n",
    "        Random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Columns: x1..x4, D, Y, e_true (true propensity of discount).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    d = 4\n",
    "    X = rng.normal(size=(n, d))\n",
    "\n",
    "    # Propensity model (drives confounding)\n",
    "    w_e = np.array([0.8, -0.4, 0.6, 0.3])\n",
    "    bias_e = -0.2\n",
    "    e = sigmoid(bias_e + X @ w_e)\n",
    "\n",
    "    D = rng.binomial(1, e)\n",
    "\n",
    "    # Outcome model\n",
    "    w_y = np.array([0.4, 0.2, -0.3, 0.1])\n",
    "    bias_y = -1.0\n",
    "    tau_true = 0.35  # log-odds lift from being discounted\n",
    "\n",
    "    lin = bias_y + X @ w_y + tau_true * D\n",
    "    p = sigmoid(lin)\n",
    "    Y = rng.binomial(1, p)\n",
    "\n",
    "    cols = {f\"x{j+1}\": X[:, j] for j in range(d)}\n",
    "    df_sim = pd.DataFrame(cols)\n",
    "    df_sim[\"D\"] = D\n",
    "    df_sim[\"Y\"] = Y\n",
    "    df_sim[\"e_true\"] = e\n",
    "    return df_sim\n",
    "\n",
    "\n",
    "def naive_ate_discount(df_sim: pd.DataFrame) -> float:\n",
    "    \"\"\"Naive ATE: difference in purchase rates E[Y|D=1] - E[Y|D=0], ignoring confounding.\"\"\"\n",
    "    m1 = float(df_sim.loc[df_sim[\"D\"] == 1, \"Y\"].mean())\n",
    "    m0 = float(df_sim.loc[df_sim[\"D\"] == 0, \"Y\"].mean())\n",
    "    return m1 - m0\n",
    "\n",
    "\n",
    "def ipw_ate_discount(df_sim: pd.DataFrame, e_col: str = \"e_true\") -> float:\n",
    "    \"\"\"IPW ATE estimator using a known or estimated propensity column.\"\"\"\n",
    "    if not {\"D\", \"Y\", e_col}.issubset(df_sim.columns):\n",
    "        raise ValueError(f\"df_sim must contain D, Y, and {e_col}.\")\n",
    "\n",
    "    d = df_sim[\"D\"].to_numpy()\n",
    "    y = df_sim[\"Y\"].to_numpy()\n",
    "    e = np.clip(df_sim[e_col].to_numpy(), 1e-6, 1.0 - 1.0e-6)\n",
    "\n",
    "    w1 = d / e\n",
    "    w0 = (1.0 - d) / (1.0 - e)\n",
    "\n",
    "    p1_hat = (w1 * y).sum() / max(w1.sum(), 1e-12)\n",
    "    p0_hat = (w0 * y).sum() / max(w0.sum(), 1e-12)\n",
    "\n",
    "    return float(p1_hat - p0_hat)\n",
    "\n",
    "\n",
    "def dr_logistic_ate_discount(df_sim: pd.DataFrame, e_col: str = \"e_true\") -> float:\n",
    "    \"\"\"Doubly-robust ATE with logistic outcome models for discounted vs non-discounted.\n",
    "\n",
    "    - Fit logistic models m1(x) and m0(x) for Y|X,D=1 and Y|X,D=0.\n",
    "    - Combine them with propensity-adjusted residuals.\n",
    "    \"\"\"\n",
    "    feature_cols = [c for c in df_sim.columns if c.startswith(\"x\")]\n",
    "    if not feature_cols:\n",
    "        raise ValueError(\"df_sim must contain feature columns starting with 'x'.\")\n",
    "    if not {\"D\", \"Y\", e_col}.issubset(df_sim.columns):\n",
    "        raise ValueError(\"df_sim must contain D, Y, and the propensity column.\")\n",
    "\n",
    "    X = df_sim[feature_cols].to_numpy()\n",
    "    d = df_sim[\"D\"].to_numpy()\n",
    "    y = df_sim[\"Y\"].to_numpy()\n",
    "    e = np.clip(df_sim[e_col].to_numpy(), 1e-6, 1.0 - 1.0e-6)\n",
    "\n",
    "    # Outcome models for Y|X,D=1 and Y|X,D=0\n",
    "    X1 = X[d == 1]\n",
    "    y1 = y[d == 1]\n",
    "    X0 = X[d == 0]\n",
    "    y0 = y[d == 0]\n",
    "\n",
    "    mdl1 = LogisticRegression(max_iter=1000).fit(X1, y1)\n",
    "    mdl0 = LogisticRegression(max_iter=1000).fit(X0, y0)\n",
    "\n",
    "    m1_hat = mdl1.predict_proba(X)[:, 1]\n",
    "    m0_hat = mdl0.predict_proba(X)[:, 1]\n",
    "\n",
    "    # DR formula\n",
    "    term = (m1_hat - m0_hat) + (d * (y - m1_hat) / e) - ((1.0 - d) * (y - m0_hat) / (1.0 - e))\n",
    "    return float(np.mean(term))\n",
    "\n",
    "\n",
    "# Single simulation demo\n",
    "df_sim_disc = simulate_discount_confounded(n=30_000, seed=777)\n",
    "ate_naive = naive_ate_discount(df_sim_disc)\n",
    "ate_ipw = ipw_ate_discount(df_sim_disc, e_col=\"e_true\")\n",
    "ate_dr = dr_logistic_ate_discount(df_sim_disc, e_col=\"e_true\")\n",
    "\n",
    "{\"ATE_naive\": ate_naive, \"ATE_IPW_true\": ate_ipw, \"ATE_DR_logistic\": ate_dr}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2cb46",
   "metadata": {},
   "source": [
    "\n",
    "### 10.1 Repeated simulation: bias and variance of estimators\n",
    "\n",
    "We now repeat the confounded discount experiment many times and compare the sampling distributions of:\n",
    "\n",
    "- naive difference in purchase rates,  \n",
    "- IPW with true propensities,  \n",
    "- DR with logistic outcome models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_discount_estimators(\n",
    "    R: int = 120,\n",
    "    n: int = 20_000,\n",
    "    seed: int | None = 999,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare naive, IPW, and DR estimators of ATE over R simulated experiments.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows: list[tuple[float, float, float]] = []\n",
    "    for _ in range(R):\n",
    "        s = int(rng.integers(0, 10_000_000))\n",
    "        df_s = simulate_discount_confounded(n=n, seed=s)\n",
    "        rows.append(\n",
    "            (\n",
    "                naive_ate_discount(df_s),\n",
    "                ipw_ate_discount(df_s, e_col=\"e_true\"),\n",
    "                dr_logistic_ate_discount(df_s, e_col=\"e_true\"),\n",
    "            )\n",
    "        )\n",
    "    return pd.DataFrame(rows, columns=[\"naive\", \"ipw\", \"dr_logit\"])\n",
    "\n",
    "\n",
    "res_disc = compare_discount_estimators(R=120, n=20_000, seed=2025)\n",
    "res_disc.agg([\"mean\", \"std\", \"min\", \"max\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = res_disc.plot(kind=\"hist\", bins=40, alpha=0.5)\n",
    "ax.set_title(\"ATE estimators under confounded discount assignment\")\n",
    "ax.set_xlabel(\"Estimated ATE on purchase probability\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d19650",
   "metadata": {},
   "source": [
    "\n",
    "**Key message for real pricing systems**\n",
    "\n",
    "- If discounts or price variants are **not randomized**, naive group comparisons are usually **biased**.  \n",
    "- IPW and DR estimators correct for this confounding when you have a decent propensity model and\n",
    "  an outcome model.  \n",
    "- The same pattern can be extended from **purchase probability** to **revenue** (with suitable\n",
    "  outcome models for revenue).  \n",
    "- For production systems, you would:\n",
    "  - log features \\(X\\), propensities \\(e(X)\\), discount/price actually shown,\n",
    "  - fit outcome models \\(m_t(X)\\) for conversion or revenue,\n",
    "  - use DR-style estimators to evaluate counterfactual pricing or discount policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d4a6e",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Bayesian demand and price elasticity\n",
    "\n",
    "So far we looked at:\n",
    "\n",
    "- **Frequentist** conversion and revenue tests,\n",
    "- A Bayesian view on **RPU**,\n",
    "- A coarse, point-estimate elasticity.\n",
    "\n",
    "Here we build a **Bayesian model for demand** (conversion probability) and derive a posterior\n",
    "distribution for the **price elasticity of demand** itself.\n",
    "\n",
    "We use:\n",
    "\n",
    "- A **Beta–Binomial** model for conversion in each arm,\n",
    "- The observed mean price in each arm,\n",
    "- A Monte Carlo approximation for the posterior distribution of elasticity.\n",
    "\n",
    "Let:\n",
    "\n",
    "- \\(p_A\\) and \\(p_B\\) be conversion probabilities at prices \\(P_A\\) and \\(P_B\\),\n",
    "- Control is baseline (arm A), treatment is alternative (arm B).\n",
    "\n",
    "The (arc) elasticity we approximate is:\n",
    "\n",
    "\\[\n",
    "\\epsilon = \\frac{(p_B - p_A)/p_A}{(P_B - P_A)/P_A},\n",
    "\\]\n",
    "\n",
    "which answers: *“For a given relative price change, what is the relative change in demand?”*\n",
    "\n",
    "We will obtain a **posterior distribution of \\\\(\\epsilon\\\\)** instead of a single point estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21056ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def posterior_demand_elasticity(\n",
    "    xA: int,\n",
    "    nA: int,\n",
    "    xB: int,\n",
    "    nB: int,\n",
    "    price_A: float,\n",
    "    price_B: float,\n",
    "    alpha0: float = 1.0,\n",
    "    beta0: float = 1.0,\n",
    "    n_draws: int = 50_000,\n",
    "    seed: int | None = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Sample from the posterior of demand and price elasticity.\n",
    "\n",
    "    We assume a Beta-Binomial model for each arm:\n",
    "\n",
    "    - p_A ~ Beta(alpha0, beta0),  p_B ~ Beta(alpha0, beta0)\n",
    "    - Data: xA successes out of nA, xB out of nB.\n",
    "    - Posterior:\n",
    "        p_A | data ~ Beta(alpha0 + xA, beta0 + nA - xA)\n",
    "        p_B | data ~ Beta(alpha0 + xB, beta0 + nB - xB)\n",
    "\n",
    "    We then define an arc elasticity (using control as baseline):\n",
    "\n",
    "        eps = ((p_B - p_A) / p_A) / ((P_B - P_A) / P_A)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xA, nA, xB, nB : int\n",
    "        Conversions and sample sizes for control (A) and treatment (B).\n",
    "    price_A, price_B : float\n",
    "        Mean prices in control and treatment arms.\n",
    "    alpha0, beta0 : float\n",
    "        Hyperparameters of the Beta prior (default: Beta(1, 1), uniform).\n",
    "    n_draws : int\n",
    "        Number of Monte Carlo draws from the posterior.\n",
    "    seed : int | None\n",
    "        Random seed for reproducibility.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Columns: pA, pB, elasticity with n_draws rows.\n",
    "    \"\"\"\n",
    "    if nA <= 0 or nB <= 0:\n",
    "        raise ValueError(\"Sample sizes nA and nB must be positive.\")\n",
    "    if price_A <= 0.0:\n",
    "        raise ValueError(\"price_A must be positive.\")\n",
    "    if price_B == price_A:\n",
    "        raise ValueError(\"price_B must differ from price_A to define elasticity.\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Posterior parameters\n",
    "    alphaA = alpha0 + xA\n",
    "    betaA = beta0 + nA - xA\n",
    "    alphaB = alpha0 + xB\n",
    "    betaB = beta0 + nB - xB\n",
    "\n",
    "    # Sample posterior conversion probabilities\n",
    "    pA_draws = rng.beta(alphaA, betaA, size=n_draws)\n",
    "    pB_draws = rng.beta(alphaB, betaB, size=n_draws)\n",
    "\n",
    "    # Relative price change (baseline: control)\n",
    "    dP_over_P = (price_B - price_A) / price_A\n",
    "\n",
    "    # Avoid division by zero in demand ratio\n",
    "    eps_list = []\n",
    "    for pA_i, pB_i in zip(pA_draws, pB_draws):\n",
    "        if pA_i <= 0.0 or dP_over_P == 0.0:\n",
    "            eps_list.append(np.nan)\n",
    "        else:\n",
    "            dQ_over_Q = (pB_i - pA_i) / pA_i\n",
    "            eps_list.append(dQ_over_Q / dP_over_P)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"pA\": pA_draws,\n",
    "            \"pB\": pB_draws,\n",
    "            \"elasticity\": np.asarray(eps_list, dtype=float),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Use observed conversion counts and prices from earlier sections\n",
    "xA_el = xA  # control conversions\n",
    "nA_el = nA  # control users\n",
    "xB_el = xB  # treatment conversions\n",
    "nB_el = nB  # treatment users\n",
    "\n",
    "price_A = float(df.loc[df[\"group\"] == \"control\", \"price\"].mean())\n",
    "price_B = float(df.loc[df[\"group\"] == \"treatment\", \"price\"].mean())\n",
    "\n",
    "post_elasticity = posterior_demand_elasticity(\n",
    "    xA=xA_el,\n",
    "    nA=nA_el,\n",
    "    xB=xB_el,\n",
    "    nB=nB_el,\n",
    "    price_A=price_A,\n",
    "    price_B=price_B,\n",
    "    alpha0=1.0,\n",
    "    beta0=1.0,\n",
    "    n_draws=60_000,\n",
    "    seed=2025,\n",
    ")\n",
    "post_elasticity.describe(percentiles=[0.025, 0.5, 0.975])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd09a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean NaNs if any (extreme rare corner cases)\n",
    "post_elasticity_valid = post_elasticity.dropna(subset=[\"elasticity\"]).copy()\n",
    "\n",
    "# Posterior summaries\n",
    "el_mean = float(post_elasticity_valid[\"elasticity\"].mean())\n",
    "el_median = float(post_elasticity_valid[\"elasticity\"].median())\n",
    "el_lo, el_hi = np.quantile(post_elasticity_valid[\"elasticity\"], [0.025, 0.975])\n",
    "\n",
    "# Probability of \"elastic\" demand in absolute value\n",
    "prob_elastic = float((post_elasticity_valid[\"elasticity\"].abs() > 1.0).mean())\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(post_elasticity_valid[\"elasticity\"], bins=80, density=True)\n",
    "plt.axvline(0.0, linestyle=\"--\")\n",
    "plt.title(\"Posterior distribution of price elasticity of demand\")\n",
    "plt.xlabel(\"elasticity (ε)\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "{\n",
    "    \"elasticity_mean\": el_mean,\n",
    "    \"elasticity_median\": el_median,\n",
    "    \"elasticity_CI95\": (el_lo, el_hi),\n",
    "    \"prob_abs_elasticity_gt_1\": prob_elastic,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b822a55",
   "metadata": {},
   "source": [
    "\n",
    "**Interpreting Bayesian demand elasticity**\n",
    "\n",
    "From the posterior distribution of elasticity:\n",
    "\n",
    "- Values **< 0** are the usual case: higher prices reduce demand.  \n",
    "- The **mean / median** give central estimates, but you should look at the **95% credible interval**\n",
    "  to understand uncertainty.  \n",
    "- `prob_abs_elasticity_gt_1` is the probability that demand is **elastic in magnitude**\n",
    "  (|ε| > 1), meaning demand changes more than proportionally to price changes.\n",
    "\n",
    "This is often more informative for pricing decisions than a single elasticity point estimate:\n",
    "you can articulate statements like:\n",
    "\n",
    "> “Given this experiment and our prior, there is a 70% probability that demand is elastic\n",
    ">  (|ε| > 1), and the 95% credible interval for ε is [−1.8, −0.4].”\n",
    "\n",
    "You can adapt the prior (`alpha0`, `beta0`) to incorporate historical data on conversion rates\n",
    "at similar price points, or extend the model to more than two prices via regression.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}