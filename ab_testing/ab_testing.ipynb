{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51549f7",
   "metadata": {},
   "source": [
    "\n",
    "# A/B Testing Playbook — Professional Notebook (Udacity + Criteo, with Modern Methods)\n",
    "\n",
    "This notebook is a **professional, end‑to‑end** A/B testing playbook built on **well‑known public datasets**.\n",
    "It serves as a **teaching** and **production** reference with **clear Markdown explanations** and **rigorous methods**.\n",
    "\n",
    "**Datasets included**\n",
    "1. **Udacity — Landing Page (`ab_data.csv`)** — user–level conversion test (old vs new page).  \n",
    "2. **Udacity — Free Trial Screener** — aggregate metrics (Gross & Net Conversion).  \n",
    "3. **Criteo Uplift Modeling** — treatment/control ads enabling **uplift/heterogeneous effects** analysis.\n",
    "\n",
    "**What you’ll find here**\n",
    "- Sanity checks (**SRM** by chi‑square), data validation and cleaning\n",
    "- EDA & rate visualizations (matplotlib only)\n",
    "- Frequentist tests: two‑proportion z‑test; **GLM(Logit)** with **day fixed effects**\n",
    "- **Bootstrap** confidence intervals for absolute lift\n",
    "- **CUPED** (variance reduction) with careful caveats and correct construction\n",
    "- **Bayesian Beta–Binomial** sanity check\n",
    "- **Power & MDE** planners, **achieved power**\n",
    "- **Multiple testing** correction (Holm) when assessing several metrics\n",
    "- **Modern uplift** evaluation (Criteo): uplift@K and Qini‑like area\n",
    "\n",
    "> Every code block is preceded by Markdown instructions and followed by a short interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7b5b3",
   "metadata": {},
   "source": [
    "## 0. Setup — Libraries & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebec864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Iterable\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional but standard for modeling\n",
    "from statsmodels.api import Logit, add_constant, OLS  # type: ignore\n",
    "from sklearn.linear_model import LogisticRegression  # used only in uplift demo\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (7, 4.5)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "RANDOM_SEED = 7\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e65f0a",
   "metadata": {},
   "source": [
    "### Core statistical helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c386439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProportionSummary:\n",
    "    p: float\n",
    "    n: int\n",
    "    x: int\n",
    "\n",
    "def summarize_proportion(x: int, n: int) -> ProportionSummary:\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive.\")\n",
    "    if not (0 <= x <= n):\n",
    "        raise ValueError(\"x must be in [0, n].\")\n",
    "    return ProportionSummary(p=x/n, n=n, x=x)\n",
    "\n",
    "def two_prop_ztest(x1: int, n1: int, x2: int, n2: int, two_sided: bool = True):\n",
    "    s1, s2 = summarize_proportion(x1, n1), summarize_proportion(x2, n2)\n",
    "    p_pool = (s1.x + s2.x) / (s1.n + s2.n)\n",
    "    se = math.sqrt(p_pool * (1 - p_pool) * (1/s1.n + 1/s2.n))\n",
    "    if se == 0.0:\n",
    "        raise ZeroDivisionError(\"SE=0; verify inputs.\")\n",
    "    z = (s1.p - s2.p) / se\n",
    "    p_val = 2 * (1 - 0.5 * (1 + math.erf(abs(z) / math.sqrt(2)))) if two_sided else (1 - 0.5 * (1 + math.erf(z / math.sqrt(2))))\n",
    "    return z, p_val\n",
    "\n",
    "def chisq_srm(nA: int, nB: int) -> float:\n",
    "    n = nA + nB\n",
    "    expected = [n/2, n/2]\n",
    "    observed = [nA, nB]\n",
    "    chi2 = sum((o - e)**2 / e for o, e in zip(observed, expected))\n",
    "    return 2 * (1 - 0.5 * (1 + math.erf(math.sqrt(chi2) / math.sqrt(2))))\n",
    "\n",
    "def bootstrap_ci_diff(pA: float, pB: float, nA: int, nB: int, B: int = 5000, alpha: float = 0.05):\n",
    "    diffs = np.empty(B, dtype=float)\n",
    "    for b in range(B):\n",
    "        xA = np.random.binomial(nA, pA)\n",
    "        xB = np.random.binomial(nB, pB)\n",
    "        diffs[b] = xB/nB - xA/nA\n",
    "    lo = float(np.quantile(diffs, alpha/2)); hi = float(np.quantile(diffs, 1 - alpha/2))\n",
    "    return lo, hi\n",
    "\n",
    "def required_n_two_proportions(pA: float, pB: float, alpha: float = 0.05, power: float = 0.8, two_sided: bool = True) -> int:\n",
    "    def invPhi(u: float) -> float:\n",
    "        return math.sqrt(2) * math.erfcinv(2*(1 - u))\n",
    "    z_alpha = abs(invPhi(1 - alpha/2)) if two_sided else abs(invPhi(1 - alpha))\n",
    "    z_beta = abs(invPhi(power))\n",
    "    pbar = 0.5*(pA + pB)\n",
    "    delta = abs(pB - pA)\n",
    "    if delta == 0.0:\n",
    "        raise ValueError(\"delta=0 implies infinite n.\")\n",
    "    se = math.sqrt(2 * pbar * (1 - pbar))\n",
    "    n = ((z_alpha + z_beta) * se / delta)**2\n",
    "    return int(math.ceil(n))\n",
    "\n",
    "def mde_for_n(pA: float, n_per_arm: int, alpha: float = 0.05, power: float = 0.8, two_sided: bool = True) -> float:\n",
    "    def invPhi(u: float) -> float:\n",
    "        return math.sqrt(2) * math.erfcinv(2*(1 - u))\n",
    "    z_alpha = abs(invPhi(1 - alpha/2)) if two_sided else abs(invPhi(1 - alpha))\n",
    "    z_beta = abs(invPhi(power))\n",
    "    se = math.sqrt(2 * pA * (1 - pA))\n",
    "    return float((z_alpha + z_beta) * se / math.sqrt(max(n_per_arm,1)))\n",
    "\n",
    "def holm_correction(pvals, alpha: float = 0.05):\n",
    "    import numpy as np\n",
    "    p = np.array(list(pvals), dtype=float)\n",
    "    m = len(p)\n",
    "    order = np.argsort(p)\n",
    "    adj = np.empty(m, dtype=float)\n",
    "    for k, idx in enumerate(order, start=1):\n",
    "        adj[idx] = (m - k + 1) * p[idx]\n",
    "    adj = np.maximum.accumulate(adj[order][::-1])[::-1]\n",
    "    return adj, order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce56691",
   "metadata": {},
   "source": [
    "## 1) Udacity — Landing Page (`ab_data.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9e51a",
   "metadata": {},
   "source": [
    "### Load, clean, SRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdf5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RAW_URL = \"https://raw.githubusercontent.com/udacity/sdand-ab-testing-project/main/ab_data.csv\"\n",
    "df = pd.read_csv(RAW_URL)\n",
    "mask_ok = ((df['group']=='control') & (df['landing_page']=='old_page')) | ((df['group']=='treatment') & (df['landing_page']=='new_page'))\n",
    "df = df[mask_ok].drop_duplicates('user_id', keep='first').copy()\n",
    "nA = (df['group']=='control').sum(); nB = (df['group']=='treatment').sum()\n",
    "p_srm = chisq_srm(nA, nB)\n",
    "nA, nB, p_srm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ab102",
   "metadata": {},
   "source": [
    "### Primary test + Bootstrap + MDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grp = df.groupby('group')['converted'].agg(['sum','count','mean']).rename(columns={'sum':'x','count':'n','mean':'rate'})\n",
    "A = summarize_proportion(int(grp.loc['control','x']), int(grp.loc['control','n']))\n",
    "B = summarize_proportion(int(grp.loc['treatment','x']), int(grp.loc['treatment','n']))\n",
    "z, p = two_prop_ztest(A.x, A.n, B.x, B.n, two_sided=True)\n",
    "ci_lo, ci_hi = bootstrap_ci_diff(A.p, B.p, A.n, B.n, B=3000, alpha=0.05)\n",
    "abs_lift = B.p - A.p\n",
    "n_per_arm = min(A.n, B.n)\n",
    "mde = mde_for_n(A.p, n_per_arm, 0.05, 0.8, True)\n",
    "pd.DataFrame({'arm':['control','treatment'],'n':[A.n,B.n],'x':[A.x,B.x],'rate':[A.p,B.p],'p_value(z)':[p,p],\n",
    "              'abs_lift(B-A)':[abs_lift,abs_lift],'boot_CI_lo':[ci_lo,ci_lo],'boot_CI_hi':[ci_hi,ci_hi],\n",
    "              'MDE_abs_at_80%_power':[mde,mde]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ec2c9",
   "metadata": {},
   "source": [
    "### GLM(Logit) with day fixed effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e3a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.api import add_constant, Logit  # type: ignore\n",
    "df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "X = pd.get_dummies(df[['group','date']], drop_first=True).rename(columns={'group_treatment':'treatment'}).astype(float)\n",
    "Xc = add_constant(X)\n",
    "y = df['converted'].astype(int).to_numpy()\n",
    "model = Logit(y, Xc).fit(disp=False)\n",
    "t_coef = model.params.get('treatment', float('nan')); t_se = model.bse.get('treatment', float('nan'))\n",
    "z_t = t_coef / t_se if t_se not in (0, float('nan')) else float('nan')\n",
    "p_t = 2 * (1 - 0.5*(1 + math.erf(abs(z_t)/math.sqrt(2)))) if not math.isnan(z_t) else float('nan')\n",
    "pd.DataFrame({'term':['treatment'],'coef':[t_coef],'se':[t_se],'z':[z_t],'p_value':[p_t]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431f5c7",
   "metadata": {},
   "source": [
    "### CUPED (day-control baseline) + OLS on adjusted outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.api import OLS  # type: ignore\n",
    "\n",
    "def cuped_transform(y, x_cov):\n",
    "    y = np.asarray(y, dtype=float); x_cov = np.asarray(x_cov, dtype=float)\n",
    "    if y.shape != x_cov.shape: raise ValueError(\"Shapes must match.\")\n",
    "    vx = np.var(x_cov); theta = 0.0 if vx==0 else float(np.cov(y, x_cov, ddof=1)[0,1] / vx)\n",
    "    return (y - theta * x_cov), theta\n",
    "\n",
    "df_c = df.copy()\n",
    "baseline = df_c[df_c['group']=='control'].groupby('date')['converted'].mean()\n",
    "df_c = df_c.merge(baseline.rename('baseline_day_control'), left_on='date', right_index=True, how='left')\n",
    "y = df_c['converted'].astype(float).to_numpy()\n",
    "x = df_c['baseline_day_control'].fillna(baseline.mean()).astype(float).to_numpy()\n",
    "y_adj, theta = cuped_transform(y, x)\n",
    "\n",
    "X_fe = add_constant(pd.get_dummies(df_c[['group','date']], drop_first=True).rename(columns={'group_treatment':'treatment'}).astype(float))\n",
    "mdl = OLS(y_adj, X_fe).fit()\n",
    "coef_t = mdl.params.get('treatment', float('nan')); se_t = mdl.bse.get('treatment', float('nan'))\n",
    "t_stat = coef_t / se_t if se_t not in (0, float('nan')) else float('nan')\n",
    "p_val = 2 * (1 - 0.5*(1 + math.erf(abs(t_stat)/math.sqrt(2)))) if not math.isnan(t_stat) else float('nan')\n",
    "pd.DataFrame({'theta':[theta],'coef_treatment_after_CUPED':[coef_t],'se':[se_t],'p_value_approx':[p_val]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12affe90",
   "metadata": {},
   "source": [
    "### Bayesian Beta–Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def beta_post_params(x, n, a0=1.0, b0=1.0):\n",
    "    return a0+x, b0+(n-x)\n",
    "\n",
    "def prob_B_gt_A(xA,nA,xB,nB, draws=50000):\n",
    "    aA,bA = beta_post_params(xA,nA); aB,bB = beta_post_params(xB,nB)\n",
    "    pA = np.random.beta(aA,bA,size=draws); pB = np.random.beta(aB,bB,size=draws)\n",
    "    return float(np.mean(pB > pA))\n",
    "\n",
    "p_B_gt_A = prob_B_gt_A(int(grp.loc['control','x']), int(grp.loc['control','n']),\n",
    "                       int(grp.loc['treatment','x']), int(grp.loc['treatment','n']))\n",
    "p_B_gt_A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa965c",
   "metadata": {},
   "source": [
    "## 2) Udacity — Free Trial Screener (Aggregates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae3d01",
   "metadata": {},
   "source": [
    "### Metrics, SRM, multiple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbbc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "totals = {\n",
    "    \"sanity\": {\"pageviews\": {\"control\": 345543, \"experiment\": 344660},\n",
    "               \"clicks\":    {\"control\": 28378,  \"experiment\": 28325}},\n",
    "    \"metrics\": {\"gross_conversion\": {\"clicks\": {\"control\": 17293, \"experiment\": 17260},\n",
    "                                     \"enrollments\": {\"control\": 3785, \"experiment\": 3423}},\n",
    "                \"net_conversion\":  {\"clicks\": {\"control\": 17293, \"experiment\": 17260},\n",
    "                                     \"payments\": {\"control\": 2033, \"experiment\": 1945}}}\n",
    "}\n",
    "\n",
    "def ratio_test(xA,nA,xB,nB):\n",
    "    A = summarize_proportion(xA,nA); B = summarize_proportion(xB,nB)\n",
    "    z,p = two_prop_ztest(A.x,A.n,B.x,B.n,True); lo,hi = bootstrap_ci_diff(A.p,B.p,A.n,B.n,3000,0.05)\n",
    "    return A,B,p,lo,hi\n",
    "\n",
    "p_srm_page = chisq_srm(totals[\"sanity\"][\"pageviews\"][\"control\"], totals[\"sanity\"][\"pageviews\"][\"experiment\"])\n",
    "p_srm_click = chisq_srm(totals[\"sanity\"][\"clicks\"][\"control\"], totals[\"sanity\"][\"clicks\"][\"experiment\"])\n",
    "\n",
    "A_gc,B_gc,p_gc,lo_gc,hi_gc = ratio_test(totals[\"metrics\"][\"gross_conversion\"][\"enrollments\"][\"control\"],\n",
    "                                        totals[\"metrics\"][\"gross_conversion\"][\"clicks\"][\"control\"],\n",
    "                                        totals[\"metrics\"][\"gross_conversion\"][\"enrollments\"][\"experiment\"],\n",
    "                                        totals[\"metrics\"][\"gross_conversion\"][\"clicks\"][\"experiment\"])\n",
    "A_nc,B_nc,p_nc,lo_nc,hi_nc = ratio_test(totals[\"metrics\"][\"net_conversion\"][\"payments\"][\"control\"],\n",
    "                                        totals[\"metrics\"][\"net_conversion\"][\"clicks\"][\"control\"],\n",
    "                                        totals[\"metrics\"][\"net_conversion\"][\"payments\"][\"experiment\"],\n",
    "                                        totals[\"metrics\"][\"net_conversion\"][\"clicks\"][\"experiment\"])\n",
    "adj, order = holm_correction([p_gc,p_nc], 0.05)\n",
    "pd.DataFrame({'metric':['gross_conversion','net_conversion'],\n",
    "              'control_rate':[A_gc.p,A_nc.p],'treatment_rate':[B_gc.p,B_nc.p],\n",
    "              'p_value_raw':[p_gc,p_nc],'p_value_Holm_adj':[adj[0],adj[1]],\n",
    "              'boot_CI_lo(B-A)':[lo_gc,lo_nc],'boot_CI_hi(B-A)':[hi_gc,hi_nc]}), {'SRM_pageviews_p':p_srm_page,'SRM_clicks_p':p_srm_click}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f8d19",
   "metadata": {},
   "source": [
    "### Power & MDE (Screener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_gc = min(totals[\"metrics\"][\"gross_conversion\"][\"clicks\"][\"control\"],\n",
    "           totals[\"metrics\"][\"gross_conversion\"][\"clicks\"][\"experiment\"])\n",
    "n_nc = min(totals[\"metrics\"][\"net_conversion\"][\"clicks\"][\"control\"],\n",
    "           totals[\"metrics\"][\"net_conversion\"][\"clicks\"][\"experiment\"])\n",
    "pA_gc = A_gc.p; pA_nc = A_nc.p\n",
    "mde_gc = mde_for_n(pA_gc, n_gc); mde_nc = mde_for_n(pA_nc, n_nc)\n",
    "pd.DataFrame({'metric':['gross_conversion','net_conversion'],\n",
    "              'n_per_arm':[n_gc, n_nc],\n",
    "              'baseline_rate(control)':[pA_gc, pA_nc],\n",
    "              'MDE_abs_at_80%_power':[mde_gc, mde_nc]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1763c",
   "metadata": {},
   "source": [
    "**CUPED note.** With only arm‑level totals, CUPED is not directly applicable. If you obtain daily aggregates, use day fixed effects or weighted GLM as a CUPED‑like strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292bd9c",
   "metadata": {},
   "source": [
    "## 3) Criteo Uplift — Heterogeneous Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595af16",
   "metadata": {},
   "source": [
    "### Load data & quick sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URL = \"https://raw.githubusercontent.com/uber/causalml/master/examples/data/criteo_uplift.csv.gz\"\n",
    "upl = pd.read_csv(URL, compression='gzip')\n",
    "if not {'treatment','conversion'}.issubset(upl.columns):\n",
    "    raise ValueError(\"Expected 'treatment' and 'conversion'.\")\n",
    "upl.groupby('treatment')['conversion'].agg(['mean','sum','count'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80260df",
   "metadata": {},
   "source": [
    "### Uplift scoring (T‑learner logistic), uplift@K and Qini-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab12a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "feat_cols = [c for c in upl.columns if c not in ('treatment','conversion')]\n",
    "X = upl[feat_cols].select_dtypes(include=[np.number]).fillna(0.0).to_numpy()\n",
    "y = upl['conversion'].to_numpy(); t = upl['treatment'].to_numpy()\n",
    "\n",
    "clf1 = LogisticRegression(max_iter=1000); clf0 = LogisticRegression(max_iter=1000)\n",
    "clf1.fit(X[t==1], y[t==1]); clf0.fit(X[t==0], y[t==0])\n",
    "p1 = clf1.predict_proba(X)[:,1]; p0 = clf0.predict_proba(X)[:,1]\n",
    "uplift_score = p1 - p0\n",
    "\n",
    "order = np.argsort(-uplift_score); y_ord = y[order]; t_ord = t[order]\n",
    "cum_treat = np.cumsum((t_ord==1) * y_ord); cum_ctrl  = np.cumsum((t_ord==0) * y_ord)\n",
    "cnt_treat = np.cumsum((t_ord==1).astype(int)); cnt_ctrl  = np.cumsum((t_ord==0).astype(int))\n",
    "rate_t = np.where(cnt_treat>0, cum_treat/np.maximum(cnt_treat,1), 0.0)\n",
    "rate_c = np.where(cnt_ctrl>0,  cum_ctrl/np.maximum(cnt_ctrl,1),  0.0)\n",
    "uplift_at_k = rate_t - rate_c\n",
    "qini = float(np.trapz(uplift_at_k, dx=1.0/len(uplift_at_k)))\n",
    "\n",
    "x = np.linspace(0,100,num=len(uplift_at_k))\n",
    "plt.figure(); plt.plot(x, uplift_at_k); plt.title(\"Uplift@K (T-learner logistic)\"); plt.xlabel(\"Top-K%\"); plt.ylabel(\"Δ rate\"); plt.tight_layout(); plt.show()\n",
    "qini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6df25d",
   "metadata": {},
   "source": [
    "**Reading.** Positive uplift among top‑ranked users indicates value in targeting. For production, prefer DR/X‑Learners with cross‑fitting and policy constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690952f7",
   "metadata": {},
   "source": [
    "\n",
    "# 5) Sequential Testing — Pocock & O'Brien–Fleming\n",
    "\n",
    "**Motivação.** Em testes online, é comum analisar resultados em múltiplos *looks* (peeking).  \n",
    "Sem correção, isto **infla** o erro tipo I. Métodos **group-sequential** (Pocock, OBF) ajustam limiares críticos por *look*.\n",
    "\n",
    "**Configuração.** Assumiremos um teste bicaudal (\\(\\alpha=0.05\\)) e \\(K\\) *looks* a frações de informação \\(t_i\\in(0,1]\\), p.ex. diárias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01471b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def z_alpha_two_sided(alpha: float = 0.05) -> float:\n",
    "    \"\"\"Return z_{1 - alpha/2} via inverse erfc.\"\"\"\n",
    "    return math.sqrt(2) * math.erfcinv(alpha)\n",
    "\n",
    "def obrien_fleming_boundaries(information_fracs: List[float], alpha: float = 0.05) -> List[float]:\n",
    "    \"\"\"\n",
    "    Two-sided O'Brien-Fleming critical z per look i: z_i = z_{1-alpha/2} / sqrt(t_i).\n",
    "    Symmetric boundaries ±z_i (conservative early, liberal late).\n",
    "    \"\"\"\n",
    "    if not information_fracs:\n",
    "        raise ValueError(\"information_fracs must be non-empty.\")\n",
    "    if not all(0 < t <= 1 for t in information_fracs):\n",
    "        raise ValueError(\"Each information fraction t must be in (0,1].\")\n",
    "    z_star = z_alpha_two_sided(alpha)\n",
    "    return [float(z_star / math.sqrt(t)) for t in information_fracs]\n",
    "\n",
    "def pocock_boundaries(K: int, alpha: float = 0.05) -> List[float]:\n",
    "    \"\"\"\n",
    "    Two-sided Pocock uses a constant boundary across looks.\n",
    "    In practice, values are tabulated. For simplicity, we provide good approximations for K<=10.\n",
    "    For two-sided alpha=0.05, the boundary is about ~2.41 for K in [2..10].\n",
    "    \"\"\"\n",
    "    if not (2 <= K <= 10):\n",
    "        raise ValueError(\"Pocock implementation supports 2<=K<=10 looks.\")\n",
    "    table = {k: 2.414 for k in range(2, 11)}\n",
    "    return [table[K]] * K\n",
    "\n",
    "def simulate_peeking(z_true: float, information_fracs: List[float], strategy: str = \"OBF\", alpha: float = 0.05, n_sims: int = 2000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate repeated z-statistics accumulating over looks.\n",
    "    - z_true: true noncentral mean of z at final look (effect size scaled).\n",
    "    - Generates Normal( mean = z_true*sqrt(t_i), sd=1 ) at each look to emulate information accrual.\n",
    "    - strategy: \"OBF\" or \"Pocock\"\n",
    "    Returns a DataFrame with stop proportions and boundary crossing rates.\n",
    "    \"\"\"\n",
    "    if strategy not in (\"OBF\", \"Pocock\"):\n",
    "        raise ValueError(\"strategy must be 'OBF' or 'Pocock'\")\n",
    "    K = len(information_fracs)\n",
    "    if strategy == \"OBF\":\n",
    "        bounds = obrien_fleming_boundaries(information_fracs, alpha=alpha)\n",
    "    else:\n",
    "        bounds = pocock_boundaries(K, alpha=alpha)\n",
    "\n",
    "    stops = np.zeros(K, dtype=int)\n",
    "    for _ in range(n_sims):\n",
    "        for i, t in enumerate(information_fracs):\n",
    "            z_i = np.random.normal(loc=z_true*math.sqrt(t), scale=1.0)\n",
    "            if abs(z_i) >= bounds[i]:\n",
    "                stops[i] += 1\n",
    "                break\n",
    "\n",
    "    prop = stops / n_sims\n",
    "    out = pd.DataFrame({\"look\": np.arange(1, K+1), \"info_frac\": information_fracs, \"crit_z\": bounds, \"stop_rate\": prop})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcdc14b",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Exemplo: limites e probabilidade de paragem\n",
    "\n",
    "Quatro *looks* (25%, 50%, 75%, 100%).  \n",
    "Comparamos OBF vs. Pocock sob um efeito verdadeiro ligeiro (\\(z_{true}=1.5\\)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "info = [0.25, 0.5, 0.75, 1.0]\n",
    "obf = obrien_fleming_boundaries(info, alpha=0.05)\n",
    "poc = pocock_boundaries(len(info), alpha=0.05)\n",
    "\n",
    "df_obf = simulate_peeking(z_true=1.5, information_fracs=info, strategy=\"OBF\", alpha=0.05, n_sims=2000)\n",
    "df_poc = simulate_peeking(z_true=1.5, information_fracs=info, strategy=\"Pocock\", alpha=0.05, n_sims=2000)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(info, obf, marker='o', label='OBF critical z')\n",
    "plt.plot(info, poc, marker='s', label='Pocock critical z')\n",
    "plt.title(\"Critical z by information fraction (two-sided α=0.05)\")\n",
    "plt.xlabel(\"Information fraction\"); plt.ylabel(\"Critical |z|\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_obf[\"look\"], df_obf[\"stop_rate\"], marker='o', label='OBF stop rate')\n",
    "plt.plot(df_poc[\"look\"], df_poc[\"stop_rate\"], marker='s', label='Pocock stop rate')\n",
    "plt.title(\"Stop rate per look (z_true=1.5)\")\n",
    "plt.xlabel(\"Look\"); plt.ylabel(\"Proportion stopped at look\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "df_obf, df_poc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef24b36",
   "metadata": {},
   "source": [
    "\n",
    "**Leitura.** OBF é conservador no início e mais permissivo no fim; Pocock mantém limite constante.  \n",
    "Ambos controlam \\(\\alpha\\) quando os *looks* são pré-definidos e os limites seguidos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c9185",
   "metadata": {},
   "source": [
    "\n",
    "# 6) Multi‑Armed Bandits — Thompson Sampling vs. A/B fixo\n",
    "\n",
    "**Objetivo.** Maximizar reward durante o teste, não apenas no fim.  \n",
    "Comparamos **Thompson Sampling** (Beta‑Bernoulli) a uma política fixa 50/50 em termos de **regret**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simulate_thompson_bernoulli(pA: float, pB: float, T: int = 10000, seed: int = 7) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Thompson Sampling para dois braços Bernoulli.\n",
    "    Retorna curves de regret cumulativo: TS vs 50/50 fixo.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    aA=bA=1.0; aB=bB=1.0  # Beta(1,1) priors\n",
    "    best = max(pA, pB)\n",
    "    regret_ts = np.zeros(T); regret_fixed = np.zeros(T)\n",
    "    cum_reg_ts = 0.0; cum_reg_fixed = 0.0\n",
    "\n",
    "    for t in range(T):\n",
    "        thetaA = rng.beta(aA, bA); thetaB = rng.beta(aB, bB)\n",
    "        arm = 0 if thetaA >= thetaB else 1\n",
    "        rewardA = rng.binomial(1, pA); rewardB = rng.binomial(1, pB)\n",
    "        reward = rewardA if arm==0 else rewardB\n",
    "        if arm==0: aA += reward; bA += 1 - reward\n",
    "        else:      aB += reward; bB += 1 - reward\n",
    "\n",
    "        cum_reg_ts += (best - (pA if arm==0 else pB))\n",
    "        regret_ts[t] = cum_reg_ts\n",
    "\n",
    "        cum_reg_fixed += (best - (0.5*pA + 0.5*pB))\n",
    "        regret_fixed[t] = cum_reg_fixed\n",
    "\n",
    "    return regret_ts, regret_fixed\n",
    "\n",
    "pA, pB = 0.10, 0.11\n",
    "reg_ts, reg_fx = simulate_thompson_bernoulli(pA, pB, T=8000, seed=11)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(reg_ts)+1), reg_ts, label=\"Thompson Sampling\")\n",
    "plt.plot(np.arange(1, len(reg_fx)+1), reg_fx, label=\"Fixed 50/50\")\n",
    "plt.title(\"Cumulative Regret — TS vs Fixed (pA=0.10, pB=0.11)\")\n",
    "plt.xlabel(\"Rounds\"); plt.ylabel(\"Cumulative regret\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c958d514",
   "metadata": {},
   "source": [
    "\n",
    "**Nota.** Bandits reduzem regret mas complicam a inferência. Para estimar efeito médio com bandits, use ponderação por propensão (IPW) e evite *naive peeking*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb6502",
   "metadata": {},
   "source": [
    "\n",
    "# 7) Decision Playbook — Regras Parametrizadas\n",
    "\n",
    "**Entrada:** lift observado e IC, **MDE** mínimo aceitável, benefício/custo, tráfego e horizonte.  \n",
    "**Saída:** recomendação `ship` / `hold` / `roll-back` + métricas de apoio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, Tuple\n",
    "\n",
    "Decision = Literal[\"ship\", \"hold\", \"roll-back\"]\n",
    "\n",
    "@dataclass\n",
    "class DecisionInputs:\n",
    "    lift_abs: float\n",
    "    ci_lo: float\n",
    "    ci_hi: float\n",
    "    baseline_rate: float\n",
    "    mde_abs: float\n",
    "    benefit_per_conversion: float\n",
    "    cost_per_user_exposed: float\n",
    "    traffic_per_day: int\n",
    "    risk_aversion: float = 1.0\n",
    "    min_days: int = 7\n",
    "\n",
    "def decision_playbook(inp: DecisionInputs) -> Tuple[Decision, dict]:\n",
    "    if not (0.0 <= inp.baseline_rate <= 1.0):\n",
    "        raise ValueError(\"baseline_rate must be in [0,1].\")\n",
    "    if inp.mde_abs <= 0.0:\n",
    "        raise ValueError(\"mde_abs must be positive.\")\n",
    "    if inp.traffic_per_day <= 0:\n",
    "        raise ValueError(\"traffic_per_day must be positive.\")\n",
    "\n",
    "    adj_lift = max(inp.ci_lo * inp.risk_aversion, inp.lift_abs / 2.0)\n",
    "\n",
    "    delta_conv_per_user = adj_lift\n",
    "    delta_conversions_day = delta_conv_per_user * inp.traffic_per_day\n",
    "    gross_benefit_day = delta_conversions_day * inp.benefit_per_conversion\n",
    "    cost_day = inp.traffic_per_day * inp.cost_per_user_exposed\n",
    "    net_benefit_horizon = (gross_benefit_day - cost_day) * inp.min_days\n",
    "\n",
    "    meets_mde = abs(adj_lift) >= inp.mde_abs\n",
    "    sig_positive = inp.ci_lo > 0.0\n",
    "    sig_negative = inp.ci_hi < 0.0\n",
    "\n",
    "    if sig_negative:\n",
    "        decision: Decision = \"roll-back\"\n",
    "    elif sig_positive and meets_mde and net_benefit_horizon > 0:\n",
    "        decision = \"ship\"\n",
    "    elif meets_mde and net_benefit_horizon > 0 and inp.lift_abs > 0:\n",
    "        decision = \"ship\"\n",
    "    else:\n",
    "        decision = \"hold\"\n",
    "\n",
    "    return decision, {\n",
    "        \"risk_adjusted_lift\": adj_lift,\n",
    "        \"meets_mde\": meets_mde,\n",
    "        \"sig_positive\": sig_positive,\n",
    "        \"sig_negative\": sig_negative,\n",
    "        \"net_benefit_horizon\": net_benefit_horizon,\n",
    "    }\n",
    "\n",
    "# Example\n",
    "example = DecisionInputs(\n",
    "    lift_abs=0.002,\n",
    "    ci_lo=-0.001, ci_hi=0.005,\n",
    "    baseline_rate=0.12,\n",
    "    mde_abs=0.003,\n",
    "    benefit_per_conversion=50.0,\n",
    "    cost_per_user_exposed=0.02,\n",
    "    traffic_per_day=20000,\n",
    "    risk_aversion=1.0,\n",
    "    min_days=14,\n",
    ")\n",
    "decision, details = decision_playbook(example)\n",
    "decision, details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c25ef",
   "metadata": {},
   "source": [
    "\n",
    "## 6.1 Inferência com Bandits — IPW (Inverse Propensity Weighting)\n",
    "\n",
    "Quando a alocação **não é fixa** (ex.: **Thompson Sampling**), a probabilidade de tratamento varia no tempo e por utilizador.\n",
    "Para obter uma estimativa **não-viesada** do efeito médio de tratamento (ATE) precisamos de **ponderar** cada observação pela **inversa da propensão** (probabilidade de ter recebido o braço observado).\n",
    "\n",
    "**Ideia básica do IPW (para outcome binário):**\n",
    "\\[\n",
    "\\widehat{\\Delta}_{IPW}\n",
    "= \\frac{\\sum_i w_i \\, T_i \\, Y_i}{\\sum_i w_i \\, T_i}\n",
    "- \\frac{\\sum_i w_i \\, (1-T_i) \\, Y_i}{\\sum_i w_i \\, (1-T_i)},\n",
    "\\quad\n",
    "w_i=\\frac{1}{\\Pr(A_i=T_i \\mid \\text{história}_i)}\n",
    "\\]\n",
    "\n",
    "- \\(T_i\\in\\{0,1\\}\\) é o tratamento, \\(Y_i\\in\\{0,1\\}\\) o outcome.\n",
    "- \\(\\Pr(A_i=T_i \\mid \\cdot)\\) é a **propensão** (probabilidade de seleção do braço) **no momento** da decisão.\n",
    "- Em TS, essa propensão vem do **posterior** no instante \\(i\\) (podemos logá-la durante o teste).\n",
    "\n",
    "Abaixo, simulamos um bandit simples **já a registar as propensões** e estimamos o ATE via **IPW** (simples e estabilizado).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def simulate_ts_with_propensity(pA: float, pB: float, T: int = 20000, seed: int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Thompson Sampling (dois braços Bernoulli) que REGISTA a propensão de escolha do braço aplicado.\n",
    "    Retorna um DataFrame com colunas: t, arm, reward, prop\n",
    "    - arm ∈ {0,1}\n",
    "    - prop = P(escolher 'arm' naquele passo), vinda do posterior Beta.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    aA=bA=1.0; aB=bB=1.0\n",
    "    rows = []\n",
    "    for t in range(T):\n",
    "        # Amostragem para decisão\n",
    "        thetaA = rng.beta(aA, bA); thetaB = rng.beta(aB, bB)\n",
    "        # Probabilidade de escolher A é P(thetaA >= thetaB) sob o posterior atual.\n",
    "        # Estimamos via Monte Carlo rápido:\n",
    "        thetasA = rng.beta(aA, bA, size=200)\n",
    "        thetasB = rng.beta(aB, bB, size=200)\n",
    "        p_choose_A = float(np.mean(thetasA >= thetasB))\n",
    "        p_choose_B = 1.0 - p_choose_A\n",
    "\n",
    "        arm = 0 if thetaA >= thetaB else 1\n",
    "        rewardA = rng.binomial(1, pA)\n",
    "        rewardB = rng.binomial(1, pB)\n",
    "        reward = rewardA if arm==0 else rewardB\n",
    "        if arm==0:\n",
    "            aA += reward; bA += 1 - reward\n",
    "            prop = p_choose_A\n",
    "        else:\n",
    "            aB += reward; bB += 1 - reward\n",
    "            prop = p_choose_B\n",
    "\n",
    "        rows.append((t, arm, reward, prop))\n",
    "    return pd.DataFrame(rows, columns=[\"t\",\"arm\",\"reward\",\"prop\"])\n",
    "\n",
    "def ipw_ate(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Estima ATE via IPW (simples e estabilizado).\n",
    "    Espera colunas: arm ∈ {0,1}, reward ∈ {0,1}, prop ∈ (0,1].\n",
    "    \"\"\"\n",
    "    if not set([\"arm\",\"reward\",\"prop\"]).issubset(df.columns):\n",
    "        raise ValueError(\"Columns arm, reward, prop are required.\")\n",
    "    # Pesos inversos\n",
    "    w = 1.0 / df[\"prop\"].to_numpy()\n",
    "    t = df[\"arm\"].to_numpy()\n",
    "    y = df[\"reward\"].to_numpy()\n",
    "\n",
    "    # IPW simples para cada braço\n",
    "    w_t = w * t\n",
    "    w_c = w * (1 - t)\n",
    "    p1_ipw = (w_t * y).sum() / max(w_t.sum(), 1e-12)\n",
    "    p0_ipw = (w_c * y).sum() / max(w_c.sum(), 1e-12)\n",
    "    delta_ipw = p1_ipw - p0_ipw\n",
    "\n",
    "    # IPW estabilizado: multiplica pesos por propensão marginal do braço\n",
    "    pi1 = float(t.mean())\n",
    "    pi0 = 1.0 - pi1\n",
    "    w_stab_t = w_t * pi1\n",
    "    w_stab_c = w_c * pi0\n",
    "    p1_stab = (w_stab_t * y).sum() / max(w_stab_t.sum(), 1e-12)\n",
    "    p0_stab = (w_stab_c * y).sum() / max(w_stab_c.sum(), 1e-12)\n",
    "    delta_stab = p1_stab - p0_stab\n",
    "\n",
    "    return {\n",
    "        \"p1_ipw\": p1_ipw, \"p0_ipw\": p0_ipw, \"delta_ipw\": delta_ipw,\n",
    "        \"p1_ipw_stab\": p1_stab, \"p0_ipw_stab\": p0_stab, \"delta_ipw_stab\": delta_stab,\n",
    "        \"pi1_observed\": pi1\n",
    "    }\n",
    "\n",
    "# Demo com pequeno lift\n",
    "sim = simulate_ts_with_propensity(pA=0.10, pB=0.11, T=40000, seed=42)\n",
    "res = ipw_ate(sim)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd4264",
   "metadata": {},
   "source": [
    "\n",
    "**Leitura.** `delta_ipw` e `delta_ipw_stab` aproximam o **efeito médio** sob alocação adaptativa.  \n",
    "Em produção: loga sempre as **propensões** por requisição (ou estima-as com um **policy model** fiel) e usa **intervalos** (bootstrap em blocos/tempo) para incerteza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99345de",
   "metadata": {},
   "source": [
    "\n",
    "## 5.2 Lan–DeMets Spending Functions — OBF e Pocock (generalização)\n",
    "\n",
    "Em vez de tabelas fixas, **Lan–DeMets** trata o controle do erro tipo I como uma **função de *spending* \\(\\alpha(t)\\)** ao longo da fração de informação \\(t\\in(0,1]\\).\n",
    "Duas escolhas clássicas reproduzem (aproximadamente) OBF e Pocock:\n",
    "\n",
    "- **OBF‑like:** \\(\\alpha(t) \\approx 2 - 2\\,\\Phi\\!\\left(\\frac{z_{\\alpha/2}}{\\sqrt{t}}\\right)\\).  \n",
    "  *Poupa* \\(\\alpha\\) no início e **gasta** no fim.\n",
    "- **Pocock‑like:** \\(\\alpha(t) \\approx \\alpha \\,\\log\\!\\big(1 + (e-1)\\,t\\big)\\).  \n",
    "  Gasto **mais uniforme** ao longo do tempo.\n",
    "\n",
    "Para *looks* discretos \\(t_1<\\dots<t_K\\), o *spending* por *look* é \\(\\alpha_i=\\alpha(t_i)-\\alpha(t_{i-1})\\), e podemos definir um limiar **crítico** aproximado como\n",
    "\\(z_i \\approx \\Phi^{-1}\\!\\big(1-\\alpha_i/2\\big)\\) (bicaudal).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Phi(z: float) -> float:\n",
    "    return 0.5 * (1 + math.erf(z / math.sqrt(2)))\n",
    "\n",
    "def Phi_inv(p: float) -> float:\n",
    "    # inverse CDF via erfcinv\n",
    "    # p in (0,1): z = sqrt(2) * erfcinv(2*(1-p))\n",
    "    return math.sqrt(2) * math.erfcinv(2*(1 - p))\n",
    "\n",
    "def spending_obf(t: np.ndarray, alpha: float = 0.05) -> np.ndarray:\n",
    "    z = Phi_inv(1 - alpha/2.0)\n",
    "    return 2 - 2*Phi(z/np.sqrt(np.clip(t, 1e-12, 1.0)))\n",
    "\n",
    "def spending_pocock(t: np.ndarray, alpha: float = 0.05) -> np.ndarray:\n",
    "    return alpha * np.log(1 + (math.e - 1)*t)\n",
    "\n",
    "def ld_boundaries(info_fracs, alpha=0.05, kind=\"OBF\"):\n",
    "    t = np.asarray(info_fracs, dtype=float)\n",
    "    if kind.upper() == \"OBF\":\n",
    "        A = spending_obf(t, alpha=alpha)\n",
    "    else:\n",
    "        A = spending_pocock(t, alpha=alpha)\n",
    "    A_prev = np.r_[0.0, A[:-1]]\n",
    "    alpha_i = np.clip(A - A_prev, 1e-10, 1.0)  # incremental spending\n",
    "    # approximate two-sided critical z for each look\n",
    "    z_i = Phi_inv(1 - alpha_i/2.0)\n",
    "    return pd.DataFrame({\"look\": np.arange(1, len(t)+1), \"t\": t, \"alpha_cum\": A, \"alpha_inc\": alpha_i, \"crit_z\": z_i})\n",
    "\n",
    "# Example grid of information fractions\n",
    "t = np.array([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "df_obf = ld_boundaries(t, alpha=0.05, kind=\"OBF\")\n",
    "df_poc = ld_boundaries(t, alpha=0.05, kind=\"POCOCK\")\n",
    "\n",
    "# Plot spending\n",
    "plt.figure()\n",
    "plt.plot(df_obf[\"t\"], df_obf[\"alpha_cum\"], marker='o', label=\"OBF-like α(t)\")\n",
    "plt.plot(df_poc[\"t\"], df_poc[\"alpha_cum\"], marker='s', label=\"Pocock-like α(t)\")\n",
    "plt.title(\"Lan–DeMets α-spending (two-sided α=0.05)\")\n",
    "plt.xlabel(\"Information fraction t\"); plt.ylabel(\"Cumulative α(t)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Plot critical z per look\n",
    "plt.figure()\n",
    "plt.plot(df_obf[\"t\"], df_obf[\"crit_z\"], marker='o', label=\"OBF-like critical z\")\n",
    "plt.plot(df_poc[\"t\"], df_poc[\"crit_z\"], marker='s', label=\"Pocock-like critical z\")\n",
    "plt.title(\"Approximate critical |z| per look via α-spending\")\n",
    "plt.xlabel(\"Information fraction t\"); plt.ylabel(\"Critical |z|\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "df_obf, df_poc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d484cf",
   "metadata": {},
   "source": [
    "\n",
    "**Notas práticas.**\n",
    "- A implementação acima fornece **aproximações úteis** para planeamento e comunicação.\n",
    "- Em ambientes regulamentados, use bibliotecas validadas de **group‑sequential design** que resolvem exatamente os limiares.\n",
    "- Para testes **desbalanceados** (1:m), substitui a fração de informação por uma métrica adequada (p.ex., informação de Fisher cumulativa).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e08c5",
   "metadata": {},
   "source": [
    "\n",
    "## 6.2 ICs com Bandits — **Block Bootstrap Temporal**\n",
    "\n",
    "Com **bandits**, as observações são **dependentes** ao longo do tempo (propensões mudam, *posterior* atualizada).\n",
    "Para intervalos de confiança mais realistas, usamos **block bootstrap por tempo**:\n",
    "\n",
    "1. Escolhe um **tamanho de bloco** \\(B\\) (p.ex., 200–1000 rondas), cobrindo a dependência local.  \n",
    "2. Particiona a série temporal em blocos contíguos.  \n",
    "3. Reamostra blocos **com reposição** até reconstruir uma série do mesmo tamanho.  \n",
    "4. Recalcula o estimador (IPW) em cada *replicate* → quantis \\([2.5\\%, 97.5\\%]\\).\n",
    "\n",
    "Abaixo: funções `block_bootstrap_ipw` e um exemplo rápido sobre a simulação `sim` criada na secção 6.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def block_bootstrap_ipw(df: pd.DataFrame, block_size: int = 500, B: int = 400, seed: int = 123) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Block bootstrap (temporal) do delta_IPW.\n",
    "    - df: deve conter colunas ['t','arm','reward','prop'] da simulação bandit.\n",
    "    - block_size: tamanho do bloco contíguo de tempo.\n",
    "    - B: número de réplicas bootstrap.\n",
    "    Retorna (lo, hi) para ~95% CI por percentis.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Sanidade\n",
    "    req = {'t','arm','reward','prop'}\n",
    "    if not req.issubset(df.columns):\n",
    "        raise ValueError(f\"df must include columns {req}\")\n",
    "\n",
    "    n = len(df)\n",
    "    # Construir blocos contíguos\n",
    "    blocks = []\n",
    "    for start in range(0, n, block_size):\n",
    "        end = min(start + block_size, n)\n",
    "        blocks.append(df.iloc[start:end])\n",
    "\n",
    "    # Função IPW simples (sem estabilização) para rapidez\n",
    "    def ipw_delta_local(d: pd.DataFrame) -> float:\n",
    "        w = 1.0 / np.clip(d[\"prop\"].to_numpy(), 1e-12, None)\n",
    "        t = d[\"arm\"].to_numpy()\n",
    "        y = d[\"reward\"].to_numpy()\n",
    "        w_t = w * t\n",
    "        w_c = w * (1 - t)\n",
    "        p1 = (w_t * y).sum() / max(w_t.sum(), 1e-12)\n",
    "        p0 = (w_c * y).sum() / max(w_c.sum(), 1e-12)\n",
    "        return float(p1 - p0)\n",
    "\n",
    "    # Bootstrap\n",
    "    boots = np.empty(B, dtype=float)\n",
    "    m = len(blocks)\n",
    "    for b in range(B):\n",
    "        # Amostrar blocos com reposição e concatenar\n",
    "        idx = rng.integers(0, m, size=m)\n",
    "        df_star = pd.concat([blocks[i] for i in idx], ignore_index=True).iloc[:n]\n",
    "        boots[b] = ipw_delta_local(df_star)\n",
    "\n",
    "    lo = float(np.quantile(boots, 0.025))\n",
    "    hi = float(np.quantile(boots, 0.975))\n",
    "    lo, hi\n",
    "\n",
    "# Exemplo: ICs block-bootstrap para IPW da simulação (secção 6.1)\n",
    "ci_lo_ipw, ci_hi_ipw = block_bootstrap_ipw(sim, block_size=800, B=300, seed=777)\n",
    "{\"ipw_block_bootstrap_CI95\": (ci_lo_ipw, ci_hi_ipw)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d29557",
   "metadata": {},
   "source": [
    "\n",
    "**Notas práticas.**\n",
    "- **Sensibilidade ao tamanho do bloco**: aumente \\(B\\) e varie o `block_size` em *stress tests*.  \n",
    "- Para logs reais, use **block bootstrap por dia/slot** (respeitando resets do tráfego e sazonalidade).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a595476",
   "metadata": {},
   "source": [
    "\n",
    "## 6.3 **Doubly‑Robust (DR) Estimator** — Consistência sob *misspecification*\n",
    "\n",
    "O estimador **DR** combina **IPW** com um **modelo de outcome** \\(m_t(x)=\\mathbb{E}[Y\\mid X=x, T=t]\\).\n",
    "É **consistente** se **ou** a propensão \\(e(x)=P(T=1\\mid X)\\) **ou** o modelo de outcome estiver correto (não precisam ambos).\n",
    "\n",
    "**Forma (binário, propensão conhecida/estimada \\(e_i\\))**:\n",
    "\\[\n",
    "\\widehat{\\tau}_{DR}\n",
    "= \\frac{1}{n}\\sum_i \\Big[ \\big(m_1(x_i)-m_0(x_i)\\big)\n",
    "+ \\frac{T_i(Y_i - m_1(x_i))}{e_i}\n",
    "- \\frac{(1-T_i)(Y_i - m_0(x_i))}{1-e_i} \\Big].\n",
    "\\]\n",
    "\n",
    "Abaixo, mostramos:\n",
    "1. Um **modelo de outcome simples** (constante por braço) — robusto e estável.  \n",
    "2. Uma versão com **logistic regression** (quando houver *features* \\(X\\)).  \n",
    "3. **ICs via block bootstrap** para \\(\\widehat{\\tau}_{DR}\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd0b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def dr_constant_outcome(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    DR com outcome models constantes por braço:\n",
    "    m1(x)=mean(Y|T=1), m0(x)=mean(Y|T=0).\n",
    "    Usa propensões \"prop\" registadas.\n",
    "    \"\"\"\n",
    "    req = {'arm','reward','prop'}\n",
    "    if not req.issubset(df.columns):\n",
    "        raise ValueError(f\"df must include columns {req}\")\n",
    "    t = df['arm'].to_numpy()\n",
    "    y = df['reward'].to_numpy()\n",
    "    e = np.clip(df['prop'].to_numpy(), 1e-6, 1-1e-6)\n",
    "\n",
    "    # Outcome models (constantes)\n",
    "    m1 = float(df.loc[df['arm']==1, 'reward'].mean())\n",
    "    m0 = float(df.loc[df['arm']==0, 'reward'].mean())\n",
    "\n",
    "    term = (m1 - m0) + (t * (y - m1) / e) - ((1 - t) * (y - m0) / (1 - e))\n",
    "    return float(np.mean(term))\n",
    "\n",
    "def block_bootstrap_dr(df: pd.DataFrame, block_size: int = 500, B: int = 300, seed: int = 123) -> tuple[float,float,float]:\n",
    "    \"\"\"\n",
    "    Block bootstrap temporal para o estimador DR (constante por braço).\n",
    "    Retorna (tau_dr_hat, lo, hi).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(df)\n",
    "    # Construir blocos contíguos\n",
    "    blocks = []\n",
    "    for start in range(0, n, block_size):\n",
    "        end = min(start + block_size, n)\n",
    "        blocks.append(df.iloc[start:end])\n",
    "\n",
    "    tau0 = dr_constant_outcome(df)\n",
    "\n",
    "    boots = np.empty(B, dtype=float)\n",
    "    m = len(blocks)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, m, size=m)\n",
    "        df_star = pd.concat([blocks[i] for i in idx], ignore_index=True).iloc[:n]\n",
    "        boots[b] = dr_constant_outcome(df_star)\n",
    "\n",
    "    lo = float(np.quantile(boots, 0.025))\n",
    "    hi = float(np.quantile(boots, 0.975))\n",
    "    return float(tau0), lo, hi\n",
    "\n",
    "tau_dr, lo_dr, hi_dr = block_bootstrap_dr(sim, block_size=800, B=300, seed=2024)\n",
    "{\"dr_tau_hat\": tau_dr, \"dr_block_bootstrap_CI95\": (lo_dr, hi_dr)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a2636",
   "metadata": {},
   "source": [
    "\n",
    "**Quando usar qual?**\n",
    "- **IPW** é simples quando tens **propensões fiáveis** (logged).  \n",
    "- **DR** oferece **robustez**: se o modelo de outcome for razoável (mesmo simples), ganhas proteção caso as propensões estejam levemente mal especificadas — e vice‑versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740ec40",
   "metadata": {},
   "source": [
    "\n",
    "## 6.4 Doubly‑Robust with **Logistic Outcome Models** (with user features)\n",
    "\n",
    "In many real experiments you have per‑user covariates **X** (e.g., device, geo, prior activity).  \n",
    "To improve efficiency and robustness, we can fit **two logistic outcome models**:\n",
    "\\(\n",
    "m_1(x)=\\Pr(Y=1\\mid X=x, T=1),\\quad\n",
    "m_0(x)=\\Pr(Y=1\\mid X=x, T=0)\n",
    "\\)\n",
    "and plug them into the DR formula:\n",
    "\\[\n",
    "\\widehat{\\tau}_{DR}\n",
    "= \\frac{1}{n}\\sum_i \\Big[ (m_1(x_i)-m_0(x_i))\n",
    "+ \\frac{T_i(Y_i - m_1(x_i))}{e_i}\n",
    "- \\frac{(1-T_i)(Y_i - m_0(x_i))}{1-e_i} \\Big].\n",
    "\\]\n",
    "\n",
    "Below we **simulate a confounded scenario** where the treatment propensity depends on **X**, and the outcome also depends on **X** and **T**.  \n",
    "We compare **IPW** vs **DR (logistic)** in terms of bias and variability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def simulate_confounded(n: int = 20000, seed: int = 123) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate binary outcome with confounding via X -> treatment and X -> outcome.\n",
    "    - Generate features X ~ N(0,1)^d\n",
    "    - Propensity e(x) = sigmoid(bias_e + w_e^T x)\n",
    "    - Outcome logit: logit P(Y=1 | X, T) = bias_y + w_y^T x + tau_true * T\n",
    "    Returns DataFrame with X columns, T, Y, and true e(x).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    d = 4\n",
    "    X = rng.normal(size=(n, d))\n",
    "    # Propensity model\n",
    "    w_e = np.array([0.8, -0.6, 0.4, 0.2])\n",
    "    bias_e = -0.2\n",
    "    e = sigmoid(bias_e + X @ w_e)\n",
    "    T = rng.binomial(1, e)\n",
    "    # Outcome model\n",
    "    w_y = np.array([0.5, 0.3, -0.2, 0.1])\n",
    "    bias_y = -1.2\n",
    "    tau_true = 0.12  # log-odds lift due to treatment\n",
    "    # Convert to probability\n",
    "    lin = bias_y + X @ w_y + tau_true * T\n",
    "    p = sigmoid(lin)\n",
    "    Y = rng.binomial(1, p)\n",
    "    cols = {f\"x{j+1}\": X[:, j] for j in range(d)}\n",
    "    df = pd.DataFrame(cols)\n",
    "    df[\"T\"] = T\n",
    "    df[\"Y\"] = Y\n",
    "    df[\"e_true\"] = e\n",
    "    # For fair comparison, we assume we know/estimate e(x).\n",
    "    # In practice, you'd fit a separate propensity model on (X,T).\n",
    "    return df\n",
    "\n",
    "def ipw_ate_from_df(df: pd.DataFrame, e_col: str = \"e_true\") -> float:\n",
    "    t = df[\"T\"].to_numpy(); y = df[\"Y\"].to_numpy()\n",
    "    e = np.clip(df[e_col].to_numpy(), 1e-6, 1-1e-6)\n",
    "    w_t = t / e\n",
    "    w_c = (1 - t) / (1 - e)\n",
    "    p1 = (w_t * y).sum() / max(w_t.sum(), 1e-12)\n",
    "    p0 = (w_c * y).sum() / max(w_c.sum(), 1e-12)\n",
    "    return float(p1 - p0)\n",
    "\n",
    "def dr_ate_logistic(df: pd.DataFrame, e_col: str = \"e_true\") -> float:\n",
    "    \"\"\"\n",
    "    DR estimator with two logistic outcome models.\n",
    "    \"\"\"\n",
    "    X = df[[c for c in df.columns if c.startswith(\"x\")]].to_numpy()\n",
    "    t = df[\"T\"].to_numpy()\n",
    "    y = df[\"Y\"].to_numpy()\n",
    "    e = np.clip(df[e_col].to_numpy(), 1e-6, 1-1e-6)\n",
    "\n",
    "    # Fit outcome models separately by arm\n",
    "    X1 = X[t == 1]; y1 = y[t == 1]\n",
    "    X0 = X[t == 0]; y0 = y[t == 0]\n",
    "    m1_lr = LogisticRegression(max_iter=1000).fit(X1, y1)\n",
    "    m0_lr = LogisticRegression(max_iter=1000).fit(X0, y0)\n",
    "\n",
    "    m1_hat = m1_lr.predict_proba(X)[:, 1]\n",
    "    m0_hat = m0_lr.predict_proba(X)[:, 1]\n",
    "\n",
    "    term = (m1_hat - m0_hat) + (t * (y - m1_hat) / e) - ((1 - t) * (y - m0_hat) / (1 - e))\n",
    "    return float(np.mean(term))\n",
    "\n",
    "# Single-run demo\n",
    "df_sim = simulate_confounded(n=30000, seed=2025)\n",
    "ate_ipw = ipw_ate_from_df(df_sim, e_col=\"e_true\")\n",
    "ate_dr  = dr_ate_logistic(df_sim, e_col=\"e_true\")\n",
    "{\"IPW_ATE\": ate_ipw, \"DR_logistic_ATE\": ate_dr}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49203f4b",
   "metadata": {},
   "source": [
    "\n",
    "### Repeated simulation: bias/variance comparison\n",
    "\n",
    "We repeat the simulation **R** times to inspect sampling variability and bias (relative to the true effect in log‑odds ≈ 0.12).  \n",
    "Note: IPW/DR estimate **difference in probabilities**, while the DGP uses a log‑odds lift.  \n",
    "The sign and **relative** performance (variance and bias) is the focus here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47665120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def repeat_compare(R: int = 100, n: int = 20000, seed: int = 9) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = []\n",
    "    for r in range(R):\n",
    "        df = simulate_confounded(n=n, seed=int(rng.integers(0, 10_000_000)))\n",
    "        ipw = ipw_ate_from_df(df, e_col=\"e_true\")\n",
    "        dr  = dr_ate_logistic(df, e_col=\"e_true\")\n",
    "        rows.append((ipw, dr))\n",
    "    out = pd.DataFrame(rows, columns=[\"ipw\", \"dr_logit\"])\n",
    "    return out\n",
    "\n",
    "res = repeat_compare(R=120, n=20000, seed=17)\n",
    "summary = res.agg([\"mean\",\"std\",\"median\",\"min\",\"max\"])\n",
    "ax = res.plot(kind=\"hist\", bins=40, alpha=0.6)\n",
    "ax.set_title(\"Sampling distribution: IPW vs DR (logistic)\")\n",
    "ax.set_xlabel(\"Estimated ATE (probability difference)\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f98ae4",
   "metadata": {},
   "source": [
    "\n",
    "**Takeaways.**\n",
    "- **DR(logistic)** tends to have **lower variance** and often **smaller bias** than pure IPW, especially when outcome models are reasonably specified.\n",
    "- With **misspecification**, DR remains **consistent** if *either* the propensity (here known) *or* the outcome model is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e36315",
   "metadata": {},
   "source": [
    "\n",
    "## 8) **Experiment Launch Checklist** (Operational & Statistical)\n",
    "\n",
    "Use this checklist to prevent preventable failures and to keep your results decision‑grade.\n",
    "\n",
    "**Design & Instrumentation**\n",
    "- ☑ Clearly define the **primary metric** (and its unit of analysis) and the **direction** of improvement.  \n",
    "- ☑ Pre-register **stopping rules**: fixed‑horizon or **group‑sequential** (Pocock/OBF/Lan–DeMets).  \n",
    "- ☑ Define **exposure eligibility** and **unit consistency** (user/session/pageview).  \n",
    "- ☑ Implement **SRM checks** (sample ratio mismatch) on traffic split and funnels.  \n",
    "- ☑ Log **timestamps**, **assignments**, **propensities** (if adaptive), and **covariates**.\n",
    "\n",
    "**Power/MDE & Risks**\n",
    "- ☑ Compute **MDE** at the baseline rate and traffic constraints; ensure business‑relevant effects are detectable.  \n",
    "- ☑ Set a **maximum duration** to avoid state drift (seasonality, product changes).  \n",
    "- ☑ If multiple KPIs, plan **multiplicity correction** (Holm or hierarchical testing).\n",
    "\n",
    "**Analysis Plan**\n",
    "- ☑ Primary analysis: two‑proportion test **with CIs**; keep a **GLM with day fixed effects** as robustness.  \n",
    "- ☑ If pre‑treatment covariates exist, consider **CUPED** or regression adjustment.  \n",
    "- ☑ If adaptive allocation (**bandits**), use **IPW/DR** + **time‑block bootstrap** for uncertainty.  \n",
    "- ☑ Guard against **peeking**: sequential boundaries / α‑spending. Store **look timestamps**.\n",
    "\n",
    "**Decision Framework**\n",
    "- ☑ Predefine **business rules** (MDE threshold, benefit/cost, risk tolerance).  \n",
    "- ☑ Use the **Decision Playbook** function to formalise **ship / hold / roll‑back**.  \n",
    "- ☑ For wins, plan a **ramp plan** (e.g., 10% → 25% → 50% → 100%) with monitoring gates.  \n",
    "- ☑ For losses, document learnings and update hypotheses/backlog.\n",
    "\n",
    "**Post‑Mortem & Governance**\n",
    "- ☑ Archive notebook, raw exports, and code versions (commit hash, environment).  \n",
    "- ☑ Summarise findings with **CIs**, effect sizes, and operational notes (incidents, outages).  \n",
    "- ☑ Maintain an **Experiment Registry** with metadata (owner, metrics, dates, links, blocking rules).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
