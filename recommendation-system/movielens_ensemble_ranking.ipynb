{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# MovieLens Recommender \u2013 Ensemble for Ranking (Precision@K / Recall@K)\n\nIn this notebook we move from **rating prediction (RMSE)** to\n**ranking-oriented evaluation** and ensembles.\n\nWe will:\n\n1. Use the MovieLens `ml-latest-small` dataset (`ratings.csv`).\n2. Train three **base models** on explicit ratings:\n   - Bias model (global + user + item effects).\n   - Item-based k-NN collaborative filtering.\n   - Matrix factorisation with biases.\n3. Define **relevance** as `rating \u2265 threshold` (e.g. 4.0).\n4. Evaluate each base model with **ranking metrics**:\n   - Hit-rate@K.\n   - Precision@K.\n   - Recall@K.\n5. Build a **stacked ensemble for ranking**:\n   - Use base model scores as features.\n   - Train a **logistic regression** meta-model to predict relevance.\n   - Rank by predicted probability of relevance.\n6. Compare base models, simple average, and stacked ensemble on ranking\n   metrics, and interpret the meta-model weights.\n\nThis gives a template for **blending recommender models** when your goal\nis top-N recommendation quality rather than rating RMSE.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration\n\nWe use:\n\n- `pandas`, `numpy` \u2013 data handling.\n- `matplotlib`, `seaborn` \u2013 plots.\n- `scikit-learn` \u2013 splits, similarity, and meta-learning.\n\nAll recommender logic (bias, item-kNN, MF) is implemented in pure\nPython/NumPy for transparency.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Set\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.linear_model import LogisticRegression\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_DIR: Path = Path(\"data\") / \"ml-latest-small\"\nRATINGS_PATH: Path = DATA_DIR / \"ratings.csv\"\n\nif not RATINGS_PATH.exists():\n    raise FileNotFoundError(\n        f\"Ratings file not found at {RATINGS_PATH.resolve()}. \"\n        \"Please ensure MovieLens 'ml-latest-small' is under data/ml-latest-small/.\"\n    )\n\nratings_df = pd.read_csv(RATINGS_PATH)\nprint(\"Ratings shape:\", ratings_df.shape)\nratings_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1.1 Utility metrics\n\nWe keep RMSE just for reference, but our target metrics will be\nhit-rate@K, precision@K, recall@K.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute root mean squared error.\n\n    Args:\n        y_true: True ratings.\n        y_pred: Predicted ratings.\n\n    Returns:\n        RMSE value.\n    \"\"\"\n    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n\n\nsns.histplot(ratings_df[\"rating\"], bins=10)\nplt.title(\"Rating distribution\")\nplt.xlabel(\"Rating\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Train / meta / test splits\n\nWe reuse the three-way split structure suitable for stacking:\n\n1. Split into `train_full` (80%) and `test` (20%).\n2. Split `train_full` into `base_train` (80%) and `meta_train` (20%).\n\n- Base models are fitted on `base_train`.\n- Meta-model is trained on `meta_train` **using base model scores**.\n- `test` is held out until final evaluation.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Step 1: main train / test split\n\ntrain_full_df, test_df = train_test_split(\n    ratings_df,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\n# Step 2: split train_full into base_train and meta_train\n\nbase_train_df, meta_train_df = train_test_split(\n    train_full_df,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\nprint(\"Base train size:\", base_train_df.shape[0])\nprint(\"Meta train size:\", meta_train_df.shape[0])\nprint(\"Test size:      \", test_df.shape[0])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Base models (same as previous ensemble notebook)\n\nWe define three base models:\n\n1. `BiasModel` \u2013 global + user + item biases.\n2. `ItemKNNModel` \u2013 item-based k-NN CF.\n3. `MatrixFactorizationModel` \u2013 latent factor model.\n\nEach has a consistent interface:\n\n- `fit(df)` \u2013 train on a ratings DataFrame.\n- `predict_df(df)` \u2013 predict scores for `(userId, movieId)` pairs.\n\nThe *scores* they output are used for both rating and ranking tasks.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.1 BiasModel\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class BiasModel:\n    \"\"\"Global + user + item bias recommender.\n\n    Predicts ratings as global mean plus user-specific and item-specific\n    deviations from that mean.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.mu: float | None = None\n        self.user_bias: Dict[int, float] = {}\n        self.item_bias: Dict[int, float] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        \"\"\"Fit bias terms from a ratings DataFrame.\n\n        Args:\n            df: DataFrame with columns `userId`, `movieId`, `rating`.\n        \"\"\"\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        self.mu = float(df[\"rating\"].mean())\n\n        user_mean = df.groupby(\"userId\")[\"rating\"].mean()\n        item_mean = df.groupby(\"movieId\")[\"rating\"].mean()\n\n        self.user_bias = (user_mean - self.mu).to_dict()\n        self.item_bias = (item_mean - self.mu).to_dict()\n\n    def predict_row(self, user_id: int, movie_id: int) -> float:\n        \"\"\"Predict rating for a single user\u2013item pair.\n\n        Args:\n            user_id: User identifier.\n            movie_id: Movie identifier.\n\n        Returns:\n            Predicted rating.\n        \"\"\"\n        if self.mu is None:\n            raise RuntimeError(\"Model has not been fitted.\")\n\n        bu = self.user_bias.get(user_id, 0.0)\n        bi = self.item_bias.get(movie_id, 0.0)\n        return float(self.mu + bu + bi)\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Predict ratings for multiple user\u2013item pairs.\n\n        Args:\n            df: DataFrame with `userId`, `movieId`.\n\n        Returns:\n            Array of predictions aligned with `df` rows.\n        \"\"\"\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self.predict_row(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nbias_model = BiasModel()\nbias_model.fit(base_train_df)\n\nbias_meta_preds = bias_model.predict_df(meta_train_df)\nprint(\"Bias model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), bias_meta_preds))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.2 ItemKNNModel\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class ItemKNNModel:\n    \"\"\"Item-based k-NN collaborative filtering model.\n\n    Uses cosine similarity between item rating vectors and a similarity-\n    weighted average over neighbours.\n    \"\"\"\n\n    def __init__(self, k: int = 40, default_rating: float = 3.5) -> None:\n        self.k = k\n        self.default_rating = float(default_rating)\n\n        self.user_id_to_index: Dict[int, int] = {}\n        self.item_id_to_index: Dict[int, int] = {}\n        self.R: np.ndarray | None = None\n        self.item_sim: np.ndarray | None = None\n\n    def fit(self, df: pd.DataFrame) -> None:\n        \"\"\"Fit the k-NN model from a ratings DataFrame.\n\n        Args:\n            df: DataFrame with `userId`, `movieId`, `rating`.\n        \"\"\"\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        unique_users = df[\"userId\"].unique()\n        unique_items = df[\"movieId\"].unique()\n\n        self.user_id_to_index = {uid: idx for idx, uid in enumerate(unique_users)}\n        self.item_id_to_index = {iid: idx for idx, iid in enumerate(unique_items)}\n\n        n_users = len(unique_users)\n        n_items = len(unique_items)\n\n        R = np.zeros((n_users, n_items), dtype=np.float32)\n        for row in df.itertuples(index=False):\n            u_idx = self.user_id_to_index[row.userId]\n            i_idx = self.item_id_to_index[row.movieId]\n            R[u_idx, i_idx] = row.rating\n\n        self.R = R\n        self.item_sim = cosine_similarity(R.T)\n\n    def _predict_single(self, user_id: int, movie_id: int) -> float:\n        if self.R is None or self.item_sim is None:\n            raise RuntimeError(\"Model has not been fitted.\")\n\n        u_idx = self.user_id_to_index.get(user_id)\n        i_idx = self.item_id_to_index.get(movie_id)\n        if u_idx is None or i_idx is None:\n            return self.default_rating\n\n        user_ratings = self.R[u_idx, :]\n        sims = self.item_sim[i_idx, :]\n\n        rated_mask = user_ratings > 0\n        rated_indices = np.where(rated_mask)[0]\n        if rated_indices.size == 0:\n            return self.default_rating\n\n        sims_rated = sims[rated_indices]\n        ratings_rated = user_ratings[rated_indices]\n\n        k_use = min(self.k, rated_indices.size)\n        top_idx = np.argsort(sims_rated)[-k_use:]\n\n        neigh_sims = sims_rated[top_idx]\n        neigh_ratings = ratings_rated[top_idx]\n\n        if np.all(neigh_sims == 0):\n            return float(neigh_ratings.mean())\n\n        pred = float(np.dot(neigh_sims, neigh_ratings) / np.sum(np.abs(neigh_sims)))\n        return pred\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self._predict_single(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nbias_global_mean = float(base_train_df[\"rating\"].mean())\n\nitem_knn_model = ItemKNNModel(k=40, default_rating=bias_global_mean)\nitem_knn_model.fit(base_train_df)\n\nitem_meta_preds = item_knn_model.predict_df(meta_train_df)\nprint(\"Item-kNN model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), item_meta_preds))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.3 MatrixFactorizationModel\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "@dataclass\nclass MFConfig:\n    n_factors: int = 30\n    n_epochs: int = 12\n    lr: float = 0.01\n    reg: float = 0.05\n\n\nclass MatrixFactorizationModel:\n    \"\"\"Matrix factorisation with biases trained via SGD.\n\n    Predicts ratings using a global mean, user/item biases and latent\n    factors for users and items.\n    \"\"\"\n\n    def __init__(self, config: MFConfig, random_state: int = 42) -> None:\n        self.config = config\n        self.random_state = random_state\n\n        self.mu: float | None = None\n        self.user_bias: Dict[int, float] = {}\n        self.item_bias: Dict[int, float] = {}\n        self.P: Dict[int, np.ndarray] = {}\n        self.Q: Dict[int, np.ndarray] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        rng = np.random.default_rng(self.random_state)\n\n        user_ids = df[\"userId\"].unique()\n        item_ids = df[\"movieId\"].unique()\n\n        self.mu = float(df[\"rating\"].mean())\n\n        self.user_bias = {u: 0.0 for u in user_ids}\n        self.item_bias = {i: 0.0 for i in item_ids}\n\n        k = self.config.n_factors\n        self.P = {u: 0.1 * rng.standard_normal(k) for u in user_ids}\n        self.Q = {i: 0.1 * rng.standard_normal(k) for i in item_ids}\n\n        lr = self.config.lr\n        reg = self.config.reg\n\n        user_arr = df[\"userId\"].to_numpy()\n        item_arr = df[\"movieId\"].to_numpy()\n        rating_arr = df[\"rating\"].to_numpy()\n\n        n_obs = len(df)\n\n        for epoch in range(self.config.n_epochs):\n            idx = rng.permutation(n_obs)\n            se = 0.0\n            for t in idx:\n                u = int(user_arr[t])\n                i = int(item_arr[t])\n                r_ui = float(rating_arr[t])\n\n                bu = self.user_bias[u]\n                bi = self.item_bias[i]\n                pu = self.P[u]\n                qi = self.Q[i]\n\n                pred = self.mu + bu + bi + float(np.dot(pu, qi))\n                err = r_ui - pred\n                se += err * err\n\n                # Bias updates\n                self.user_bias[u] = bu + lr * (err - reg * bu)\n                self.item_bias[i] = bi + lr * (err - reg * bi)\n\n                # Latent factors updates\n                pu_new = pu + lr * (err * qi - reg * pu)\n                qi_new = qi + lr * (err * pu - reg * qi)\n\n                self.P[u] = pu_new\n                self.Q[i] = qi_new\n\n            train_rmse = float(np.sqrt(se / n_obs))\n            print(f\"Epoch {epoch+1}/{self.config.n_epochs} - train RMSE: {train_rmse:.4f}\")\n\n    def predict_single(self, user_id: int, movie_id: int) -> float:\n        if self.mu is None:\n            raise RuntimeError(\"Model has not been fitted.\")\n\n        bu = self.user_bias.get(user_id)\n        bi = self.item_bias.get(movie_id)\n        pu = self.P.get(user_id)\n        qi = self.Q.get(movie_id)\n\n        if bu is None or bi is None or pu is None or qi is None:\n            return float(self.mu)\n\n        return float(self.mu + bu + bi + float(np.dot(pu, qi)))\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self.predict_single(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nmf_config = MFConfig(n_factors=30, n_epochs=10, lr=0.01, reg=0.05)\nmf_model = MatrixFactorizationModel(config=mf_config, random_state=RANDOM_STATE)\n\nmf_model.fit(base_train_df)\n\nmf_meta_preds = mf_model.predict_df(meta_train_df)\nprint(\"MF model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), mf_meta_preds))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We now have three base models with predictions on `meta_train_df`.\nNext we define relevance and ranking metrics, and build a logistic\nstacking model for ranking.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Relevance definition and ranking metrics\n\nWe transform explicit ratings into a binary **relevance** signal:\n\n- relevant if `rating \u2265 REL_THRESHOLD`.\n- not relevant otherwise.\n\nThen we compute per-user ranking metrics (hit-rate@K, precision@K,\nrecall@K) by ranking the user's items according to model scores.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "REL_THRESHOLD: float = 4.0\nK_EVAL: int = 10\n\n\ndef get_user_relevant_items(df: pd.DataFrame, user_id: int, threshold: float = REL_THRESHOLD) -> Set[int]:\n    \"\"\"Return set of relevant movieIds for a user.\n\n    Args:\n        df: Ratings DataFrame.\n        user_id: User identifier.\n        threshold: Rating threshold to consider relevant.\n\n    Returns:\n        Set of movieIds.\n    \"\"\"\n    mask = (df[\"userId\"] == user_id) & (df[\"rating\"] >= threshold)\n    return set(df.loc[mask, \"movieId\"].unique())\n\n\ndef ranking_metrics_for_model_on_test(\n    test_df: pd.DataFrame,\n    scores: np.ndarray,\n    k: int = K_EVAL,\n    threshold: float = REL_THRESHOLD,\n) -> Dict[str, float]:\n    \"\"\"Compute hit-rate, precision@K, recall@K from per-row scores.\n\n    Args:\n        test_df: Test ratings DataFrame.\n        scores: Score array aligned with `test_df` rows.\n        k: Cutoff for top-K.\n        threshold: Rating threshold for relevance.\n\n    Returns:\n        Dict with hit_rate, precision_at_k, recall_at_k, n_eval_users.\n    \"\"\"\n    df_scores = test_df.copy()\n    df_scores[\"score\"] = scores\n\n    users = df_scores[\"userId\"].unique()\n\n    hits = 0\n    sum_precision = 0.0\n    sum_recall = 0.0\n    n_eval_users = 0\n\n    for u in users:\n        user_rows = df_scores[df_scores[\"userId\"] == u]\n        relevant_items = get_user_relevant_items(test_df, u, threshold=threshold)\n        if not relevant_items:\n            continue  # skip users without positives in test\n\n        n_eval_users += 1\n\n        user_rows_sorted = user_rows.sort_values(\"score\", ascending=False)\n        top_k = user_rows_sorted.head(k)\n\n        recommended_items = set(top_k[\"movieId\"].tolist())\n        n_relevant_in_top = len(recommended_items & relevant_items)\n\n        if n_relevant_in_top > 0:\n            hits += 1\n\n        precision_u = n_relevant_in_top / min(k, len(user_rows_sorted))\n        recall_u = n_relevant_in_top / len(relevant_items)\n\n        sum_precision += precision_u\n        sum_recall += recall_u\n\n    if n_eval_users == 0:\n        raise ValueError(\"No users with relevant items in test for ranking evaluation.\")\n\n    hit_rate = hits / n_eval_users\n    precision_at_k = sum_precision / n_eval_users\n    recall_at_k = sum_recall / n_eval_users\n\n    return {\n        \"hit_rate\": hit_rate,\n        \"precision_at_k\": precision_at_k,\n        \"recall_at_k\": recall_at_k,\n        \"n_eval_users\": float(n_eval_users),\n    }\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Base models as rankers on test set\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "y_test = test_df[\"rating\"].to_numpy()\n\nbias_test_scores = bias_model.predict_df(test_df)\nitem_test_scores = item_knn_model.predict_df(test_df)\nmf_test_scores = mf_model.predict_df(test_df)\n\nmetrics_bias = ranking_metrics_for_model_on_test(test_df, bias_test_scores, k=K_EVAL)\nmetrics_item = ranking_metrics_for_model_on_test(test_df, item_test_scores, k=K_EVAL)\nmetrics_mf = ranking_metrics_for_model_on_test(test_df, mf_test_scores, k=K_EVAL)\n\nmetrics_bias, metrics_item, metrics_mf\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Simple untrained average of scores\n\navg_test_scores = (bias_test_scores + item_test_scores + mf_test_scores) / 3.0\nmetrics_avg = ranking_metrics_for_model_on_test(test_df, avg_test_scores, k=K_EVAL)\nmetrics_avg\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "rows = []\nfor name, m in [\n    (\"BiasModel\", metrics_bias),\n    (\"ItemKNNModel\", metrics_item),\n    (\"MFModel\", metrics_mf),\n    (\"SimpleAverage\", metrics_avg),\n]:\n    rows.append(\n        {\n            \"model\": name,\n            \"hit_rate\": m[\"hit_rate\"],\n            \"precision_at_k\": m[\"precision_at_k\"],\n            \"recall_at_k\": m[\"recall_at_k\"],\n        }\n    )\n\nbase_ranking_df = pd.DataFrame(rows)\nbase_ranking_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "base_ranking_melt = base_ranking_df.melt(id_vars=\"model\", var_name=\"metric\", value_name=\"value\")\n\nsns.barplot(data=base_ranking_melt, x=\"metric\", y=\"value\", hue=\"model\")\nplt.ylim(0, 1)\nplt.ylabel(\"Score\")\nplt.title(f\"Base models \u2013 ranking metrics (K={K_EVAL}, threshold={REL_THRESHOLD})\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Stacked ensemble for ranking (logistic regression)\n\nWe now train a **logistic regression** meta-model that learns to map\nbase scores to a probability of relevance.\n\nFor each row in `meta_train_df` we have:\n\n- Base features: `[bias_score, item_knn_score, mf_score]`.\n- Label: `1` if `rating \u2265 REL_THRESHOLD`, else `0`.\n\nThe meta-model learns to combine base scores in a way that better\nseparates relevant from non-relevant items.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Build meta-training features and labels for ranking\n\nX_meta_rank = np.vstack([\n    bias_meta_preds,\n    item_meta_preds,\n    mf_meta_preds,\n]).T\n\ny_meta_rank = (meta_train_df[\"rating\"].to_numpy() >= REL_THRESHOLD).astype(int)\n\nprint(\"X_meta_rank shape:\", X_meta_rank.shape)\nprint(\"Positive rate in meta labels:\", y_meta_rank.mean())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "meta_rank_model = LogisticRegression(\n    penalty=\"l2\",\n    C=1.0,\n    solver=\"lbfgs\",\n    max_iter=1000,\n    random_state=RANDOM_STATE,\n)\n\nmeta_rank_model.fit(X_meta_rank, y_meta_rank)\n\nprint(\"Meta-ranking coefficients (Bias, ItemKNN, MF):\", meta_rank_model.coef_)\nprint(\"Meta-ranking intercept:\", meta_rank_model.intercept_)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 6.1 Evaluate stacked ensemble on test set\n\nWe apply the logistic meta-model to the test set:\n\n1. Compute base scores on test (already done).\n2. Form `X_test_rank`.\n3. Predict `p(relevant)` using `predict_proba`.\n4. Rank items per user by this probability.\n5. Compute hit-rate@K, precision@K, recall@K.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "X_test_rank = np.vstack([\n    bias_test_scores,\n    item_test_scores,\n    mf_test_scores,\n]).T\n\nstacked_test_proba = meta_rank_model.predict_proba(X_test_rank)[:, 1]\n\nmetrics_stacked_rank = ranking_metrics_for_model_on_test(\n    test_df,\n    stacked_test_proba,\n    k=K_EVAL,\n    threshold=REL_THRESHOLD,\n)\n\nmetrics_stacked_rank\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "rows_full = rows.copy()\nrows_full.append(\n    {\n        \"model\": \"StackedLogistic\",\n        \"hit_rate\": metrics_stacked_rank[\"hit_rate\"],\n        \"precision_at_k\": metrics_stacked_rank[\"precision_at_k\"],\n        \"recall_at_k\": metrics_stacked_rank[\"recall_at_k\"],\n    }\n)\n\nranking_compare_df = pd.DataFrame(rows_full)\nranking_compare_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "ranking_compare_melt = ranking_compare_df.melt(id_vars=\"model\", var_name=\"metric\", value_name=\"value\")\n\nsns.barplot(data=ranking_compare_melt, x=\"metric\", y=\"value\", hue=\"model\")\nplt.ylim(0, 1)\nplt.ylabel(\"Score\")\nplt.title(f\"Base models vs stacked ensemble \u2013 ranking (K={K_EVAL})\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Interpreting meta-ranking weights\n\nWe inspect logistic regression coefficients as **learned weights** over\nbase scores.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "coef_names = [\"BiasModel\", \"ItemKNNModel\", \"MFModel\"]\ncoef_values = meta_rank_model.coef_[0]\n\ncoef_rank_df = pd.DataFrame({\"base_model\": coef_names, \"weight\": coef_values})\ncoef_rank_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "sns.barplot(data=coef_rank_df, x=\"base_model\", y=\"weight\")\nplt.title(\"Meta-ranking weights (logistic regression coefficients)\")\nplt.ylabel(\"Coefficient\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Interpretation:\n\n- Larger positive coefficient \u2192 that base model's score strongly\n  increases the probability of relevance.\n- Near-zero coefficient \u2192 little contribution.\n- Negative coefficient \u2192 meta-model uses that score mostly as a\n  corrective signal.\n\nBecause base scores are rating-like (~0.5\u20135.0), coefficients correspond\nroughly to how much a one-point change in a base score changes the log\nodds of relevance.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Design rationale\n\n### 8.1 Why treat ranking as classification over base scores?\n\nRanking metrics (precision@K, recall@K) are non-differentiable.\nOptimising them directly is hard. Instead, we:\n\n- Train base models on ratings as usual.\n- Use their scores as features in a **probabilistic classifier**.\n- Let the classifier learn to map scores \u2192 probability of relevance.\n\nWhen we rank by these probabilities, we implicitly optimise for\nseparating positives from negatives, which aligns well with ranking\nmetrics.\n\n### 8.2 Why logistic regression?\n\n- It is simple and interpretable.\n- Works well with unbalanced labels (few relevant items).\n- Produces probabilistic scores.\n- Coefficients tell us how much each base model matters.\n\n### 8.3 Why a single global meta-model?\n\nWe train a **global model** across all users and items. This is a good\nstarting point and often performs well. More complex designs include:\n\n- Adding user features (e.g. user activity, segments).\n- Adding item features (e.g. popularity, genre-based scores).\n- Using different meta-models per user cluster.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Extensions\n\n1. **More base models**\n   - Add LightFM, Surprise SVD, content-based models.\n   - Use their scores as additional columns in `X_meta_rank`.\n\n2. **Cross-validated stacking**\n   - Generate out-of-fold base scores for a more robust meta-training\n     dataset.\n\n3. **Alternative meta-learners**\n   - Gradient boosting or random forests over base scores.\n\n4. **User-level calibration**\n   - Learn user-specific or segment-specific meta-models if you suspect\n     behaviour differs strongly across groups.\n\nThis notebook gives you a full-ranking ensemble pipeline, parallel to\nthe RMSE-focused stacking notebook but aligned with top-N recommendation\nmetrics.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}