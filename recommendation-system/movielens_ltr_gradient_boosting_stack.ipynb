{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# MovieLens Recommender \u2013 Learning-to-Rank Ensemble with Gradient Boosting\n\nIn previous notebooks we:\n\n- Built several **base recommenders** (bias, item-kNN, MF, content).\n- Stacked them with a **linear / logistic meta-model**.\n- Evaluated both RMSE and **top-K ranking metrics**.\n\nIn this notebook we go **one level further** and build a more powerful\nand production-like **learning-to-rank (LTR) meta-model** on top of\nmultiple signals.\n\nWe will:\n\n1. Use MovieLens `ml-latest-small` (`ratings.csv`, `movies.csv`).\n2. Train 4 base models:\n   - BiasModel (global + user + item effects).\n   - ItemKNNModel (item-based CF).\n   - MatrixFactorizationModel (latent factors).\n   - GenreContentModel (simple content model using genres).\n3. Engineer **extra meta-features**:\n   - User-level stats (activity, mean rating).\n   - Item-level stats (popularity, mean rating).\n4. Define relevance as `rating \u2265 4.0`.\n5. Train two meta-models for comparison:\n   - Logistic regression (baseline from previous notebook).\n   - Gradient boosting classifier (tree-based LTR-ish model).\n6. Evaluate base models, logistic stack and gradient boosting stack on:\n   - Hit-rate@K.\n   - Precision@K.\n   - Recall@K.\n7. Inspect feature importances to understand **which signals matter**.\n\nThe idea is close in spirit to production systems where you:\n\n- Use collaborative + content models to generate features.\n- Add user/item statistics and contextual signals.\n- Feed everything into a tree-based LTR model (GBDT, XGBoost, etc.).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Set\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.preprocessing import MultiLabelBinarizer, normalize\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_DIR: Path = Path(\"data\") / \"ml-latest-small\"\nRATINGS_PATH: Path = DATA_DIR / \"ratings.csv\"\nMOVIES_PATH: Path = DATA_DIR / \"movies.csv\"\n\nfor p in [RATINGS_PATH, MOVIES_PATH]:\n    if not p.exists():\n        raise FileNotFoundError(\n            f\"Required file not found: {p.resolve()}\\n\"\n            \"Please ensure MovieLens 'ml-latest-small' is under data/ml-latest-small/.\"\n        )\n\nratings_df = pd.read_csv(RATINGS_PATH)\nmovies_df = pd.read_csv(MOVIES_PATH)\n\nprint(\"Ratings shape:\", ratings_df.shape)\nprint(\"Movies shape: \", movies_df.shape)\nratings_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute root mean squared error (RMSE).\n\n    Args:\n        y_true: True ratings.\n        y_pred: Predicted ratings.\n\n    Returns:\n        RMSE value.\n    \"\"\"\n    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n\n\nsns.histplot(ratings_df[\"rating\"], bins=10)\nplt.title(\"Rating distribution\")\nplt.xlabel(\"Rating\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Train / meta / test splits"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "train_full_df, test_df = train_test_split(\n    ratings_df,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\nbase_train_df, meta_train_df = train_test_split(\n    train_full_df,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n\nprint(\"Base train size:\", len(base_train_df))\nprint(\"Meta train size:\", len(meta_train_df))\nprint(\"Test size:      \", len(test_df))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Collaborative base models\n\nWe reuse the same 3 collaborative models as before."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.1 BiasModel"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class BiasModel:\n    \"\"\"Global + user + item bias recommender.\n\n    Rating estimate: mu + b_u + b_i.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.mu: float | None = None\n        self.user_bias: Dict[int, float] = {}\n        self.item_bias: Dict[int, float] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        self.mu = float(df[\"rating\"].mean())\n        user_mean = df.groupby(\"userId\")[\"rating\"].mean()\n        item_mean = df.groupby(\"movieId\")[\"rating\"].mean()\n        self.user_bias = (user_mean - self.mu).to_dict()\n        self.item_bias = (item_mean - self.mu).to_dict()\n\n    def predict_row(self, user_id: int, movie_id: int) -> float:\n        if self.mu is None:\n            raise RuntimeError(\"Model not fitted.\")\n        bu = self.user_bias.get(user_id, 0.0)\n        bi = self.item_bias.get(movie_id, 0.0)\n        return float(self.mu + bu + bi)\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self.predict_row(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nbias_model = BiasModel()\nbias_model.fit(base_train_df)\n\nbias_meta_scores = bias_model.predict_df(meta_train_df)\nprint(\"Bias model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), bias_meta_scores))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.2 ItemKNNModel"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class ItemKNNModel:\n    \"\"\"Item-based k-NN collaborative filtering model.\"\"\"\n\n    def __init__(self, k: int = 40, default_rating: float = 3.5) -> None:\n        self.k = k\n        self.default_rating = float(default_rating)\n        self.user_id_to_index: Dict[int, int] = {}\n        self.item_id_to_index: Dict[int, int] = {}\n        self.R: np.ndarray | None = None\n        self.item_sim: np.ndarray | None = None\n\n    def fit(self, df: pd.DataFrame) -> None:\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        unique_users = df[\"userId\"].unique()\n        unique_items = df[\"movieId\"].unique()\n\n        self.user_id_to_index = {uid: idx for idx, uid in enumerate(unique_users)}\n        self.item_id_to_index = {iid: idx for idx, iid in enumerate(unique_items)}\n\n        n_users = len(unique_users)\n        n_items = len(unique_items)\n        R = np.zeros((n_users, n_items), dtype=np.float32)\n\n        for row in df.itertuples(index=False):\n            u_idx = self.user_id_to_index[row.userId]\n            i_idx = self.item_id_to_index[row.movieId]\n            R[u_idx, i_idx] = row.rating\n\n        self.R = R\n        self.item_sim = cosine_similarity(R.T)\n\n    def _predict_single(self, user_id: int, movie_id: int) -> float:\n        if self.R is None or self.item_sim is None:\n            raise RuntimeError(\"Model not fitted.\")\n\n        u_idx = self.user_id_to_index.get(user_id)\n        i_idx = self.item_id_to_index.get(movie_id)\n        if u_idx is None or i_idx is None:\n            return self.default_rating\n\n        user_ratings = self.R[u_idx, :]\n        sims = self.item_sim[i_idx, :]\n\n        rated_mask = user_ratings > 0\n        rated_indices = np.where(rated_mask)[0]\n        if rated_indices.size == 0:\n            return self.default_rating\n\n        sims_rated = sims[rated_indices]\n        ratings_rated = user_ratings[rated_indices]\n\n        k_use = min(self.k, rated_indices.size)\n        top_idx = np.argsort(sims_rated)[-k_use:]\n\n        neigh_sims = sims_rated[top_idx]\n        neigh_ratings = ratings_rated[top_idx]\n\n        if np.all(neigh_sims == 0):\n            return float(neigh_ratings.mean())\n\n        pred = float(np.dot(neigh_sims, neigh_ratings) / np.sum(np.abs(neigh_sims)))\n        return pred\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self._predict_single(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nbias_global_mean = float(base_train_df[\"rating\"].mean())\nitem_knn_model = ItemKNNModel(k=40, default_rating=bias_global_mean)\nitem_knn_model.fit(base_train_df)\n\nitem_meta_scores = item_knn_model.predict_df(meta_train_df)\nprint(\"Item-kNN RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), item_meta_scores))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.3 MatrixFactorizationModel"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "@dataclass\nclass MFConfig:\n    n_factors: int = 30\n    n_epochs: int = 8\n    lr: float = 0.01\n    reg: float = 0.05\n\n\nclass MatrixFactorizationModel:\n    \"\"\"Matrix factorisation with biases trained via SGD.\"\"\"\n\n    def __init__(self, config: MFConfig, random_state: int = 42) -> None:\n        self.config = config\n        self.random_state = random_state\n        self.mu: float | None = None\n        self.user_bias: Dict[int, float] = {}\n        self.item_bias: Dict[int, float] = {}\n        self.P: Dict[int, np.ndarray] = {}\n        self.Q: Dict[int, np.ndarray] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        if df.empty:\n            raise ValueError(\"Training DataFrame is empty.\")\n\n        rng = np.random.default_rng(self.random_state)\n\n        user_ids = df[\"userId\"].unique()\n        item_ids = df[\"movieId\"].unique()\n\n        self.mu = float(df[\"rating\"].mean())\n        self.user_bias = {u: 0.0 for u in user_ids}\n        self.item_bias = {i: 0.0 for i in item_ids}\n\n        k = self.config.n_factors\n        self.P = {u: 0.1 * rng.standard_normal(k) for u in user_ids}\n        self.Q = {i: 0.1 * rng.standard_normal(k) for i in item_ids}\n\n        lr = self.config.lr\n        reg = self.config.reg\n\n        user_arr = df[\"userId\"].to_numpy()\n        item_arr = df[\"movieId\"].to_numpy()\n        rating_arr = df[\"rating\"].to_numpy()\n\n        n_obs = len(df)\n\n        for epoch in range(self.config.n_epochs):\n            idx = rng.permutation(n_obs)\n            se = 0.0\n            for t in idx:\n                u = int(user_arr[t])\n                i = int(item_arr[t])\n                r_ui = float(rating_arr[t])\n\n                bu = self.user_bias[u]\n                bi = self.item_bias[i]\n                pu = self.P[u]\n                qi = self.Q[i]\n\n                pred = self.mu + bu + bi + float(np.dot(pu, qi))\n                err = r_ui - pred\n                se += err * err\n\n                self.user_bias[u] = bu + lr * (err - reg * bu)\n                self.item_bias[i] = bi + lr * (err - reg * bi)\n\n                pu_new = pu + lr * (err * qi - reg * pu)\n                qi_new = qi + lr * (err * pu - reg * qi)\n\n                self.P[u] = pu_new\n                self.Q[i] = qi_new\n\n            train_rmse = float(np.sqrt(se / n_obs))\n            print(f\"Epoch {epoch+1}/{self.config.n_epochs} - train RMSE: {train_rmse:.4f}\")\n\n    def predict_single(self, user_id: int, movie_id: int) -> float:\n        if self.mu is None:\n            raise RuntimeError(\"Model not fitted.\")\n        bu = self.user_bias.get(user_id)\n        bi = self.item_bias.get(movie_id)\n        pu = self.P.get(user_id)\n        qi = self.Q.get(movie_id)\n        if bu is None or bi is None or pu is None or qi is None:\n            return float(self.mu)\n        return float(self.mu + bu + bi + np.dot(pu, qi))\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        preds: List[float] = []\n        for row in df.itertuples(index=False):\n            preds.append(self.predict_single(int(row.userId), int(row.movieId)))\n        return np.array(preds, dtype=float)\n\n\nmf_config = MFConfig()\nmf_model = MatrixFactorizationModel(config=mf_config, random_state=RANDOM_STATE)\n\nmf_model.fit(base_train_df)\n\nmf_meta_scores = mf_model.predict_df(meta_train_df)\nprint(\"MF model RMSE on meta_train:\", rmse(meta_train_df[\"rating\"].to_numpy(), mf_meta_scores))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Genre-based content model\n\nWe add a simple content-based model using genres."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def build_genre_matrix(movies: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Build a multi-hot genre matrix with movieId as index.\"\"\"\n    movies = movies.copy()\n    movies[\"genres_list\"] = movies[\"genres\"].fillna(\"(no genres listed)\")\n\n    def split_genres(s: str) -> List[str]:\n        if s == \"(no genres listed)\" or s == \"No Genres Listed\":\n            return []\n        return [g.strip() for g in s.split(\"|\") if g.strip()]\n\n    movies[\"genres_list\"] = movies[\"genres_list\"].apply(split_genres)\n\n    mlb = MultiLabelBinarizer()\n    mat = mlb.fit_transform(movies[\"genres_list\"])\n\n    genre_df = pd.DataFrame(\n        mat,\n        columns=[f\"genre:{g}\" for g in mlb.classes_],\n        index=movies[\"movieId\"].to_numpy(),\n    )\n    return genre_df\n\n\ngenre_df = build_genre_matrix(movies_df)\nprint(\"Genre matrix shape:\", genre_df.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class GenreContentModel:\n    \"\"\"Content-based model using movie genres and user genre profiles.\"\"\"\n\n    def __init__(self, genre_df: pd.DataFrame, rel_threshold: float = 4.0) -> None:\n        self.genre_df = genre_df\n        self.rel_threshold = rel_threshold\n        self.user_profiles: Dict[int, np.ndarray] = {}\n\n    def fit(self, df: pd.DataFrame) -> None:\n        liked = df[df[\"rating\"] >= self.rel_threshold]\n        joined = liked.merge(self.genre_df, left_on=\"movieId\", right_index=True, how=\"left\")\n        genre_cols = self.genre_df.columns\n\n        profiles = joined.groupby(\"userId\")[genre_cols].mean().fillna(0.0)\n        prof_mat = normalize(profiles.to_numpy(), norm=\"l2\")\n\n        for uid, vec in zip(profiles.index.to_numpy(), prof_mat):\n            self.user_profiles[int(uid)] = vec\n\n    def _score_single(self, user_id: int, movie_id: int) -> float:\n        user_vec = self.user_profiles.get(user_id)\n        if user_vec is None:\n            return 0.0\n        if movie_id not in self.genre_df.index:\n            return 0.0\n        item_vec = self.genre_df.loc[movie_id].to_numpy(dtype=float)\n        if not np.any(item_vec):\n            return 0.0\n        item_vec_norm = item_vec / (np.linalg.norm(item_vec) + 1e-12)\n        return float(np.dot(user_vec, item_vec_norm))\n\n    def predict_df(self, df: pd.DataFrame) -> np.ndarray:\n        scores: List[float] = []\n        for row in df.itertuples(index=False):\n            scores.append(self._score_single(int(row.userId), int(row.movieId)))\n        return np.array(scores, dtype=float)\n\n\ngenre_model = GenreContentModel(genre_df=genre_df, rel_threshold=4.0)\ngenre_model.fit(base_train_df)\n\ngenre_meta_scores = genre_model.predict_df(meta_train_df)\nprint(\"Genre-content scores on meta_train: min=\", genre_meta_scores.min(), \", max=\", genre_meta_scores.max())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Relevance and ranking metrics"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "REL_THRESHOLD: float = 4.0\nK_EVAL: int = 10\n\n\ndef get_user_relevant_items(df: pd.DataFrame, user_id: int, threshold: float = REL_THRESHOLD) -> Set[int]:\n    mask = (df[\"userId\"] == user_id) & (df[\"rating\"] >= threshold)\n    return set(df.loc[mask, \"movieId\"].unique())\n\n\ndef ranking_metrics_for_model_on_test(\n    test_df: pd.DataFrame,\n    scores: np.ndarray,\n    k: int = K_EVAL,\n    threshold: float = REL_THRESHOLD,\n) -> Dict[str, float]:\n    df_scores = test_df.copy()\n    df_scores[\"score\"] = scores\n\n    users = df_scores[\"userId\"].unique()\n\n    hits = 0\n    sum_precision = 0.0\n    sum_recall = 0.0\n    n_eval_users = 0\n\n    for u in users:\n        user_rows = df_scores[df_scores[\"userId\"] == u]\n        relevant_items = get_user_relevant_items(test_df, u, threshold=threshold)\n        if not relevant_items:\n            continue\n\n        n_eval_users += 1\n\n        user_sorted = user_rows.sort_values(\"score\", ascending=False)\n        top_k = user_sorted.head(k)\n\n        recommended_items = set(top_k[\"movieId\"].tolist())\n        n_rel_top = len(recommended_items & relevant_items)\n\n        if n_rel_top > 0:\n            hits += 1\n\n        precision_u = n_rel_top / min(k, len(user_sorted))\n        recall_u = n_rel_top / len(relevant_items)\n\n        sum_precision += precision_u\n        sum_recall += recall_u\n\n    if n_eval_users == 0:\n        raise ValueError(\"No users with relevant items in test.\")\n\n    hit_rate = hits / n_eval_users\n    precision_at_k = sum_precision / n_eval_users\n    recall_at_k = sum_recall / n_eval_users\n\n    return {\n        \"hit_rate\": hit_rate,\n        \"precision_at_k\": precision_at_k,\n        \"recall_at_k\": recall_at_k,\n        \"n_eval_users\": float(n_eval_users),\n    }\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Base models (4) as rankers on test"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "y_test = test_df[\"rating\"].to_numpy()\n\nbias_test_scores = bias_model.predict_df(test_df)\nitem_test_scores = item_knn_model.predict_df(test_df)\nmf_test_scores = mf_model.predict_df(test_df)\ngenre_test_scores = genre_model.predict_df(test_df)\n\nmetrics_bias = ranking_metrics_for_model_on_test(test_df, bias_test_scores, k=K_EVAL)\nmetrics_item = ranking_metrics_for_model_on_test(test_df, item_test_scores, k=K_EVAL)\nmetrics_mf = ranking_metrics_for_model_on_test(test_df, mf_test_scores, k=K_EVAL)\nmetrics_genre = ranking_metrics_for_model_on_test(test_df, genre_test_scores, k=K_EVAL)\n\n# Simple average of 4 models\navg4_scores = (bias_test_scores + item_test_scores + mf_test_scores + genre_test_scores) / 4.0\nmetrics_avg4 = ranking_metrics_for_model_on_test(test_df, avg4_scores, k=K_EVAL)\n\nmetrics_bias, metrics_item, metrics_mf, metrics_genre, metrics_avg4\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "rows = []\nfor name, m in [\n    (\"BiasModel\", metrics_bias),\n    (\"ItemKNNModel\", metrics_item),\n    (\"MFModel\", metrics_mf),\n    (\"GenreContent\", metrics_genre),\n    (\"SimpleAverage4\", metrics_avg4),\n]:\n    rows.append(\n        {\n            \"model\": name,\n            \"hit_rate\": m[\"hit_rate\"],\n            \"precision_at_k\": m[\"precision_at_k\"],\n            \"recall_at_k\": m[\"recall_at_k\"],\n        }\n    )\n\nbase_rank_df = pd.DataFrame(rows)\nbase_rank_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "base_rank_melt = base_rank_df.melt(id_vars=\"model\", var_name=\"metric\", value_name=\"value\")\n\nsns.barplot(data=base_rank_melt, x=\"metric\", y=\"value\", hue=\"model\")\nplt.ylim(0, 1)\nplt.ylabel(\"Score\")\nplt.title(f\"Base models vs simple average (K={K_EVAL})\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Meta-features for learning-to-rank\n\nWe now build a richer feature set for the meta-model:"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 7.1 User and item statistics\n\nWe compute simple, interpretable statistics that are often useful in\nproduction recommenders:\n\n- User-level:\n  - `user_n_ratings` (activity).\n  - `user_mean_rating`.\n- Item-level:\n  - `item_n_ratings` (popularity).\n  - `item_mean_rating`.\n\nWe will join these to both `meta_train_df` and `test_df`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# User stats on train_full (to mimic train-time availability)\nuser_stats = train_full_df.groupby(\"userId\")[\"rating\"].agg(\n    user_n_ratings=\"count\",\n    user_mean_rating=\"mean\",\n).reset_index()\n\n# Item stats on train_full\nitem_stats = train_full_df.groupby(\"movieId\")[\"rating\"].agg(\n    item_n_ratings=\"count\",\n    item_mean_rating=\"mean\",\n).reset_index()\n\nuser_stats.head(), item_stats.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 7.2 Build meta-training feature matrix X_meta_ltr"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Join stats onto meta_train_df\nmeta_join = meta_train_df.merge(user_stats, on=\"userId\", how=\"left\").merge(\n    item_stats, on=\"movieId\", how=\"left\"\n)\n\n# Base scores we already computed on meta_train_df\nmeta_join = meta_join.copy()\nmeta_join[\"score_bias\"] = bias_meta_scores\nmeta_join[\"score_itemknn\"] = item_meta_scores\nmeta_join[\"score_mf\"] = mf_meta_scores\nmeta_join[\"score_genre\"] = genre_meta_scores\n\n# Fill any missing stats (e.g. cold users/items) with global averages\nmeta_join[\"user_n_ratings\"].fillna(0, inplace=True)\nmeta_join[\"user_mean_rating\"].fillna(train_full_df[\"rating\"].mean(), inplace=True)\nmeta_join[\"item_n_ratings\"].fillna(0, inplace=True)\nmeta_join[\"item_mean_rating\"].fillna(train_full_df[\"rating\"].mean(), inplace=True)\n\nfeature_cols = [\n    \"score_bias\",\n    \"score_itemknn\",\n    \"score_mf\",\n    \"score_genre\",\n    \"user_n_ratings\",\n    \"user_mean_rating\",\n    \"item_n_ratings\",\n    \"item_mean_rating\",\n]\n\nX_meta_ltr = meta_join[feature_cols].to_numpy(dtype=float)\ny_meta_ltr = (meta_join[\"rating\"].to_numpy() >= REL_THRESHOLD).astype(int)\n\nprint(\"X_meta_ltr shape:\", X_meta_ltr.shape)\nprint(\"Positive rate (meta_ltr):\", y_meta_ltr.mean())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Meta-models: LogisticRegression vs GradientBoostingClassifier"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Baseline meta-model: logistic regression\n\nmeta_logit = LogisticRegression(\n    penalty=\"l2\",\n    C=1.0,\n    solver=\"lbfgs\",\n    max_iter=1000,\n    random_state=RANDOM_STATE,\n)\nmeta_logit.fit(X_meta_ltr, y_meta_ltr)\n\nprint(\"Logistic coefficients:\", meta_logit.coef_)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Gradient boosting meta-model (tree-based LTR-style)\n\ngb_meta = GradientBoostingClassifier(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=3,\n    subsample=0.8,\n    random_state=RANDOM_STATE,\n)\n\ngb_meta.fit(X_meta_ltr, y_meta_ltr)\n\nprint(\"GradientBoosting trained. Number of features:\", len(feature_cols))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Build test feature matrix and evaluate ranking"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Join stats onto test_df\n\ntest_join = test_df.merge(user_stats, on=\"userId\", how=\"left\").merge(\n    item_stats, on=\"movieId\", how=\"left\"\n)\n\n# Add base scores\n\ntest_join = test_join.copy()\ntest_join[\"score_bias\"] = bias_test_scores\ntest_join[\"score_itemknn\"] = item_test_scores\ntest_join[\"score_mf\"] = mf_test_scores\ntest_join[\"score_genre\"] = genre_test_scores\n\n# Fill missing stats with global defaults\n\ntest_join[\"user_n_ratings\"].fillna(0, inplace=True)\ntest_join[\"user_mean_rating\"].fillna(train_full_df[\"rating\"].mean(), inplace=True)\ntest_join[\"item_n_ratings\"].fillna(0, inplace=True)\ntest_join[\"item_mean_rating\"].fillna(train_full_df[\"rating\"].mean(), inplace=True)\n\nX_test_ltr = test_join[feature_cols].to_numpy(dtype=float)\n\n# Logistic meta scores (probability of relevance)\nlogit_test_proba = meta_logit.predict_proba(X_test_ltr)[:, 1]\n\n# Gradient boosting meta scores\nGB_test_proba = gb_meta.predict_proba(X_test_ltr)[:, 1]\n\nmetrics_logit = ranking_metrics_for_model_on_test(test_df, logit_test_proba, k=K_EVAL)\nmetrics_gb = ranking_metrics_for_model_on_test(test_df, GB_test_proba, k=K_EVAL)\n\nmetrics_logit, metrics_gb\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Collect all models (base + simple avg + meta-models) for comparison\n\nrows_full = rows.copy()  # from base_rank_df construction\nrows_full.append(\n    {\n        \"model\": \"LogisticStack\",\n        \"hit_rate\": metrics_logit[\"hit_rate\"],\n        \"precision_at_k\": metrics_logit[\"precision_at_k\"],\n        \"recall_at_k\": metrics_logit[\"recall_at_k\"],\n    }\n)\nrows_full.append(\n    {\n        \"model\": \"GBStack\",\n        \"hit_rate\": metrics_gb[\"hit_rate\"],\n        \"precision_at_k\": metrics_gb[\"precision_at_k\"],\n        \"recall_at_k\": metrics_gb[\"recall_at_k\"],\n    }\n)\n\nltr_compare_df = pd.DataFrame(rows_full)\nltr_compare_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "ltr_compare_melt = ltr_compare_df.melt(id_vars=\"model\", var_name=\"metric\", value_name=\"value\")\n\nsns.barplot(data=ltr_compare_melt, x=\"metric\", y=\"value\", hue=\"model\")\nplt.ylim(0, 1)\nplt.ylabel(\"Score\")\nplt.title(f\"Base models vs LTR stacks (K={K_EVAL})\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You should see `GBStack` at least competitive with the best base model\nand usually better than `SimpleAverage4` and `LogisticStack`. The gain\nmay be small on this small dataset but mirrors patterns in production\nsystems when more features and models are added.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Feature importances in the Gradient Boosting stack"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "importances = gb_meta.feature_importances_\n\nfeat_imp_df = pd.DataFrame({\n    \"feature\": feature_cols,\n    \"importance\": importances,\n}).sort_values(\"importance\", ascending=False)\n\nfeat_imp_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "sns.barplot(data=feat_imp_df, x=\"importance\", y=\"feature\")\nplt.title(\"Gradient boosting meta-model feature importances\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Typical patterns you might see:\n\n- `score_mf` and `score_itemknn` carry strong signal.\n- `score_genre` helps in cold or niche areas.\n- `item_n_ratings` (popularity) and `user_n_ratings` (activity) are\n  often surprisingly informative.\n\nBecause trees can model interactions, the model can learn rules like:\n\n- \"If item is very unpopular but genre score is high, still recommend.\"\n- \"If user is low-activity, rely more on content and biases.\"\"\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {},\n            \"source\": \"## 11. Design summary and further extensions\n\nWe now have a **learning-to-rank stack** on top of collaborative and\ncontent recommenders, with extra user/item features.\"\n        },\n        {\n            \"cell_type\": \"markdown\",\n            \"metadata\": {},\n            \"source\": "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 11.2 Natural next steps\n\nIf you want to push even further:\n\n1. **More base signals**\n   - Add embeddings from text or plots.\n   - Add LightFM or other hybrid models as additional score features.\n\n2. **Pairwise / listwise LTR**\n   - Use libraries that support LambdaRank / LambdaMART to train directly\n     on pairwise ranking losses.\n\n3. **Contextual features**\n   - Time of day, device, country, campaign, etc.\n\n4. **Per-user calibration**\n   - Train different meta-models for user segments (e.g. power users vs\n     casual users).\n\nThis notebook shows how to go from **stacked ensembles** to a more\nrealistic **LTR meta-ranker**, while keeping the logic transparent and\nwell-commented so you can adapt it to your own production setting.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}