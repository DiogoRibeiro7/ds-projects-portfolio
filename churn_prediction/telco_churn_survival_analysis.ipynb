{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Telco Customer Churn \u2013 Survival Analysis (Time-to-Churn)\n\nThis notebook is an **extended survival analysis project** built on the IBM\n**Telco Customer Churn** dataset.\n\nInstead of only predicting **whether** a customer churns, we focus on\n**when** churn happens. We treat churn as a **time-to-event** problem and use\nsurvival analysis.\n\n---\n\n## Objectives\n\n1. Reframe churn as a **time-to-event** problem using `tenure` and `Churn`.\n2. Build and interpret **Kaplan\u2013Meier survival curves**:\n   - Overall survival.\n   - By contract type and other segments.\n   - Log-rank tests for differences between groups.\n3. Fit and interpret a **Cox Proportional Hazards model**:\n   - Feature engineering for survival.\n   - Hazard ratios and their business meaning.\n4. Check **model assumptions and diagnostics**:\n   - Proportional hazards assumption.\n   - Concordance index (discrimination).\n5. Create **scenario analyses**:\n   - Predicted survival curves for example customers.\n   - Survival by risk segments.\n6. (Optional) Explore an alternative **Aalen Additive** model.\n\nThis notebook is self-contained. It loads the Telco dataset directly\nfrom a CSV file.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration\n\nWe use:\n\n- `pandas`, `numpy` for data handling.\n- `matplotlib`, `seaborn` for visualisation.\n- `lifelines` for survival analysis:\n  - `KaplanMeierFitter`\n  - `CoxPHFitter`\n  - `AalenAdditiveFitter` (optional)\n  - log-rank tests and concordance index.\n\nWe also set a random seed and define the expected data path.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom lifelines import KaplanMeierFitter, CoxPHFitter, AalenAdditiveFitter\nfrom lifelines.statistics import logrank_test, multivariate_logrank_test\nfrom lifelines.utils import concordance_index\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_PATH: Path = Path(\"data\") / \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Data file not found at {DATA_PATH.resolve()}. \"\n        \"Please download the Telco churn CSV and place it under the 'data/' directory.\"\n    )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data loading and basic cleaning\n\nWe reuse the Telco dataset, but now we care especially about:\n\n- `tenure` \u2013 months the customer has stayed.\n- `Churn` \u2013 `Yes` if the customer churned, `No` if still active.\n\nWe do minimal but explicit cleaning:\n\n1. Convert `TotalCharges` to numeric.\n2. Drop rows with missing `TotalCharges`.\n3. Drop duplicate `customerID`s.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_telco_data(path: Path) -> pd.DataFrame:\n    \"\"\"Load the Telco customer churn dataset from CSV.\n\n    Args:\n        path: Path to the CSV file.\n\n    Returns:\n        DataFrame with Telco churn data.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the DataFrame is empty.\n    \"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path!s}\")\n\n    df: pd.DataFrame = pd.read_csv(path)\n    if df.empty:\n        raise ValueError(f\"Loaded DataFrame is empty: {path!s}\")\n    return df\n\n\ndef clean_telco_data(raw_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Clean Telco data: fix types, handle missing, drop duplicates.\n\n    Steps:\n    - Convert `TotalCharges` to numeric (invalid entries -> NaN).\n    - Drop rows with missing `TotalCharges`.\n    - Drop duplicate `customerID` rows.\n\n    Args:\n        raw_df: Raw Telco DataFrame.\n\n    Returns:\n        Cleaned DataFrame.\n    \"\"\"\n    df = raw_df.copy()\n\n    if \"TotalCharges\" not in df.columns:\n        raise ValueError(\"Expected 'TotalCharges' column not found.\")\n\n    # Convert TotalCharges to numeric\n    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n\n    # Show missing values\n    missing = df.isna().sum()\n    print(\"Missing values per column (non-zero only):\")\n    display(missing[missing > 0])\n\n    before_rows: int = df.shape[0]\n    df = df.dropna(subset=[\"TotalCharges\"])\n    after_rows: int = df.shape[0]\n    print(f\"Dropped {before_rows - after_rows} rows with missing TotalCharges.\")\n\n    # Drop duplicate customers\n    before_rows = df.shape[0]\n    df = df.drop_duplicates(subset=[\"customerID\"])\n    after_rows = df.shape[0]\n    print(f\"Dropped {before_rows - after_rows} duplicate customerID rows.\")\n\n    return df.reset_index(drop=True)\n\n\nraw_df: pd.DataFrame = load_telco_data(DATA_PATH)\ntelco_df: pd.DataFrame = clean_telco_data(raw_df)\n\ndisplay(telco_df.head())\ndisplay(telco_df[[\"tenure\", \"Churn\"]].describe(include=\"all\"))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Section summary\n\nWe now have a clean Telco dataset with valid `TotalCharges` and unique\n`customerID`s. The columns `tenure` and `Churn` are ready to be used as the\ncore survival variables.\n\nNext we construct the explicit **survival dataset**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Constructing the survival dataset\n\nFor survival analysis, we need at least two columns:\n\n- **Duration**: how long each subject is observed.\n  - Here: `tenure` (months since joining).\n- **Event indicator**: whether the event of interest happened.\n  - Here: `churn_event` = 1 if the customer churned (`Churn == \"Yes\"`),\n    0 otherwise (`No` \u2192 censored observation).\n\nWe will:\n\n1. Map `Churn` from `Yes` / `No` to `1` / `0`.\n2. Keep additional features for later modelling.\n3. Summarise the censoring structure.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Sanity check of essential columns\nrequired_cols: List[str] = [\"customerID\", \"tenure\", \"Churn\"]\nmissing_required = [c for c in required_cols if c not in telco_df.columns]\nif missing_required:\n    raise KeyError(f\"Missing required columns in Telco data: {missing_required}\")\n\nsurv_df = telco_df.copy()\n\n# Event indicator: 1 = churned, 0 = censored (still active)\nsurv_df[\"churn_event\"] = surv_df[\"Churn\"].map({\"No\": 0, \"Yes\": 1}).astype(int)\n\n# Basic summary of events vs censored\nevent_counts = surv_df[\"churn_event\"].value_counts().rename(index={0: \"censored\", 1: \"event\"})\nprint(\"Churn event vs censored counts:\")\nprint(event_counts)\n\nprint(\"\\nTenure distribution by churn_event:\")\ndisplay(surv_df.groupby(\"churn_event\")[\"tenure\"].describe())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Quick visual: tenure vs churn\n\nBefore jumping into Kaplan\u2013Meier, we quickly visualise the tenure distribution\nby churn status.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "sns.kdeplot(data=surv_df, x=\"tenure\", hue=\"churn_event\", common_norm=False, fill=True, alpha=0.4)\nplt.title(\"Tenure distribution by churn event (0=censored, 1=churn)\")\nplt.xlabel(\"Tenure (months)\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We see that churned customers typically have **shorter tenure**. Survival\nanalysis will quantify this pattern over time.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Global Kaplan\u2013Meier survival curve\n\nWe now estimate the **overall survival function** using the\n`KaplanMeierFitter`.\n\n- The curve starts at 1 (100% of customers at time 0).\n- It steps down at churn times.\n- At each tenure `t`, the curve value is:\n  > Estimated probability that a randomly chosen customer is still\n  > active (has not churned) by time `t`.\n\nWe also compute the **median survival time**.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "kmf = KaplanMeierFitter()\n\nT = surv_df[\"tenure\"]  # durations\nE = surv_df[\"churn_event\"]  # events\n\nkmf.fit(durations=T, event_observed=E, label=\"All customers\")\n\nkmf.plot_survival_function()\nplt.title(\"Kaplan\u2013Meier survival curve \u2013 All customers\")\nplt.xlabel(\"Tenure (months)\")\nplt.ylabel(\"Survival probability (still a customer)\")\nplt.show()\n\nprint(\"Median survival time (months):\", kmf.median_survival_time_)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Interpretation:**\n\n- The curve shows how quickly the customer base decays over time.\n- The median survival time (if defined) tells us roughly when **50% of\n  customers have churned**.\n\nNext, we compare survival curves across important **segments**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Survival by contract type (+ log-rank test)\n\nContract type is a key driver of churn. We compare survival curves for\nlevels of `Contract`:\n\n- `Month-to-month`\n- `One year`\n- `Two year`\n\nWe then use a **log-rank test** to statistically assess whether survival\ncurves differ.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "if \"Contract\" not in surv_df.columns:\n    raise KeyError(\"Expected 'Contract' column not found in data.\")\n\nkmf_contract = KaplanMeierFitter()\ncontract_types = surv_df[\"Contract\"].unique()\n\nplt.figure(figsize=(8, 5))\nfor c in contract_types:\n    mask = surv_df[\"Contract\"] == c\n    T_c = surv_df.loc[mask, \"tenure\"]\n    E_c = surv_df.loc[mask, \"churn_event\"]\n\n    kmf_contract.fit(T_c, event_observed=E_c, label=str(c))\n    kmf_contract.plot_survival_function()\n\nplt.title(\"Survival curves by contract type\")\nplt.xlabel(\"Tenure (months)\")\nplt.ylabel(\"Survival probability\")\nplt.legend(title=\"Contract\")\nplt.show()\n\n# Global log-rank / multivariate test across all contract groups\nmulti_lr = multivariate_logrank_test(\n    event_durations=surv_df[\"tenure\"],\n    groups=surv_df[\"Contract\"],\n    event_observed=surv_df[\"churn_event\"],\n)\nprint(\"Multivariate log-rank test across Contract groups:\")\nprint(multi_lr.summary)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Contract type usually shows **strongly different survival patterns**:\n\n- `Month-to-month` \u2192 steeper decline, shorter survival.\n- `One year`, `Two year` \u2192 flatter curves, longer retention.\n\nThe log-rank test p-value indicates whether these differences are\nstatistically significant (typically they are).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Survival by other segments\n\nWe briefly look at survival by:\n\n- `InternetService` (e.g. DSL vs Fiber optic vs No).\n- `PaymentMethod` (e.g. electronic check vs automatic methods).\n\nThis is mainly descriptive, to spot interesting patterns.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def plot_km_by_group(\n    df: pd.DataFrame,\n    group_col: str,\n    duration_col: str = \"tenure\",\n    event_col: str = \"churn_event\",\n    max_groups: int = 5,\n) -> None:\n    \"\"\"Plot Kaplan\u2013Meier survival curves by values of a grouping column.\n\n    Args:\n        df: Survival DataFrame.\n        group_col: Column defining groups.\n        duration_col: Duration column name.\n        event_col: Event indicator column name.\n        max_groups: Maximum number of distinct groups to plot.\n    \"\"\"\n    if group_col not in df.columns:\n        raise KeyError(f\"Column {group_col!r} not found in DataFrame.\")\n\n    kmf_local = KaplanMeierFitter()\n\n    groups = df[group_col].value_counts().index[:max_groups]\n    plt.figure(figsize=(8, 5))\n    for g in groups:\n        mask = df[group_col] == g\n        T_g = df.loc[mask, duration_col]\n        E_g = df.loc[mask, event_col]\n        kmf_local.fit(T_g, event_observed=E_g, label=str(g))\n        kmf_local.plot_survival_function()\n\n    plt.title(f\"Survival by {group_col}\")\n    plt.xlabel(\"Tenure (months)\")\n    plt.ylabel(\"Survival probability\")\n    plt.legend(title=group_col)\n    plt.show()\n\n\nfor col in [\"InternetService\", \"PaymentMethod\"]:\n    if col in surv_df.columns:\n        plot_km_by_group(surv_df, col, max_groups=4)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "These segment-based curves can highlight **high-risk configurations**, e.g.\nparticular combinations of service and payment method with lower survival.\n\nNext we move to a **multivariate model**: the Cox proportional hazards model.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Cox Proportional Hazards model\n\nKaplan\u2013Meier curves describe survival but do not handle multiple features\nsimultaneously. For that, we use the **Cox Proportional Hazards (PH) model**.\n\nCox model:\n\n- Models the **hazard** (instantaneous risk of churn) as a function of features.\n- Outputs **log hazard ratios** and **hazard ratios** (`exp(coef)`).\n- Interpretable: each coefficient is the multiplicative effect on the hazard.\n\n### 7.1 Feature engineering for Cox\n\n`lifelines.CoxPHFitter` expects a purely numeric DataFrame.\nWe will:\n\n1. Choose a set of features that are both meaningful and not too many.\n2. Convert Yes/No columns to 0/1.\n3. One-hot encode categorical variables with `get_dummies`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Feature set for Cox model (you can adjust this list)\ncox_features_raw: List[str] = [\n    \"tenure\",          # duration (also used as covariate; we could omit if we want)\n    \"churn_event\",     # event indicator\n    \"SeniorCitizen\",\n    \"Partner\",\n    \"Dependents\",\n    \"PaperlessBilling\",\n    \"Contract\",\n    \"PaymentMethod\",\n    \"MonthlyCharges\",\n    \"TotalCharges\",\n]\n\nmissing_cox_feats = [c for c in cox_features_raw if c not in surv_df.columns]\nif missing_cox_feats:\n    raise KeyError(f\"Missing expected columns for Cox model: {missing_cox_feats}\")\n\ncox_df = surv_df[cox_features_raw].copy()\n\n# Map Yes/No style columns to 0/1\nbool_like_cols: List[str] = [\"Partner\", \"Dependents\", \"PaperlessBilling\"]\nfor col in bool_like_cols:\n    if col in cox_df.columns:\n        cox_df[col] = cox_df[col].map({\"No\": 0, \"Yes\": 1}).astype(int)\n\n# One-hot encode categorical variables (dropping baseline)\ncox_df_encoded = pd.get_dummies(\n    cox_df,\n    columns=[\"Contract\", \"PaymentMethod\"],\n    drop_first=True,\n)\n\nprint(\"Cox design matrix shape:\", cox_df_encoded.shape)\ncox_df_encoded.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 7.2 Train / test split for Cox model\n\nAlthough survival models are often fit on the full dataset, it is useful\nfor evaluation to keep a **train / test split**.\n\nWe will:\n\n- Split rows into train and test sets.\n- Fit Cox on train.\n- Evaluate concordance index (c-index) on both.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from sklearn.model_selection import train_test_split\n\n# We keep the encoded dataset but separate duration/event from covariates\n\ntrain_df, test_df = train_test_split(\n    cox_df_encoded,\n    test_size=0.3,\n    random_state=RANDOM_STATE,\n    stratify=cox_df_encoded[\"churn_event\"],\n)\n\nprint(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 7.3 Fit Cox model and inspect summary\n\nWe now fit `CoxPHFitter` using:\n\n- `duration_col = 'tenure'`\n- `event_col = 'churn_event'`\n\nWe then print the summary with hazard ratios.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "cph = CoxPHFitter()\n\ncph.fit(train_df, duration_col=\"tenure\", event_col=\"churn_event\")\n\n# Text summary\ncph.print_summary()  # includes exp(coef), p-values, CIs\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Extract and visualise hazard ratios\nsummary_df = cph.summary.copy()\nsummary_df[\"hazard_ratio\"] = summary_df[\"exp(coef)\"]\n\nsummary_sorted = summary_df.sort_values(\"hazard_ratio\", ascending=False)\n\ndisplay(summary_sorted[[\"hazard_ratio\", \"p\", \"exp(coef) lower 95%\", \"exp(coef) upper 95%\"]])\n\nplt.figure(figsize=(8, 6))\nplt.barh(summary_sorted.index, summary_sorted[\"hazard_ratio\"])\nplt.axvline(1.0, linestyle=\"--\", color=\"black\")\nplt.xlabel(\"Hazard ratio (exp(coef))\")\nplt.title(\"Cox model hazard ratios\")\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Interpreting hazard ratios:**\n\n- Hazard ratio **> 1** \u2192 higher values of the feature are associated with\n  **higher churn risk** (customers churn earlier).\n- Hazard ratio **< 1** \u2192 higher values of the feature are associated with\n  **lower churn risk** (customers stay longer).\n\nExamples (exact values depend on your run):\n\n- `Contract_Two year` with hazard ratio << 1 means two-year contracts\n  **strongly reduce** churn risk vs the baseline contract.\n- A slight hazard ratio > 1 for `MonthlyCharges` suggests higher fees\n  modestly increase churn risk, controlling for other variables.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Model performance: concordance index\n\nThe **concordance index (c-index)** measures how well the model ranks\ncustomers by risk:\n\n- `1.0` \u2013 perfect ranking.\n- `0.5` \u2013 random ranking.\n\nWe compute c-index on train and test sets using the Cox risk scores.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Predicted partial hazards (risk scores)\ntrain_risk_scores = cph.predict_partial_hazard(train_df).values.ravel()\ntest_risk_scores = cph.predict_partial_hazard(test_df).values.ravel()\n\nc_index_train = concordance_index(\n    train_df[\"tenure\"], -train_risk_scores, train_df[\"churn_event\"]\n)\n# Note: we negate scores because higher risk -> shorter survival.\n\nc_index_test = concordance_index(\n    test_df[\"tenure\"], -test_risk_scores, test_df[\"churn_event\"]\n)\n\nprint(f\"C-index (train): {c_index_train:.3f}\")\nprint(f\"C-index (test):  {c_index_test:.3f}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "A decent c-index (typically 0.7\u20130.8) indicates the model is reasonably good\nat ranking customers by churn risk over time.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Proportional hazards assumption checks\n\nThe Cox model assumes that **hazard ratios are constant over time**\n(proportional hazards). We can roughly check this with\n`cph.check_assumptions`.\n\nThis function prints diagnostics and can generate residual plots\n(Schoenfeld residuals) to see if the effect of a covariate changes over time.\n\n> Note: this is more of a *diagnostic guide* than a hard rule; use judgement.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# This will print diagnostics and, if `show_plots=True`, open plots.\n# Run it interactively in your environment to inspect results.\n\ncph.check_assumptions(train_df, show_plots=False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "If the diagnostics highlight strong violations for a variable, you can:\n\n- Interact with that variable (e.g. create interactions).\n- Stratify the model by that variable.\n- Use time-varying effects or an alternative survival model.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Predicted survival curves for example customers\n\nTo make the model tangible, we construct a few **synthetic customers** and\nplot their predicted survival curves.\n\nExample profiles:\n\n- **Customer A** \u2013 Month-to-month, electronic check, higher monthly charges.\n- **Customer B** \u2013 Two-year contract, automatic bank transfer, lower charges.\n\nWe use the fitted Cox model to predict their survival functions over time.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Build base profile using medians for numeric and common values for binary\nbase_profile: Dict[str, float] = {\n    \"SeniorCitizen\": float(surv_df[\"SeniorCitizen\"].median()),\n    \"Partner\": 0.0,\n    \"Dependents\": 0.0,\n    \"PaperlessBilling\": 1.0,\n    \"MonthlyCharges\": float(surv_df[\"MonthlyCharges\"].median()),\n    \"TotalCharges\": float(surv_df[\"TotalCharges\"].median()),\n}\n\n# Columns used in the Cox design matrix (excluding duration/event)\ncox_feature_cols: List[str] = [col for col in cox_df_encoded.columns if col not in [\"tenure\", \"churn_event\"]]\n\n\ndef make_cox_design_row(\n    contract: str,\n    payment_method: str,\n    monthly_charges: float,\n) -> pd.DataFrame:\n    \"\"\"Create a single-row Cox design matrix for a hypothetical customer.\n\n    Args:\n        contract: Contract type as in original data.\n        payment_method: PaymentMethod as in original data.\n        monthly_charges: MonthlyCharges value to set.\n\n    Returns:\n        1-row DataFrame with columns matching cox_feature_cols.\n    \"\"\"\n    # Start with zeros for design matrix\n    data: Dict[str, float] = {col: 0.0 for col in cox_feature_cols}\n\n    # Insert base numeric/binary features\n    tmp = base_profile.copy()\n    tmp[\"MonthlyCharges\"] = monthly_charges\n\n    for col in data.keys():\n        if col in tmp:\n            data[col] = float(tmp[col])\n\n    # Handle one-hot encoded Contract_* columns\n    for col in cox_feature_cols:\n        if col.startswith(\"Contract_\"):\n            data[col] = 0.0\n    target_contract_col = f\"Contract_{contract}\"\n    if target_contract_col in data:\n        data[target_contract_col] = 1.0\n\n    # Handle one-hot encoded PaymentMethod_* columns\n    for col in cox_feature_cols:\n        if col.startswith(\"PaymentMethod_\"):\n            data[col] = 0.0\n    target_pm_col = f\"PaymentMethod_{payment_method}\"\n    if target_pm_col in data:\n        data[target_pm_col] = 1.0\n\n    return pd.DataFrame([data])\n\n\n# Define two example customers\nexample_A = make_cox_design_row(\n    contract=\"Month-to-month\",\n    payment_method=\"Electronic check\",\n    monthly_charges=float(surv_df[\"MonthlyCharges\"].quantile(0.75)),\n)\n\nexample_B = make_cox_design_row(\n    contract=\"Two year\",\n    payment_method=\"Bank transfer (automatic)\",\n    monthly_charges=float(surv_df[\"MonthlyCharges\"].quantile(0.25)),\n)\n\n# Time grid for prediction\nmax_tenure = float(surv_df[\"tenure\"].max())\ntimeline = np.linspace(1, max_tenure, 60)\n\nsurv_A = cph.predict_survival_function(example_A, times=timeline)\nsurv_B = cph.predict_survival_function(example_B, times=timeline)\n\nplt.figure(figsize=(8, 5))\nplt.plot(timeline, surv_A.T, label=\"A: M2M, high charges\")\nplt.plot(timeline, surv_B.T, label=\"B: 2-year, low charges\")\nplt.xlabel(\"Tenure (months)\")\nplt.ylabel(\"Predicted survival probability\")\nplt.title(\"Predicted survival curves for example customers\")\nplt.legend()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "In practice, these type of curves help answer questions like:\n\n- *\u201cIf I move this customer from month-to-month to a two-year contract,\n   how much longer do I expect them to stay?\u201d*\n- *\u201cHow does lowering monthly charges for high-risk customers change their\n   expected survival?\u201d*\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11. Survival by model-based risk segments\n\nWe can combine survival analysis with **risk scores** from the Cox model.\n\nSteps:\n\n1. Use the Cox model to compute risk scores (partial hazards) for all customers.\n2. Split customers into **risk quantiles** (e.g. low / medium / high risk).\n3. Plot Kaplan\u2013Meier curves by risk segment.\n\nIf the model is useful, high-risk segments should have lower survival.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Compute risk scores for the full encoded dataset\nfull_risk_scores = cph.predict_partial_hazard(cox_df_encoded).values.ravel()\n\nrisk_df = cox_df_encoded[[\"tenure\", \"churn_event\"]].copy()\nrisk_df[\"risk_score\"] = full_risk_scores\n\n# Define quantile-based risk segments\nrisk_df[\"risk_segment\"] = pd.qcut(\n    risk_df[\"risk_score\"],\n    q=3,\n    labels=[\"low_risk\", \"medium_risk\", \"high_risk\"],\n)\n\nrisk_df[\"risk_segment\"].value_counts()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Plot KM curves by risk segment\nkmf_risk = KaplanMeierFitter()\n\nplt.figure(figsize=(8, 5))\nfor seg in [\"low_risk\", \"medium_risk\", \"high_risk\"]:\n    mask = risk_df[\"risk_segment\"] == seg\n    T_seg = risk_df.loc[mask, \"tenure\"]\n    E_seg = risk_df.loc[mask, \"churn_event\"]\n\n    kmf_risk.fit(T_seg, event_observed=E_seg, label=seg)\n    kmf_risk.plot_survival_function()\n\nplt.title(\"Survival curves by model-based risk segment\")\nplt.xlabel(\"Tenure (months)\")\nplt.ylabel(\"Survival probability\")\nplt.legend(title=\"Risk segment\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "If the model is doing a good job, you should see:\n\n- **High-risk** segment with the **lowest survival**.\n- **Low-risk** segment with the **highest survival**.\n\nThis directly links the model output (risk score) to time-based retention.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12. Optional: Aalen Additive model\n\nThe **Aalen Additive model** is an alternative to Cox that models the hazard\nas a **sum** of time-varying coefficients times the covariates.\n\nIt is more flexible in some situations, and can show how the effect of\nfeatures **changes over time**.\n\nWe fit a small Aalen model on a simplified feature set just to illustrate.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Simplified design for Aalen (to keep things readable)\naalen_features_raw: List[str] = [\n    \"tenure\",\n    \"churn_event\",\n    \"SeniorCitizen\",\n    \"MonthlyCharges\",\n    \"TotalCharges\",\n]\n\nmissing_aalen = [c for c in aalen_features_raw if c not in surv_df.columns]\nif not missing_aalen:\n    aalen_df = surv_df[aalen_features_raw].copy()\n\n    aalen_fitter = AalenAdditiveFitter(fit_intercept=True)\n    aalen_fitter.fit(\n        aalen_df,\n        duration_col=\"tenure\",\n        event_col=\"churn_event\",\n    )\n\n    aalen_fitter.plot()\n    plt.title(\"Aalen Additive model \u2013 cumulative coefficients over time\")\n    plt.show()\nelse:\n    print(\"Skipping Aalen model; missing columns:\", missing_aalen)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The Aalen coefficient plots can show, for example, whether the effect of\n`SeniorCitizen` or `MonthlyCharges` on churn risk strengthens or weakens as\ntenure increases.\n\nFor production applications, you would typically choose **one core model**\n(Cox or a more advanced alternative) and spend more time validating it.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 13. Summary and next steps\n\nIn this expanded survival analysis notebook we:\n\n1. Reframed Telco churn as a **time-to-event** problem.\n2. Built **Kaplan\u2013Meier survival curves**:\n   - Overall.\n   - By contract and other segments.\n   - Assessed group differences via log-rank tests.\n3. Trained a **Cox Proportional Hazards model**:\n   - Engineered suitable covariates.\n   - Interpreted hazard ratios in business language.\n   - Evaluated model discrimination via **concordance index**.\n   - Checked proportional hazards assumptions.\n4. Created **scenario-based survival curves** for hypothetical customers.\n5. Combined model risk scores with survival curves to build **risk-based\n   survival segments**.\n6. Briefly explored an **Aalen Additive model** as an alternative.\n\n### How this complements classification-based churn models\n\n- Classification:\n  - \"Will this customer churn in the next X months?\"\n- Survival:\n  - \"How long until this customer is likely to churn?\"\n  - \"How does churn risk evolve over time?\"\n\nTogether, they support:\n\n- **Retention timing** \u2013 when to intervene for each segment.\n- **Contract / pricing design** \u2013 how different contracts change expected\n  lifetime.\n- **Scenario analysis** \u2013 compare policies via predicted survival.\n\n### Possible extensions\n\n- Include more features (services, add-ons, internet type) into the Cox model.\n- Use **time-varying covariates** (e.g. evolving usage or billing behaviour).\n- Explore parametric survival models (Weibull, log-logistic, etc.).\n- Integrate survival predictions into a **lifetime value (LTV)** model by\n  combining expected survival with monthly revenue.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}