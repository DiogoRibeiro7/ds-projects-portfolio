{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Time Series Forecasting \u2013 Energy Load with External Regressors\n",
        "\n",
        "This notebook is a complete mini-project for **time series forecasting** in an\n",
        "energy / traffic setting. It is structured in three tiers:\n",
        "\n",
        "1. **Tier 1 \u2013 Analysis**: EDA, decomposition, anomaly detection, cross-correlation.\n",
        "2. **Tier 2 \u2013 Modelling**: ARIMA/SARIMA-type models + ML regressors with exogenous variables and proper evaluation.\n",
        "3. **Tier 3 \u2013 Advanced / Deployment**: Probabilistic forecasts and a simple champion\u2013challenger setup.\n",
        "\n",
        "The notebook assumes you have an **hourly (or daily) energy load dataset** with an\n",
        "external regressor such as temperature. You can adapt the loading cell to your\n",
        "own data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup and Data Loading\n",
        "\n",
        "Expected data format (you can adjust this to your dataset):\n",
        "\n",
        "- CSV file under `data/energy.csv`.\n",
        "- Columns:\n",
        "  - `timestamp` \u2013 Datetime string.\n",
        "  - `load` \u2013 Energy load (target).\n",
        "  - `temperature` \u2013 External regressor (optional but recommended).\n",
        "\n",
        "If your dataset has different names, you can change the column references in\n",
        "this section.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "\n",
        "DATA_PATH: Path = Path(\"data\") / \"energy.csv\"\n",
        "RANDOM_STATE: int = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Expected data at {DATA_PATH.resolve()} \u2013 please place your CSV there \"\n",
        "        \"or edit DATA_PATH.\"\n",
        "    )\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Basic sanity checks and parsing\n",
        "expected_cols = {\"timestamp\", \"load\"}\n",
        "if not expected_cols.issubset(df.columns):\n",
        "    raise ValueError(\n",
        "        f\"Dataframe must contain at least columns {expected_cols}, got {set(df.columns)}\"\n",
        "    )\n",
        "\n",
        "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "df = df.sort_values(\"timestamp\").dropna(subset=[\"timestamp\", \"load\"])\n",
        "df = df.set_index(\"timestamp\").asfreq(\"H\")  # adjust to 'D' for daily data\n",
        "\n",
        "df[[\"load\"]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper metrics\n",
        "\n",
        "We will use RMSE, MAE and MAPE throughout the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Compute root mean squared error.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : np.ndarray\n",
        "        True target values.\n",
        "    y_pred : np.ndarray\n",
        "        Predicted target values.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        RMSE value.\n",
        "    \"\"\"\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "\n",
        "def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    \"\"\"Compute mean absolute percentage error (MAPE).\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Very small true values are clipped to avoid division by zero.\n",
        "    \"\"\"\n",
        "    y_true_safe = np.clip(y_true, 1e-6, None)\n",
        "    return float(np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100.0)\n",
        "\n",
        "\n",
        "y_example = df[\"load\"].to_numpy()\n",
        "print(\"Example RMSE self-check:\", rmse(y_example, y_example))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tier 1 \u2013 Time Series Analysis\n",
        "\n",
        "Goals:\n",
        "\n",
        "- Visualise the series.\n",
        "- Decompose into trend, seasonality and residual.\n",
        "- Detect anomalies with simple rules and statistical thresholds.\n",
        "- Explore cross-correlation with an external regressor (e.g. temperature).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Basic EDA and decomposition\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot the full time series\n",
        "df[\"load\"].plot()\n",
        "plt.title(\"Energy load over time\")\n",
        "plt.ylabel(\"Load\")\n",
        "plt.show()\n",
        "\n",
        "# Seasonal decomposition (adjust period to your data)\n",
        "period = 24  # e.g. daily seasonality for hourly data\n",
        "decomp = seasonal_decompose(df[\"load\"].dropna(), model=\"additive\", period=period)\n",
        "fig = decomp.plot()\n",
        "fig.set_size_inches(10, 8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df[\"trend\"] = decomp.trend\n",
        "df[\"seasonal\"] = decomp.seasonal\n",
        "df[\"resid\"] = decomp.resid\n",
        "df[[\"load\", \"trend\", \"seasonal\", \"resid\"]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Simple anomaly detection\n",
        "\n",
        "We apply two simple approaches:\n",
        "\n",
        "- **Statistical residual-based**: flag points where residuals exceed a\n",
        "  multiple of the residual standard deviation.\n",
        "- **Rule-based**: flag values below zero or above a high quantile.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Statistical anomaly detection based on residuals\n",
        "resid = df[\"resid\"].dropna()\n",
        "resid_std = resid.std()\n",
        "z_thresh = 3.0\n",
        "\n",
        "stat_anomaly_mask = np.abs(resid) > z_thresh * resid_std\n",
        "stat_anomalies = resid[stat_anomaly_mask]\n",
        "\n",
        "# Rule-based anomalies (negative load or extremely high)\n",
        "upper_threshold = df[\"load\"].quantile(0.99)\n",
        "rule_anomaly_mask = (df[\"load\"] < 0) | (df[\"load\"] > upper_threshold)\n",
        "rule_anomalies = df[\"load\"][rule_anomaly_mask]\n",
        "\n",
        "print(\"Statistical anomalies:\", stat_anomalies.shape[0])\n",
        "print(\"Rule-based anomalies: \", rule_anomalies.shape[0])\n",
        "\n",
        "# Visualise anomalies\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df.index, df[\"load\"], label=\"load\", alpha=0.7)\n",
        "plt.scatter(stat_anomalies.index, stat_anomalies.index.map(df[\"load\"]),\n",
        "            color=\"red\", label=\"statistical\", s=15)\n",
        "plt.scatter(rule_anomalies.index, rule_anomalies.values,\n",
        "            color=\"orange\", label=\"rule-based\", s=15)\n",
        "plt.legend()\n",
        "plt.title(\"Detected anomalies\")\n",
        "plt.ylabel(\"Load\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Cross-correlation with external regressors\n",
        "\n",
        "If the dataset includes a `temperature` column, we can inspect the\n",
        "relationship between temperature and load, including lagged effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if \"temperature\" in df.columns:\n",
        "    # Basic scatter and joint plot\n",
        "    sns.scatterplot(x=\"temperature\", y=\"load\", data=df.sample(min(2000, len(df))))\n",
        "    plt.title(\"Load vs Temperature (sample)\")\n",
        "    plt.show()\n",
        "\n",
        "    # Cross-correlation function (simplified)\n",
        "    # We consider a range of lags and compute correlation coefficients.\n",
        "    max_lag = 48  # two days for hourly data\n",
        "    load_vals = df[\"load\"].fillna(method=\"ffill\").to_numpy()\n",
        "    temp_vals = df[\"temperature\"].fillna(method=\"ffill\").to_numpy()\n",
        "\n",
        "    lags = np.arange(-max_lag, max_lag + 1)\n",
        "    ccs: List[float] = []\n",
        "    for lag in lags:\n",
        "        if lag < 0:\n",
        "            x = temp_vals[:lag]\n",
        "            y = load_vals[-lag:]\n",
        "        elif lag > 0:\n",
        "            x = temp_vals[lag:]\n",
        "            y = load_vals[:-lag]\n",
        "        else:\n",
        "            x = temp_vals\n",
        "            y = load_vals\n",
        "        ccs.append(np.corrcoef(x, y)[0, 1])\n",
        "\n",
        "    plt.plot(lags, ccs)\n",
        "    plt.axhline(0, color=\"black\", linewidth=0.8)\n",
        "    plt.xlabel(\"Lag (hours, temp leads if negative)\")\n",
        "    plt.ylabel(\"Cross-correlation\")\n",
        "    plt.title(\"Cross-correlation between temperature and load\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No 'temperature' column found \u2013 skipping cross-correlation analysis.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tier 2 \u2013 Modelling\n",
        "\n",
        "Goals:\n",
        "\n",
        "- Train standard time series models:\n",
        "  - SARIMAX (ARIMA with seasonality and exogenous variables).\n",
        "  - Machine-learning regressor with lagged features and exogenous vars.\n",
        "- Use robust evaluation:\n",
        "  - Rolling-origin cross-validation.\n",
        "  - Error distribution analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Train / test split\n",
        "\n",
        "We keep the last chunk of the series as a hold-out test set. The earlier\n",
        "part is used for model fitting and cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "horizon = 24 * 7  # one week of hourly data for test set; adjust as needed\n",
        "if len(df) <= horizon * 2:\n",
        "    raise ValueError(\"Not enough data relative to chosen horizon.\")\n",
        "\n",
        "train_df = df.iloc[:-horizon].copy()\n",
        "test_df = df.iloc[-horizon:].copy()\n",
        "\n",
        "print(\"Train size:\", train_df.shape[0])\n",
        "print(\"Test size: \", test_df.shape[0])\n",
        "\n",
        "train_df[[\"load\"]].tail(), test_df[[\"load\"]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 SARIMAX model (with optional exogenous regressor)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def fit_sarimax(\n",
        "    train: pd.DataFrame,\n",
        "    test: pd.DataFrame,\n",
        "    order: Tuple[int, int, int] = (1, 0, 1),\n",
        "    seasonal_order: Tuple[int, int, int, int] = (1, 1, 1, 24),\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Fit a SARIMAX model and forecast on the test horizon.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train : pd.DataFrame\n",
        "        Training set with column `load` and optionally `temperature`.\n",
        "    test : pd.DataFrame\n",
        "        Test set with the same columns. Only index length is used.\n",
        "    order : tuple\n",
        "        Non-seasonal ARIMA order (p, d, q).\n",
        "    seasonal_order : tuple\n",
        "        Seasonal ARIMA order (P, D, Q, s).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[np.ndarray, np.ndarray]\n",
        "        Tuple of (fitted_values on train, forecast_values on test).\n",
        "    \"\"\"\n",
        "    endog_train = train[\"load\"].astype(float)\n",
        "    exog_cols: List[str] = []\n",
        "    if \"temperature\" in train.columns:\n",
        "        exog_cols = [\"temperature\"]\n",
        "\n",
        "    exog_train = train[exog_cols] if exog_cols else None\n",
        "    exog_test = test[exog_cols] if exog_cols else None\n",
        "\n",
        "    model = SARIMAX(\n",
        "        endog_train,\n",
        "        exog=exog_train,\n",
        "        order=order,\n",
        "        seasonal_order=seasonal_order,\n",
        "        enforce_stationarity=False,\n",
        "        enforce_invertibility=False,\n",
        "    )\n",
        "    res = model.fit(disp=False)\n",
        "\n",
        "    fitted = res.fittedvalues.to_numpy()\n",
        "    forecast = res.forecast(steps=len(test), exog=exog_test).to_numpy()\n",
        "    return fitted, forecast\n",
        "\n",
        "\n",
        "sarimax_fitted, sarimax_forecast = fit_sarimax(train_df, test_df)\n",
        "\n",
        "print(\"SARIMAX train RMSE:\", rmse(train_df[\"load\"].to_numpy(), sarimax_fitted))\n",
        "print(\"SARIMAX test RMSE: \", rmse(test_df[\"load\"].to_numpy(), sarimax_forecast))\n",
        "\n",
        "plt.plot(train_df.index, train_df[\"load\"], label=\"train\")\n",
        "plt.plot(test_df.index, test_df[\"load\"], label=\"test\")\n",
        "plt.plot(test_df.index, sarimax_forecast, label=\"sarimax forecast\")\n",
        "plt.legend()\n",
        "plt.title(\"SARIMAX forecast vs actual\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Feature engineering for ML regressor\n",
        "\n",
        "We now build a supervised learning dataset with:\n",
        "\n",
        "- Lagged load values.\n",
        "- Calendar features: hour of day, day of week.\n",
        "- External regressor (temperature).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def make_lagged_features(\n",
        "    df_in: pd.DataFrame,\n",
        "    n_lags: int = 24,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Create a feature matrix with lagged load and calendar features.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_in : pd.DataFrame\n",
        "        Input time series with index as timestamp and columns including `load`.\n",
        "    n_lags : int\n",
        "        Number of lagged `load` features to include.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Feature DataFrame aligned with the original index (rows with\n",
        "        insufficient history are dropped).\n",
        "    \"\"\"\n",
        "    df_feat = df_in.copy()\n",
        "\n",
        "    # Lag features\n",
        "    for lag in range(1, n_lags + 1):\n",
        "        df_feat[f\"lag_{lag}\"] = df_feat[\"load\"].shift(lag)\n",
        "\n",
        "    # Calendar features\n",
        "    df_feat[\"hour\"] = df_feat.index.hour\n",
        "    df_feat[\"dayofweek\"] = df_feat.index.dayofweek\n",
        "\n",
        "    # Keep temperature if present\n",
        "    if \"temperature\" in df_feat.columns:\n",
        "        df_feat[\"temperature\"] = df_feat[\"temperature\"].interpolate(limit_direction=\"both\")\n",
        "\n",
        "    df_feat = df_feat.dropna()\n",
        "    return df_feat\n",
        "\n",
        "\n",
        "lagged_df = make_lagged_features(df, n_lags=24)\n",
        "lagged_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Align train/test indices with lagged features\n",
        "lagged_train = lagged_df.loc[train_df.index.intersection(lagged_df.index)]\n",
        "lagged_test = lagged_df.loc[test_df.index.intersection(lagged_df.index)]\n",
        "\n",
        "feature_cols = [c for c in lagged_train.columns if c != \"load\"]\n",
        "X_train = lagged_train[feature_cols].to_numpy()\n",
        "y_train = lagged_train[\"load\"].to_numpy()\n",
        "X_test = lagged_test[feature_cols].to_numpy()\n",
        "y_test = lagged_test[\"load\"].to_numpy()\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Rolling-origin cross-validation for the ML model\n",
        "\n",
        "We use `TimeSeriesSplit` to approximate rolling-origin evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_ts_model_cv(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    n_splits: int = 5,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Evaluate a scikit-learn regressor with time series CV.\n",
        "\n",
        "    A GradientBoostingRegressor is used for illustration.\n",
        "    \"\"\"\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    metrics: List[Dict[str, float]] = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X), start=1):\n",
        "        X_tr, X_val = X[train_idx], X[val_idx]\n",
        "        y_tr, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        model = GradientBoostingRegressor(\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_estimators=300,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=3,\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "        y_hat = model.predict(X_val)\n",
        "\n",
        "        metrics.append(\n",
        "            {\n",
        "                \"fold\": fold,\n",
        "                \"rmse\": rmse(y_val, y_hat),\n",
        "                \"mae\": mean_absolute_error(y_val, y_hat),\n",
        "                \"mape\": mape(y_val, y_hat),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(metrics)\n",
        "\n",
        "\n",
        "cv_metrics = evaluate_ts_model_cv(X_train, y_train, n_splits=5)\n",
        "cv_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.boxplot(data=cv_metrics.melt(id_vars=\"fold\", value_vars=[\"rmse\", \"mape\"]),\n",
        "            x=\"variable\", y=\"value\")\n",
        "plt.title(\"Rolling-origin CV error distribution\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Fit final ML model and evaluate on test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gbr_model = GradientBoostingRegressor(\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        ")\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_test = gbr_model.predict(X_test)\n",
        "\n",
        "print(\"GBR test RMSE:\", rmse(y_test, y_pred_test))\n",
        "print(\"GBR test MAE: \", mean_absolute_error(y_test, y_pred_test))\n",
        "print(\"GBR test MAPE:\", mape(y_test, y_pred_test))\n",
        "\n",
        "plt.plot(lagged_test.index, y_test, label=\"actual\")\n",
        "plt.plot(lagged_test.index, y_pred_test, label=\"gbr forecast\")\n",
        "plt.legend()\n",
        "plt.title(\"Gradient boosting forecast vs actual\")\n",
        "plt.show()\n",
        "\n",
        "sns.histplot(y_test - y_pred_test, bins=30)\n",
        "plt.title(\"GBR residual distribution\")\n",
        "plt.xlabel(\"Error (y_true - y_pred)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tier 3 \u2013 Probabilistic Forecasts and Champion\u2013Challenger\n",
        "\n",
        "Goals:\n",
        "\n",
        "- Provide **uncertainty estimates** around forecasts.\n",
        "- Set up a simple **champion\u2013challenger** comparison between models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Quantile Gradient Boosting (probabilistic forecasts)\n",
        "\n",
        "We fit separate GradientBoostingRegressor models to estimate quantiles\n",
        "of the conditional distribution of `load` given features. For simplicity\n",
        "we use three quantiles: 0.1, 0.5, 0.9.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "quantiles = [0.1, 0.5, 0.9]\n",
        "quantile_models: Dict[float, GradientBoostingRegressor] = {}\n",
        "\n",
        "for q in quantiles:\n",
        "    model_q = GradientBoostingRegressor(\n",
        "        loss=\"quantile\",\n",
        "        alpha=q,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=3,\n",
        "    )\n",
        "    model_q.fit(X_train, y_train)\n",
        "    quantile_models[q] = model_q\n",
        "\n",
        "quantile_forecasts = {q: m.predict(X_test) for q, m in quantile_models.items()}\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "idx = lagged_test.index\n",
        "plt.plot(idx, y_test, label=\"actual\", color=\"black\")\n",
        "plt.plot(idx, quantile_forecasts[0.5], label=\"q0.5\", linestyle=\"--\")\n",
        "plt.fill_between(idx,\n",
        "                 quantile_forecasts[0.1],\n",
        "                 quantile_forecasts[0.9],\n",
        "                 alpha=0.3,\n",
        "                 label=\"q0.1\u2013q0.9 band\")\n",
        "plt.legend()\n",
        "plt.title(\"Probabilistic forecast with quantile gradient boosting\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Simple conformal-style intervals from residuals\n",
        "\n",
        "As a lightweight alternative, we can build intervals by:\n",
        "\n",
        "- Computing residuals on a validation set.\n",
        "- Taking empirical quantiles of |residual|.\n",
        "- Expanding point forecasts by that amount.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Here we reuse the CV folds to approximate residual distribution.\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "all_abs_residuals: List[float] = []\n",
        "\n",
        "for train_idx, val_idx in tscv.split(X_train):\n",
        "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
        "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "    m = GradientBoostingRegressor(\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=3,\n",
        "    )\n",
        "    m.fit(X_tr, y_tr)\n",
        "    y_hat_val = m.predict(X_val)\n",
        "    all_abs_residuals.extend(np.abs(y_val - y_hat_val))\n",
        "\n",
        "all_abs_residuals_arr = np.array(all_abs_residuals)\n",
        "alpha = 0.1  # 90% interval\n",
        "q_conformal = np.quantile(all_abs_residuals_arr, 1 - alpha)\n",
        "print(\"Conformal absolute error quantile (90% band):\", q_conformal)\n",
        "\n",
        "point_forecast = y_pred_test\n",
        "lower_band = point_forecast - q_conformal\n",
        "upper_band = point_forecast + q_conformal\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(lagged_test.index, y_test, label=\"actual\", color=\"black\")\n",
        "plt.plot(lagged_test.index, point_forecast, label=\"point forecast\", linestyle=\"--\")\n",
        "plt.fill_between(lagged_test.index, lower_band, upper_band, alpha=0.3, label=\"conformal band\")\n",
        "plt.legend()\n",
        "plt.title(\"Point forecast with conformal-style prediction band\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Champion\u2013challenger comparison\n",
        "\n",
        "We compare three candidates on the same test horizon:\n",
        "\n",
        "- **Na\u00efve**: last observed value carried forward.\n",
        "- **SARIMAX**: from Tier 2.\n",
        "- **GBR**: ML regressor with lagged features.\n",
        "\n",
        "You can extend this table with additional models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Align lengths for fair comparison\n",
        "y_test_sarimax = test_df[\"load\"].to_numpy()\n",
        "\n",
        "# Naive forecast: last observed load from train repeated over horizon\n",
        "naive_forecast = np.repeat(train_df[\"load\"].iloc[-1], len(y_test_sarimax))\n",
        "\n",
        "champion_rows = []\n",
        "\n",
        "def model_metrics(name: str, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"rmse\": rmse(y_true, y_pred),\n",
        "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
        "        \"mape\": mape(y_true, y_pred),\n",
        "    }\n",
        "\n",
        "champion_rows.append(model_metrics(\"NaiveLast\", y_test_sarimax, naive_forecast))\n",
        "champion_rows.append(model_metrics(\"SARIMAX\", y_test_sarimax, sarimax_forecast))\n",
        "\n",
        "# For GBR, y_test comes from lagged_test; align with test_df tail\n",
        "aligned_len = min(len(y_test_sarimax), len(y_test))\n",
        "champion_rows.append(\n",
        "    model_metrics(\n",
        "        \"GBR\",\n",
        "        y_test[-aligned_len:],\n",
        "        y_pred_test[-aligned_len:],\n",
        "    )\n",
        ")\n",
        "\n",
        "champion_df = pd.DataFrame(champion_rows).set_index(\"model\")\n",
        "champion_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "champion_df.sort_values(\"rmse\").plot(kind=\"bar\")\n",
        "plt.title(\"Champion\u2013challenger comparison (lower is better)\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a production setting you would:\n",
        "\n",
        "- Declare the current best model as **champion**.\n",
        "- Introduce new models as **challengers**.\n",
        "- Continuously evaluate them on rolling windows.\n",
        "- Promote challengers to champion when they show consistent uplift\n",
        "  under your business metrics (not only RMSE/MAPE).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Next Steps\n",
        "\n",
        "- Replace the placeholder dataset with your real energy or traffic data.\n",
        "- Tune SARIMAX orders and GBR hyperparameters.\n",
        "- Add more exogenous variables (calendar events, holidays, pricing, etc.).\n",
        "- Wrap the final model into a small API or batch scoring pipeline for deployment.\n",
        "\n",
        "This notebook gives you a full path from **EDA** to **probabilistic forecasts**\n",
        "and a simple **champion\u2013challenger** framework for time series forecasting.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}