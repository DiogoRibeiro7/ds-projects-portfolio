{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc3cd01",
   "metadata": {},
   "source": [
    "\n",
    "# Sequential and Always-Valid Testing for A/B Experiments\n",
    "\n",
    "This notebook focuses on **sequential / always-valid testing** in A/B experiments.\n",
    "\n",
    "We compare three approaches on simulated data:\n",
    "\n",
    "1. **Fixed-horizon test**  \n",
    "   - Decide only once at the planned sample size (classical design).  \n",
    "2. **Naive peeking**  \n",
    "   - Look at the p-value many times and stop as soon as it is < 0.05.  \n",
    "   - This inflates Type I error (false positive rate).  \n",
    "3. **Mixture sequential test (mSPRT / e-value)**  \n",
    "   - Uses an **always-valid** test statistic built from the z-score.  \n",
    "   - You can stop **any time** you like and still control Type I error.\n",
    "\n",
    "We use a **simplified but practical** mSPRT-style test for the difference in Bernoulli\n",
    "conversion between two arms (control vs treatment). The construction:\n",
    "\n",
    "- At each interim look, we compute the usual z-statistic \\(Z_t\\) for the difference in proportions.  \n",
    "- Under the null, \\(Z_t \\approx \\mathcal{N}(0, 1)\\).  \n",
    "- We define an **e-value** (likelihood ratio) based on a **mixture alternative**:\n",
    "\n",
    "\\[\n",
    "E_t = \\frac{f_\\text{mix}(Z_t)}{f_0(Z_t)},\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\(f_0\\) is the standard normal density,  \n",
    "- \\(f_\\text{mix}\\) is a mixture of normal alternatives centered at 0 with variance \\(1+\\tau^2\\).\n",
    "\n",
    "This yields a closed-form expression:\n",
    "\n",
    "\\[\n",
    "E_t = \\frac{1}{\\sqrt{1 + \\tau^2}} \n",
    "\\exp\\left( \\frac{\\tau^2}{2(1 + \\tau^2)} Z_t^2 \\right).\n",
    "\\]\n",
    "\n",
    "Under \\(H_0\\), \\(E_t\\) is a **nonnegative martingale** with \\(\\mathbb{E}[E_t] = 1\\).  \n",
    "By Ville's inequality, for any \\(\\alpha\\in(0,1)\\):\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}_0\\big(\\sup_t E_t \\ge 1/\\alpha\\big) \\le \\alpha,\n",
    "\\]\n",
    "\n",
    "so the stopping rule “**reject when \\(E_t \\ge 1/\\alpha\\)**” is valid even if we peek all the time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e99f23",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c415462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Literal, Dict, Any\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4835f0",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Proportion helpers and z-test\n",
    "\n",
    "We re-use a typed summary class and a two-sample proportion z-test. These are standard\n",
    "building blocks for A/B tests with a Bernoulli conversion metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class PropSummary:\n",
    "    \"\"\"Summary of a Bernoulli proportion.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    p : float\n",
    "        Sample proportion x / n.\n",
    "    n : int\n",
    "        Sample size.\n",
    "    x : int\n",
    "        Number of successes.\n",
    "    \"\"\"\n",
    "    p: float\n",
    "    n: int\n",
    "    x: int\n",
    "\n",
    "\n",
    "def summarize_prop(x: int, n: int) -> PropSummary:\n",
    "    \"\"\"Validate and summarize a proportion sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : int\n",
    "        Number of successes, in [0, n].\n",
    "    n : int\n",
    "        Sample size, must be positive.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PropSummary\n",
    "        Dataclass with p, n, x.\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be positive.\")\n",
    "    if not (0 <= x <= n):\n",
    "        raise ValueError(\"x must satisfy 0 <= x <= n.\")\n",
    "    return PropSummary(p=x / n, n=n, x=x)\n",
    "\n",
    "\n",
    "def two_prop_z(\n",
    "    x1: int,\n",
    "    n1: int,\n",
    "    x2: int,\n",
    "    n2: int,\n",
    ") -> float:\n",
    "    \"\"\"Compute the z-statistic for a two-sample proportion test.\n",
    "\n",
    "    Uses the usual pooled-variance estimate under H0: p1 = p2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x1, n1, x2, n2 : int\n",
    "        Success counts and sample sizes for arms 1 and 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        z-statistic (signed), where z > 0 means arm 2 has higher conversion.\n",
    "    \"\"\"\n",
    "    s1, s2 = summarize_prop(x1, n1), summarize_prop(x2, n2)\n",
    "    p_pool = (s1.x + s2.x) / (s1.n + s2.n)\n",
    "    se = math.sqrt(p_pool * (1.0 - p_pool) * (1.0 / s1.n + 1.0 / s2.n))\n",
    "    if se == 0.0:\n",
    "        raise ZeroDivisionError(\"Standard error is zero; check inputs.\")\n",
    "    z = (s2.p - s1.p) / se\n",
    "    return float(z)\n",
    "\n",
    "\n",
    "def two_prop_pvalue_from_z(z: float, two_sided: bool = True) -> float:\n",
    "    \"\"\"Compute p-value from a z-statistic using the normal CDF approximation.\"\"\"\n",
    "    # standard normal CDF via erf\n",
    "    cdf = 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))\n",
    "    if two_sided:\n",
    "        p = 2.0 * min(cdf, 1.0 - cdf)\n",
    "    else:\n",
    "        p = 1.0 - cdf\n",
    "    return float(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f817d5a0",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Simulated streaming experiments\n",
    "\n",
    "We simulate experiments as **streams of users** arriving over time.\n",
    "\n",
    "Each user:\n",
    "\n",
    "- Is randomly assigned to `group ∈ {control, treatment}` with probability 0.5 each.  \n",
    "- Converts with probability:\n",
    "  - `p_control` in the control arm,  \n",
    "  - `p_treat` in the treatment arm.\n",
    "\n",
    "We consider both:\n",
    "\n",
    "- **A/A (null)** experiments: `p_control = p_treat`, to estimate Type I error.  \n",
    "- **A/B (effect)** experiments: `p_treat > p_control`, to compare detection performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_stream(\n",
    "    n: int,\n",
    "    p_control: float,\n",
    "    p_treat: float,\n",
    "    seed: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Simulate a streaming A/B experiment with Bernoulli conversion.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Total number of users.\n",
    "    p_control : float\n",
    "        Conversion probability in control.\n",
    "    p_treat : float\n",
    "        Conversion probability in treatment.\n",
    "    seed : int | None\n",
    "        Random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Columns: index (arrival order), group, converted.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    group_flag = rng.binomial(1, 0.5, size=n)\n",
    "    group = np.where(group_flag == 0, \"control\", \"treatment\")\n",
    "\n",
    "    p = np.where(group_flag == 0, p_control, p_treat)\n",
    "    converted = rng.binomial(1, p)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"group\": group,\n",
    "            \"converted\": converted.astype(int),\n",
    "        }\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6278dc68",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Fixed-horizon and naive-peeking tests\n",
    "\n",
    "We revisit two simple strategies:\n",
    "\n",
    "1. **Fixed-horizon**: compute the z-test and p-value once at the final sample size `n`.  \n",
    "2. **Naive-peeking**: every `look_step` users, recompute the p-value and stop early if `p < α`.\n",
    "\n",
    "Both use the same z-test; the only difference is **how often we look**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7206e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ztest_at_n(df_stream: pd.DataFrame, n: int, two_sided: bool = True) -> float:\n",
    "    \"\"\"Compute two-sided p-value at a fixed sample size using the z-test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_stream : DataFrame\n",
    "        Streaming experiment data with columns group, converted.\n",
    "    n : int\n",
    "        Sample size at which to compute the test.\n",
    "    two_sided : bool\n",
    "        If True, use two-sided p-value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        p-value at sample size n.\n",
    "    \"\"\"\n",
    "    sub = df_stream.iloc[:n]\n",
    "    tab = (\n",
    "        sub.groupby(\"group\")[\"converted\"]\n",
    "           .agg([\"sum\", \"count\"])\n",
    "           .rename(columns={\"sum\": \"x\", \"count\": \"n\"})\n",
    "    )\n",
    "    x1, n1 = int(tab.loc[\"control\", \"x\"]), int(tab.loc[\"control\", \"n\"])\n",
    "    x2, n2 = int(tab.loc[\"treatment\", \"x\"]), int(tab.loc[\"treatment\", \"n\"])\n",
    "    z = two_prop_z(x1, n1, x2, n2)\n",
    "    p = two_prop_pvalue_from_z(z, two_sided=two_sided)\n",
    "    return p\n",
    "\n",
    "\n",
    "def naive_peek_pvalues(\n",
    "    df_stream: pd.DataFrame,\n",
    "    look_step: int,\n",
    "    two_sided: bool = True,\n",
    ") -> Tuple[list[int], list[float]]:\n",
    "    \"\"\"Compute p-values over time by peeking every `look_step` users.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_stream : DataFrame\n",
    "        Streaming experiment data.\n",
    "    look_step : int\n",
    "        Frequency of interim looks.\n",
    "    two_sided : bool\n",
    "        If True, two-sided p-values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    look_sizes : list[int]\n",
    "        Sample sizes at which we looked.\n",
    "    p_values : list[float]\n",
    "        p-values at each look.\n",
    "    \"\"\"\n",
    "    look_sizes: list[int] = []\n",
    "    p_values: list[float] = []\n",
    "\n",
    "    n_total = df_stream.shape[0]\n",
    "    for n in range(look_step, n_total + 1, look_step):\n",
    "        p = ztest_at_n(df_stream, n, two_sided=two_sided)\n",
    "        look_sizes.append(n)\n",
    "        p_values.append(p)\n",
    "\n",
    "    return look_sizes, p_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98226dbe",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Mixture sequential test: an always-valid e-value\n",
    "\n",
    "We now define an **always-valid sequential test** based on a **mixture likelihood ratio**\n",
    "for the z-statistic.\n",
    "\n",
    "Recall:\n",
    "\n",
    "- Under \\(H_0\\), the z-statistic \\(Z_t\\) is approximately \\(\\mathcal{N}(0,1)\\).  \n",
    "- Under a mixture of normal alternatives centered at 0 with variance \\(1 + \\tau^2\\),\n",
    "\n",
    "  \\[\n",
    "  f_\\text{mix}(z) = \\frac{1}{\\sqrt{2\\pi (1 + \\tau^2)}} \n",
    "  \\exp\\Big(-\\frac{z^2}{2(1 + \\tau^2)}\\Big).\n",
    "  \\]\n",
    "\n",
    "- Under the null, the density is \\(f_0(z) = (2\\pi)^{-1/2} \\exp(-z^2/2)\\).\n",
    "\n",
    "The resulting **mixture likelihood ratio** (our e-value) is:\n",
    "\n",
    "\\[\n",
    "E(z) = \\frac{f_\\text{mix}(z)}{f_0(z)}\n",
    "= \\frac{1}{\\sqrt{1 + \\tau^2}} \n",
    "  \\exp\\left( \\frac{\\tau^2}{2(1 + \\tau^2)} z^2 \\right).\n",
    "\\]\n",
    "\n",
    "Under \\(H_0\\), \\(E(Z_t)\\) is a nonnegative martingale with expectation 1.\n",
    "We then define:\n",
    "\n",
    "- The running maximum \\(E^*_t = \\max_{s \\le t} E(Z_s)\\).  \n",
    "- A stopping rule: **reject** when \\(E^*_t \\ge 1/\\alpha\\).  \n",
    "- An always-valid p-value at time t: \\(p_t = \\min(1, 1 / E^*_t)\\).\n",
    "\n",
    "This controls the Type I error at level \\(\\alpha\\) regardless of when we stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mixture_sprt_evalue(\n",
    "    z: float,\n",
    "    tau2: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"Compute the mixture SPRT e-value E(z) for a z-statistic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : float\n",
    "        Z-statistic (approximately standard normal under H0).\n",
    "    tau2 : float\n",
    "        Prior variance parameter for the mixture of normal alternatives.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        E(z) = f_mix(z) / f_0(z).\n",
    "    \"\"\"\n",
    "    if tau2 <= 0.0:\n",
    "        raise ValueError(\"tau2 must be positive.\")\n",
    "    # E(z) = 1/sqrt(1+tau2) * exp( tau2/(2(1+tau2)) * z^2 )\n",
    "    coef = 1.0 / math.sqrt(1.0 + tau2)\n",
    "    exponent = (tau2 / (2.0 * (1.0 + tau2))) * (z ** 2)\n",
    "    return float(coef * math.exp(exponent))\n",
    "\n",
    "\n",
    "def always_valid_test_on_stream(\n",
    "    df_stream: pd.DataFrame,\n",
    "    look_step: int,\n",
    "    alpha: float = 0.05,\n",
    "    tau2: float = 1.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run an always-valid mixture sequential test on a streaming A/B experiment.\n",
    "\n",
    "    At each look (every `look_step` users), we compute the z-statistic for\n",
    "    difference in proportions, then the e-value E(z), and track the running\n",
    "    maximum E*.\n",
    "\n",
    "    We reject when E* >= 1/alpha.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_stream : DataFrame\n",
    "        Streaming experiment data with columns group, converted.\n",
    "    look_step : int\n",
    "        Frequency of interim looks.\n",
    "    alpha : float\n",
    "        Desired Type I error level.\n",
    "    tau2 : float\n",
    "        Mixture prior variance for the e-value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Keys:\n",
    "        - rejected : bool, whether the test ever crossed the boundary.\n",
    "        - stopping_n : int | None, sample size at first rejection (None if never).\n",
    "        - look_sizes : list[int], sample sizes at each look.\n",
    "        - e_values : list[float], e-value at each look.\n",
    "        - e_running_max : list[float], running max E* at each look.\n",
    "        - p_always_valid : list[float], always-valid p-value at each look.\n",
    "    \"\"\"\n",
    "    look_sizes: list[int] = []\n",
    "    e_values: list[float] = []\n",
    "    e_running_max: list[float] = []\n",
    "    p_always_valid: list[float] = []\n",
    "\n",
    "    E_star = 0.0\n",
    "    rejected = False\n",
    "    stopping_n: int | None = None\n",
    "\n",
    "    n_total = df_stream.shape[0]\n",
    "    boundary = 1.0 / alpha\n",
    "\n",
    "    for n in range(look_step, n_total + 1, look_step):\n",
    "        sub = df_stream.iloc[:n]\n",
    "        tab = (\n",
    "            sub.groupby(\"group\")[\"converted\"]\n",
    "               .agg([\"sum\", \"count\"])\n",
    "               .rename(columns={\"sum\": \"x\", \"count\": \"n\"})\n",
    "        )\n",
    "        x1, n1 = int(tab.loc[\"control\", \"x\"]), int(tab.loc[\"control\", \"n\"])\n",
    "        x2, n2 = int(tab.loc[\"treatment\", \"x\"]), int(tab.loc[\"treatment\", \"n\"])\n",
    "\n",
    "        z = two_prop_z(x1, n1, x2, n2)\n",
    "        e = mixture_sprt_evalue(z, tau2=tau2)\n",
    "\n",
    "        E_star = max(E_star, e)\n",
    "        p_ev = min(1.0, 1.0 / E_star if E_star > 0.0 else 1.0)\n",
    "\n",
    "        look_sizes.append(n)\n",
    "        e_values.append(e)\n",
    "        e_running_max.append(E_star)\n",
    "        p_always_valid.append(p_ev)\n",
    "\n",
    "        if (not rejected) and (E_star >= boundary):\n",
    "            rejected = True\n",
    "            stopping_n = n\n",
    "\n",
    "    return {\n",
    "        \"rejected\": rejected,\n",
    "        \"stopping_n\": stopping_n,\n",
    "        \"look_sizes\": look_sizes,\n",
    "        \"e_values\": e_values,\n",
    "        \"e_running_max\": e_running_max,\n",
    "        \"p_always_valid\": p_always_valid,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f647b",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Comparing Type I error under A/A (no true effect)\n",
    "\n",
    "We now compare **false positive rates** of:\n",
    "\n",
    "1. Fixed-horizon z-test at sample size `n_total`.  \n",
    "2. Naive peeking (check p-value every `look_step` users, stop if p < α).  \n",
    "3. Always-valid mixture test (reject when e-process crosses 1/α).\n",
    "\n",
    "All experiments here are **A/A**: `p_control = p_treat = 0.10`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_type1_rates(\n",
    "    n_experiments: int = 500,\n",
    "    n_total: int = 10_000,\n",
    "    p: float = 0.10,\n",
    "    look_step: int = 500,\n",
    "    alpha: float = 0.05,\n",
    "    tau2: float = 1.0,\n",
    "    seed: int | None = 2025,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Estimate Type I error for three strategies via A/A simulations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_experiments : int\n",
    "        Number of simulated experiments.\n",
    "    n_total : int\n",
    "        Total users per experiment.\n",
    "    p : float\n",
    "        Conversion rate for both arms under H0.\n",
    "    look_step : int\n",
    "        Frequency of interim looks for peeking / always-valid test.\n",
    "    alpha : float\n",
    "        Nominal test level.\n",
    "    tau2 : float\n",
    "        Mixture prior variance for the e-value.\n",
    "    seed : int | None\n",
    "        Random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Approximate Type I error for each method.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    fixed_rejects = 0\n",
    "    peek_rejects = 0\n",
    "    always_rejects = 0\n",
    "\n",
    "    for _ in range(n_experiments):\n",
    "        s = int(rng.integers(0, 10_000_000))\n",
    "        df_stream = simulate_stream(n_total, p_control=p, p_treat=p, seed=s)\n",
    "\n",
    "        # 1) Fixed-horizon\n",
    "        p_fix = ztest_at_n(df_stream, n_total, two_sided=True)\n",
    "        if p_fix < alpha:\n",
    "            fixed_rejects += 1\n",
    "\n",
    "        # 2) Naive peeking\n",
    "        look_sizes, pvals = naive_peek_pvalues(df_stream, look_step=look_step, two_sided=True)\n",
    "        if any(pv < alpha for pv in pvals):\n",
    "            peek_rejects += 1\n",
    "\n",
    "        # 3) Always-valid mixture test\n",
    "        res_ev = always_valid_test_on_stream(\n",
    "            df_stream, look_step=look_step, alpha=alpha, tau2=tau2\n",
    "        )\n",
    "        if res_ev[\"rejected\"]:\n",
    "            always_rejects += 1\n",
    "\n",
    "    return {\n",
    "        \"fixed_horizon_alpha_hat\": fixed_rejects / n_experiments,\n",
    "        \"naive_peek_alpha_hat\": peek_rejects / n_experiments,\n",
    "        \"always_valid_alpha_hat\": always_rejects / n_experiments,\n",
    "    }\n",
    "\n",
    "\n",
    "type1_estimates = simulate_type1_rates(\n",
    "    n_experiments=300,\n",
    "    n_total=8000,\n",
    "    p=0.10,\n",
    "    look_step=400,\n",
    "    alpha=0.05,\n",
    "    tau2=1.0,\n",
    "    seed=2025,\n",
    ")\n",
    "type1_estimates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02340580",
   "metadata": {},
   "source": [
    "\n",
    "You should see something like:\n",
    "\n",
    "- `fixed_horizon_alpha_hat` ≈ 0.05 (close to nominal 5%).  \n",
    "- `naive_peek_alpha_hat` **well above** 0.05 (inflated false positives).  \n",
    "- `always_valid_alpha_hat` ≈ 0.05 (controlled type I error even with flexible stopping).\n",
    "\n",
    "This illustrates why naive peeking is dangerous, and how an always-valid test can\n",
    "restore valid inference while still allowing flexible stopping rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7944cce",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Power and stopping behavior under A/B (true effect)\n",
    "\n",
    "We now simulate experiments with a **real treatment effect**, e.g.:\n",
    "\n",
    "- `p_control = 0.10`,  \n",
    "- `p_treat   = 0.12` (absolute lift of 2 percentage points).\n",
    "\n",
    "We compare:\n",
    "\n",
    "- Probability of detection (power).  \n",
    "- Distribution of stopping sample sizes for the always-valid test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ec30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_power_and_stopping(\n",
    "    n_experiments: int = 300,\n",
    "    n_total: int = 10_000,\n",
    "    p_control: float = 0.10,\n",
    "    p_treat: float = 0.12,\n",
    "    look_step: int = 500,\n",
    "    alpha: float = 0.05,\n",
    "    tau2: float = 1.0,\n",
    "    seed: int | None = 42,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Compare power of fixed / naive / always-valid under a true effect.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_experiments : int\n",
    "        Number of simulated experiments.\n",
    "    n_total : int\n",
    "        Max users per experiment.\n",
    "    p_control, p_treat : float\n",
    "        Conversion probabilities in control and treatment (H1).\n",
    "    look_step : int\n",
    "        Frequency of interim looks.\n",
    "    alpha : float\n",
    "        Significance level.\n",
    "    tau2 : float\n",
    "        Mixture prior variance.\n",
    "    seed : int | None\n",
    "        Random seed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Contains detection rates and stopping size summaries.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    fixed_detect = 0\n",
    "    peek_detect = 0\n",
    "    always_detect = 0\n",
    "    always_stoppings: list[int] = []\n",
    "\n",
    "    for _ in range(n_experiments):\n",
    "        s = int(rng.integers(0, 10_000_000))\n",
    "        df_stream = simulate_stream(n_total, p_control=p_control, p_treat=p_treat, seed=s)\n",
    "\n",
    "        # Fixed-horizon\n",
    "        p_fix = ztest_at_n(df_stream, n_total, two_sided=True)\n",
    "        if p_fix < alpha:\n",
    "            fixed_detect += 1\n",
    "\n",
    "        # Naive peeking\n",
    "        _, pvals = naive_peek_pvalues(df_stream, look_step=look_step, two_sided=True)\n",
    "        if any(pv < alpha for pv in pvals):\n",
    "            peek_detect += 1\n",
    "\n",
    "        # Always-valid\n",
    "        res_ev = always_valid_test_on_stream(\n",
    "            df_stream, look_step=look_step, alpha=alpha, tau2=tau2\n",
    "        )\n",
    "        if res_ev[\"rejected\"]:\n",
    "            always_detect += 1\n",
    "            if res_ev[\"stopping_n\"] is not None:\n",
    "                always_stoppings.append(res_ev[\"stopping_n\"])\n",
    "        else:\n",
    "            # no detection, treat stopping at n_total\n",
    "            always_stoppings.append(n_total)\n",
    "\n",
    "    # Summaries\n",
    "    detection_rates = {\n",
    "        \"fixed_horizon_power\": fixed_detect / n_experiments,\n",
    "        \"naive_peek_power\": peek_detect / n_experiments,\n",
    "        \"always_valid_power\": always_detect / n_experiments,\n",
    "    }\n",
    "\n",
    "    stopping_series = pd.Series(always_stoppings)\n",
    "    stopping_summary = stopping_series.describe(percentiles=[0.25, 0.5, 0.75])\n",
    "\n",
    "    return {\n",
    "        \"detection_rates\": detection_rates,\n",
    "        \"always_valid_stopping_sizes\": stopping_series,\n",
    "        \"always_valid_stopping_summary\": stopping_summary,\n",
    "    }\n",
    "\n",
    "\n",
    "res_power = simulate_power_and_stopping(\n",
    "    n_experiments=300,\n",
    "    n_total=8000,\n",
    "    p_control=0.10,\n",
    "    p_treat=0.12,\n",
    "    look_step=400,\n",
    "    alpha=0.05,\n",
    "    tau2=1.0,\n",
    "    seed=777,\n",
    ")\n",
    "res_power[\"detection_rates\"], res_power[\"always_valid_stopping_summary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f845a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot distribution of stopping sample size for the always-valid test\n",
    "stopping_sizes = res_power[\"always_valid_stopping_sizes\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(stopping_sizes, bins=20)\n",
    "plt.xlabel(\"stopping sample size (n)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Always-valid test: distribution of stopping sample sizes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd6c3d",
   "metadata": {},
   "source": [
    "\n",
    "Typically you will see that:\n",
    "\n",
    "- The always-valid test detects the effect with **similar or slightly lower power**\n",
    "  compared to naive peeking at the same nominal α (because naive peeking is effectively\n",
    "  using a higher alpha in practice).  \n",
    "- Many experiments **stop early** when the effect is strong enough, saving traffic and time.  \n",
    "- The Type I error is still controlled at ≈ α by construction.\n",
    "\n",
    "This makes always-valid tests attractive for real growth teams:\n",
    "\n",
    "- You can monitor experiments continuously.  \n",
    "- You can make decisions as soon as evidence is strong enough.  \n",
    "- You keep a clean, interpretable notion of “5% false positive rate”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2c97c",
   "metadata": {},
   "source": [
    "\n",
    "## 7) How to use this in a real experimentation workflow\n",
    "\n",
    "Practical recipe for a growth / product team:\n",
    "\n",
    "1. **Choose a primary metric** (e.g. conversion).  \n",
    "2. Decide a maximum exposure `n_total` and a look frequency `look_step`.  \n",
    "3. Run an always-valid mixture test as in this notebook:\n",
    "   - Track `E*` and always-valid p-value `p_t`.  \n",
    "   - Stop and **declare success** once `p_t < α` and guardrails are acceptable.  \n",
    "4. If the experiment reaches `n_total` without crossing the boundary, treat the result as\n",
    "   **non-significant** and decide whether to hold, rerun, or accept a small/no effect.\n",
    "\n",
    "Extensions you can add on top:\n",
    "\n",
    "- Combine with **CUPED** or regression adjustment to reduce variance before computing z.  \n",
    "- Add **Bayesian decision layers** on top of the always-valid test (e.g., require a minimum\n",
    "  effect size in absolute terms).  \n",
    "- Log all z and e-process values to make post-hoc audits and dashboards.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}