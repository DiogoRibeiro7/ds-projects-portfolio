{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# MovieLens Recommender \u2013 Modern Libraries & Advanced Evaluation\n\nThis notebook pushes the MovieLens project further using **specialised\nrecommender libraries** and richer evaluation.\n\nWe will:\n\n1. Use **Surprise** to train strong classic models (SVD, KNNBaseline).\n2. Build an **implicit feedback** dataset and train a **LightFM** model.\n3. Evaluate both explicit and implicit models with:\n   - RMSE (ratings).\n   - Precision@K / Recall@K / Hit-rate (ranking).\n4. Inspect **embeddings** visually for items.\n\n> This notebook assumes you have already downloaded MovieLens\n> `ml-latest-small` and placed it under:\n>\n> ```text\n> data/ml-latest-small/ratings.csv\n> data/ml-latest-small/movies.csv   # optional but recommended\n> ```\n\n> It also assumes you can install extra packages in your environment\n> (Surprise, LightFM).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. Environment setup (to run outside this notebook)\n\nRun these commands in your environment **before** executing the notebook\ncells that import Surprise or LightFM:\n\n```bash\npip install scikit-surprise lightfm\n```\n\nIf you use conda:\n\n```bash\nconda install -c conda-forge scikit-surprise lightfm\n```\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration\n\nWe use:\n\n- `pandas`, `numpy` \u2013 general data handling.\n- `matplotlib`, `seaborn` \u2013 visualisations.\n- `surprise` \u2013 SVD and KNNBaseline models for explicit ratings.\n- `lightfm` \u2013 matrix factorisation model for implicit feedback.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, Iterable, List, Tuple, Set\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_DIR: Path = Path(\"data\") / \"ml-latest-small\"\nRATINGS_PATH: Path = DATA_DIR / \"ratings.csv\"\nMOVIES_PATH: Path = DATA_DIR / \"movies.csv\"\n\nif not RATINGS_PATH.exists():\n    raise FileNotFoundError(\n        f\"Ratings file not found at {RATINGS_PATH.resolve()}. \"\n        \"Please download MovieLens (ml-latest-small) and place ratings.csv under data/ml-latest-small/.\"\n    )\n\nratings_df = pd.read_csv(RATINGS_PATH)\nprint(\"Ratings shape:\", ratings_df.shape)\nratings_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Quick sanity checks and visuals similar to the previous notebook, but\nkept brief here to focus on modelling.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "n_users = ratings_df[\"userId\"].nunique()\nn_items = ratings_df[\"movieId\"].nunique()\n\nprint(f\"Users: {n_users}, Movies: {n_items}, Ratings: {len(ratings_df)}\")\nprint(f\"Density: {len(ratings_df) / (n_users * n_items):.6f}\")\n\nsns.histplot(ratings_df[\"rating\"], bins=10)\nplt.title(\"Rating distribution\")\nplt.xlabel(\"Rating\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Surprise \u2013 Explicit Feedback Models (SVD, KNNBaseline)\n\nThe **Surprise** library provides efficient implementations of many\nclassic collaborative filtering algorithms with convenient evaluation.\n\nWe will:\n\n1. Wrap MovieLens ratings into a Surprise `Dataset`.\n2. Train:\n   - **SVD** (matrix factorisation with biases).\n   - **KNNBaseline** (neighbour-based with baseline estimates).\n3. Evaluate **RMSE** and **MAE** on a hold-out set.\n4. Derive **ranking metrics** (Precision@K, Recall@K) from predictions.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from surprise import Dataset, Reader, SVD, KNNBaseline\nfrom surprise.model_selection import train_test_split as surprise_train_test_split\nfrom surprise import accuracy\n\n# Wrap DataFrame into Surprise Dataset\nreader = Reader(rating_scale=(ratings_df[\"rating\"].min(), ratings_df[\"rating\"].max()))\ndata = Dataset.load_from_df(ratings_df[[\"userId\", \"movieId\", \"rating\"]], reader)\n\ntrainset, testset = surprise_train_test_split(data, test_size=0.2, random_state=RANDOM_STATE)\nprint(\"Surprise train size:\", len(trainset.all_ratings()))\nprint(\"Surprise test size: \", len(testset))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.1 SVD model\n\nWe start with SVD, which is a strong baseline: latent factors + biases\ntrained with SGD.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "svd = SVD(\n    n_factors=80,\n    n_epochs=25,\n    lr_all=0.005,\n    reg_all=0.02,\n    random_state=RANDOM_STATE,\n)\n\nsvd.fit(trainset)\n\nsvd_predictions = svd.test(testset)\n\nrmse_svd = accuracy.rmse(svd_predictions, verbose=True)\nmae_svd = accuracy.mae(svd_predictions, verbose=True)\n\nprint(f\"SVD RMSE: {rmse_svd:.4f}, MAE: {mae_svd:.4f}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.2 KNNBaseline model (item-based)\n\nNext we use `KNNBaseline`, which combines:\n\n- Baseline estimates: \\( \\mu + b_u + b_i \\).\n- k-NN similarity on items.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "sim_options = {\n    \"name\": \"pearson_baseline\",\n    \"user_based\": False,  # item-based\n}\n\nknn_baseline = KNNBaseline(\n    k=40,\n    min_k=2,\n    sim_options=sim_options,\n)\n\nknn_baseline.fit(trainset)\nknn_predictions = knn_baseline.test(testset)\n\nrmse_knn = accuracy.rmse(knn_predictions, verbose=True)\nmae_knn = accuracy.mae(knn_predictions, verbose=True)\n\nprint(f\"KNNBaseline RMSE: {rmse_knn:.4f}, MAE: {mae_knn:.4f}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Visual comparison (RMSE)\n\nrmse_df = pd.DataFrame(\n    {\n        \"model\": [\"SVD\", \"KNNBaseline\"],\n        \"rmse\": [rmse_svd, rmse_knn],\n    }\n)\n\nsns.barplot(data=rmse_df, x=\"model\", y=\"rmse\")\nplt.title(\"Surprise models \u2013 RMSE\")\nplt.ylabel(\"RMSE\")\nplt.show()\n\nrmse_df.sort_values(\"rmse\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.3 Ranking metrics from Surprise predictions\n\nSurprise focuses on rating prediction, but we can **post-process** its\npredictions to compute ranking metrics.\n\nStrategy:\n\n- For each user in the test set:\n  - Consider items in the test set as candidate items.\n  - Treat ratings \u2265 4.0 as relevant.\n  - Use SVD predictions to rank items.\n  - Compute Precision@K / Recall@K / Hit-rate.\n\nThis focuses on **generalisation** to seen-in-test items. A more realistic\nsetting would use all unseen items as candidates.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from collections import defaultdict\n\n\ndef build_user_item_true_pred(\n    predictions,\n    rel_threshold: float = 4.0,\n) -> Dict[int, Dict[str, Dict[int, float]]]:\n    \"\"\"Organise Surprise predictions by user.\n\n    Args:\n        predictions: List of Surprise Prediction objects.\n        rel_threshold: Threshold for relevance.\n\n    Returns:\n        Nested dict: user -> {\"true\": {item: rating}, \"pred\": {item: est}}.\n    \"\"\"\n    data: Dict[int, Dict[str, Dict[int, float]]] = defaultdict(lambda: {\"true\": {}, \"pred\": {}})\n    for pred in predictions:\n        uid = int(pred.uid)\n        iid = int(pred.iid)\n        true_r = float(pred.r_ui)\n        est_r = float(pred.est)\n        data[uid][\"true\"][iid] = true_r\n        data[uid][\"pred\"][iid] = est_r\n    return data\n\n\ndef ranking_metrics_from_predictions(\n    predictions,\n    k: int = 10,\n    rel_threshold: float = 4.0,\n) -> Dict[str, float]:\n    \"\"\"Compute hit-rate, precision@K, recall@K from Surprise predictions.\n\n    Args:\n        predictions: List of Surprise Prediction objects.\n        k: Cutoff for top-K.\n        rel_threshold: Relevance threshold on true ratings.\n\n    Returns:\n        Dict with metrics.\n    \"\"\"\n    data = build_user_item_true_pred(predictions, rel_threshold=rel_threshold)\n\n    hits = 0\n    total_users_with_rel = 0\n    sum_precision = 0.0\n    sum_recall = 0.0\n    n_eval_users = 0\n\n    for uid, d in data.items():\n        true_dict = d[\"true\"]\n        pred_dict = d[\"pred\"]\n\n        relevant_items: Set[int] = {i for i, r in true_dict.items() if r >= rel_threshold}\n        if not relevant_items:\n            continue\n\n        total_users_with_rel += 1\n        n_eval_users += 1\n\n        # Rank items by predicted rating (descending)\n        sorted_items = sorted(pred_dict.items(), key=lambda x: x[1], reverse=True)\n        top_items = [iid for iid, _ in sorted_items[:k]]\n        top_set = set(top_items)\n\n        n_rel_in_top = len(top_set & relevant_items)\n\n        if n_rel_in_top > 0:\n            hits += 1\n\n        precision_u = n_rel_in_top / k\n        recall_u = n_rel_in_top / len(relevant_items)\n\n        sum_precision += precision_u\n        sum_recall += recall_u\n\n    if n_eval_users == 0:\n        raise ValueError(\"No users with relevant items for ranking metrics.\")\n\n    hit_rate = hits / total_users_with_rel\n    precision_at_k = sum_precision / n_eval_users\n    recall_at_k = sum_recall / n_eval_users\n\n    return {\n        \"hit_rate\": hit_rate,\n        \"precision_at_k\": precision_at_k,\n        \"recall_at_k\": recall_at_k,\n        \"n_eval_users\": float(n_eval_users),\n    }\n\n\nsvd_rank_metrics_k10 = ranking_metrics_from_predictions(svd_predictions, k=10, rel_threshold=4.0)\nknn_rank_metrics_k10 = ranking_metrics_from_predictions(knn_predictions, k=10, rel_threshold=4.0)\n\nsvd_rank_metrics_k10, knn_rank_metrics_k10\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Visual comparison of ranking metrics (K=10)\n\nmetrics_svd = pd.DataFrame(\n    {\n        \"metric\": [\"hit_rate\", \"precision_at_k\", \"recall_at_k\"],\n        \"value\": [\n            svd_rank_metrics_k10[\"hit_rate\"],\n            svd_rank_metrics_k10[\"precision_at_k\"],\n            svd_rank_metrics_k10[\"recall_at_k\"],\n        ],\n    }\n)\nmetrics_svd[\"model\"] = \"SVD\"\n\nmetrics_knn = pd.DataFrame(\n    {\n        \"metric\": [\"hit_rate\", \"precision_at_k\", \"recall_at_k\"],\n        \"value\": [\n            knn_rank_metrics_k10[\"hit_rate\"],\n            knn_rank_metrics_k10[\"precision_at_k\"],\n            knn_rank_metrics_k10[\"recall_at_k\"],\n        ],\n    }\n)\nmetrics_knn[\"model\"] = \"KNNBaseline\"\n\nmetrics_all = pd.concat([metrics_svd, metrics_knn], axis=0)\n\nsns.barplot(data=metrics_all, x=\"metric\", y=\"value\", hue=\"model\")\nplt.ylim(0, 1)\nplt.title(\"Surprise models \u2013 ranking metrics (K=10)\")\nplt.ylabel(\"Score\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Surprise gives us strong baselines with relatively little code. Next we\nshift to **implicit feedback**, which is closer to many production setups.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. LightFM \u2013 Implicit Feedback Model\n\nMany modern recommenders are based on **implicit feedback**:\n\n- Views, clicks, watches, purchases.\n- No explicit ratings, but we interpret interactions as *positive signals*.\n\nTo approximate this, we convert MovieLens ratings into implicit events:\n\n- Consider ratings \u2265 4.0 as **positive interactions**.\n- Build a sparse user\u2013item matrix of 1s (interaction) and 0s.\n\nWe then train a **LightFM** model with a ranking loss (BPR).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from lightfm import LightFM\nfrom lightfm.data import Dataset as LFMDataset\nfrom lightfm.evaluation import precision_at_k as lfm_precision_at_k, recall_at_k as lfm_recall_at_k\n\n# Threshold for positive interaction\nPOS_THRESH: float = 4.0\n\nimplicit_df = ratings_df.copy()\nimplicit_df[\"interaction\"] = (implicit_df[\"rating\"] >= POS_THRESH).astype(int)\n\nprint(implicit_df[\"interaction\"].value_counts(normalize=True))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Build LightFM Dataset object\n\nlfm_dataset = LFMDataset()\nlfm_dataset.fit(\n    users=implicit_df[\"userId\"].unique(),\n    items=implicit_df[\"movieId\"].unique(),\n)\n\n# Build interactions matrix\n(interactions, weights) = lfm_dataset.build_interactions(\n    (row.userId, row.movieId) for row in implicit_df.itertuples(index=False) if row.interaction == 1\n)\n\nprint(\"Interactions shape:\", interactions.shape)\nprint(\"Num non-zero interactions:\", interactions.getnnz())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We now train a LightFM model with **BPR** (Bayesian Personalised Ranking)\nloss, which is commonly used for implicit recommenders.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Train-test split for implicit data\n\nfrom lightfm.cross_validation import random_train_test_split\n\nlfm_train, lfm_test = random_train_test_split(\n    interactions,\n    test_percentage=0.2,\n    random_state=np.random.RandomState(RANDOM_STATE),\n)\n\nprint(\"Train interactions:\", lfm_train.getnnz())\nprint(\"Test interactions: \", lfm_test.getnnz())\n\nlfm_model = LightFM(\n    no_components=40,\n    loss=\"bpr\",\n    learning_rate=0.05,\n    random_state=RANDOM_STATE,\n)\n\nlfm_model.fit(\n    lfm_train,\n    epochs=25,\n    num_threads=4,\n    verbose=True,\n)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.1 Evaluate LightFM with ranking metrics\n\nLightFM comes with `precision_at_k` and `recall_at_k` evaluators that work\non the interaction matrices.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "k_eval = 10\n\nprecision_lfm = lfm_precision_at_k(\n    lfm_model,\n    lfm_test,\n    train_interactions=lfm_train,\n    k=k_eval,\n    num_threads=4,\n).mean()\n\nrecall_lfm = lfm_recall_at_k(\n    lfm_model,\n    lfm_test,\n    train_interactions=lfm_train,\n    k=k_eval,\n    num_threads=4,\n).mean()\n\nprint(f\"LightFM (BPR) precision@{k_eval}: {precision_lfm:.4f}\")\nprint(f\"LightFM (BPR) recall@{k_eval}:    {recall_lfm:.4f}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Compare LightFM ranking metrics with Surprise SVD ranking metrics (approx)\n\nranking_compare_df = pd.DataFrame(\n    {\n        \"model\": [\"SVD (explicit)\", \"LightFM (implicit)\"],\n        \"precision_at_10\": [svd_rank_metrics_k10[\"precision_at_k\"], precision_lfm],\n        \"recall_at_10\": [svd_rank_metrics_k10[\"recall_at_k\"], recall_lfm],\n    }\n)\n\nranking_compare_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "ranking_melt = ranking_compare_df.melt(id_vars=\"model\", var_name=\"metric\", value_name=\"value\")\n\nsns.barplot(data=ranking_melt, x=\"metric\", y=\"value\", hue=\"model\")\nplt.ylim(0, 1)\nplt.title(\"Ranking metrics \u2013 explicit vs implicit models\")\nplt.ylabel(\"Score\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Comparing explicit SVD and implicit LightFM is not strictly apples-to-apples,\nbut this gives a sense of how both behave as rankers.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Visualising item embeddings (LightFM)\n\nModern recommender models often learn **embedding vectors** for users and\nitems. We can visualize item embeddings to inspect the learned structure.\n\nWe will:\n\n1. Extract item embedding matrix from LightFM.\n2. Reduce to 2D via t-SNE or PCA.\n3. Plot a sample of movies with labels (if `movies.csv` exists).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\n# Get item embeddings (latent factors)\nitem_embeddings = lfm_model.get_item_representations()[1]  # (n_items, no_components)\n\nprint(\"Item embeddings shape:\", item_embeddings.shape)\n\n# First reduce with PCA for stability, then t-SNE for 2D\npca = PCA(n_components=min(20, item_embeddings.shape[1]))\nitem_pca = pca.fit_transform(item_embeddings)\n\n# t-SNE in 2D (be careful with runtime for large n_items; we sample)\nmax_items_viz = 500\nn_items_total = item_pca.shape[0]\nindices = np.random.choice(n_items_total, size=min(max_items_viz, n_items_total), replace=False)\n\nitem_pca_sample = item_pca[indices]\n\ntsne = TSNE(n_components=2, init=\"pca\", random_state=RANDOM_STATE, perplexity=30.0)\nitem_tsne = tsne.fit_transform(item_pca_sample)\n\nemb_df = pd.DataFrame(item_tsne, columns=[\"x\", \"y\"])\nemb_df[\"item_internal_id\"] = indices\n\nsns.scatterplot(data=emb_df, x=\"x\", y=\"y\", s=20, alpha=0.7)\nplt.title(\"LightFM item embeddings (t-SNE, sample)\")\nplt.xlabel(\"dim 1\")\nplt.ylabel(\"dim 2\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "If you want to annotate a few points with movie titles, you can match\nLightFM's internal item ids to `movieId`s. Below we show how to\napproximate that.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Map internal LightFM item ids back to movieIds\n\nitem_id_map, _ = lfm_dataset.mapping()[1]  # dict: movieId -> internal_id\n\n# Reverse mapping\ninternal_to_movie = {internal: movie for movie, internal in item_id_map.items()}\n\nemb_df[\"movieId\"] = emb_df[\"item_internal_id\"].map(internal_to_movie)\n\nif MOVIES_PATH.exists():\n    movies_df = pd.read_csv(MOVIES_PATH)\n    emb_with_titles = emb_df.merge(movies_df[[\"movieId\", \"title\"]], on=\"movieId\", how=\"left\")\n\n    # Show a few random points with titles to inspect\n    emb_with_titles.sample(10, random_state=RANDOM_STATE)[[\"movieId\", \"title\", \"x\", \"y\"]]\nelse:\n    print(\"movies.csv not found; cannot add titles to embeddings.\")\n    emb_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "You can also manually pick a **cluster** in the embedding plot and inspect\ntitles within it to see if they are semantically related.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Putting it together \u2013 model comparison summary\n\nWe now summarise the main models from this notebook:\n\n- **Surprise SVD** \u2013 explicit MF, strong RMSE + decent ranking.\n- **Surprise KNNBaseline** \u2013 baseline + neighbours.\n- **LightFM (BPR)** \u2013 implicit MF optimised for ranking.\n\nYou can combine ideas:\n\n- Use explicit SVD for rating prediction tasks.\n- Use LightFM or other implicit methods for large\u2013scale ranking.\n- Mix collaborative and content features (LightFM supports that).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5.1 Simple metric table\n\nBelow is a small template; adapt as you run the notebook and capture\nactual numbers from your execution.\n\n```text\nModel                | Type        | RMSE (explicit) | Precision@10 | Recall@10\n---------------------|------------|-----------------|-------------|----------\nSVD (Surprise)       | explicit    | ~...            | ~...        | ~...\nKNNBaseline (Surprise)| explicit   | ~...            | ~...        | ~...\nLightFM (BPR)        | implicit    | n/a             | ~...        | ~...\n```\n\nThe exact values will depend on random seeds and hyperparameters.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Where to go next\n\nYou now have:\n\n- A **manual, from-scratch** recommender notebook (baseline + MF) and\n  ranking evaluation.\n- A **modern-libraries** notebook using Surprise and LightFM, with\n  embedding visualisations.\n\nNatural next steps:\n\n1. **Hyperparameter search**\n   - Use Surprise's cross-validation utilities to tune SVD / KNN.\n   - Tune LightFM `no_components`, `learning_rate`, epoch count, and loss\n     (e.g. WARP vs BPR).\n\n2. **Hybrid models**\n   - Add movie genres or tags as item features in LightFM.\n   - Add user features (age, location, segments).\n\n3. **Sequence-aware models**\n   - For more recent datasets with timestamps, build session-based\n     recommenders using RNNs or Transformers (e.g. `RecBole`, `transformers4rec`).\n\n4. **Productionisation**\n   - Export embeddings and scores.\n   - Build a simple API for top-N recommendations.\n   - Integrate with a UI or downstream pipeline.\n\nThis notebook completes a more **modern stack** for recommender systems\nwhile keeping the code transparent and editable.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}