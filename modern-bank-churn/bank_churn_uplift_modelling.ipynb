{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Bank Customer Churn \u2013 Uplift Modelling for Retention Campaigns (Simulated RCT)\n\nThis notebook is a **new churn project** focused on **uplift modelling**.\n\nInstead of only predicting **who will churn**, we want to estimate **who will be\npositively influenced by a retention campaign**.\n\nBecause the `Churn_Modelling.csv` dataset does **not** contain an actual\nexperiment, we:\n\n- **Simulate** a retention campaign as a randomised experiment (treatment vs control).\n- Construct **potential outcomes** with heterogeneous treatment effects.\n- Fit an **uplift model** to learn where the campaign works best.\n\nHigh-level steps:\n\n1. Load and clean the bank churn dataset.\n2. Simulate a randomised retention campaign (treatment assignment).\n3. Simulate heterogeneous treatment effects and an observed post-campaign churn.\n4. Estimate average treatment effects (ATE) and segment-level effects.\n5. Train an uplift model using the **two-model approach** (treated vs control models).\n6. Evaluate uplift by **uplift-by-quantile** analysis.\n7. Discuss how to use uplift scores to target future retention campaigns.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Imports and configuration\n\nWe use:\n\n- `pandas`, `numpy` for data handling.\n- `matplotlib`, `seaborn` for plots.\n- `scikit-learn` for modelling.\n\nWe assume the dataset is available at:\n\n```text\ndata/Churn_Modelling.csv\n```\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    roc_auc_score,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (8, 5)\n\nRANDOM_STATE: int = 42\nnp.random.seed(RANDOM_STATE)\n\nDATA_PATH: Path = Path(\"data\") / \"Churn_Modelling.csv\"\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Data file not found at {DATA_PATH.resolve()}. \"\n        \"Please download the Bank Customer Churn CSV and place it under the 'data/' directory.\"\n    )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Load and clean the data\n\nWe load the bank churn dataset and perform basic cleaning:\n\n- Drop identifier columns.\n- Ensure `Exited` exists and is encoded as 0/1.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def load_bank_churn_data(path: Path) -> pd.DataFrame:\n    \"\"\"Load the bank customer churn dataset from a CSV file.\n\n    Args:\n        path: Path to the CSV file.\n\n    Returns:\n        DataFrame containing the bank churn data.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the loaded DataFrame is empty.\n    \"\"\"\n    if not path.exists():\n        raise FileNotFoundError(f\"File not found: {path!s}\")\n\n    df: pd.DataFrame = pd.read_csv(path)\n    if df.empty:\n        raise ValueError(f\"Loaded DataFrame is empty: {path!s}\")\n\n    return df\n\n\ndef clean_bank_churn_data(raw_df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Clean the bank customer churn dataset.\n\n    - Drop identifier columns.\n    - Ensure `Exited` exists and is integer 0/1.\n\n    Args:\n        raw_df: Raw bank churn DataFrame.\n\n    Returns:\n        Cleaned DataFrame.\n    \"\"\"\n    df = raw_df.copy()\n\n    id_cols: List[str] = [\"RowNumber\", \"CustomerId\", \"Surname\"]\n    drop_cols: List[str] = [c for c in id_cols if c in df.columns]\n    if drop_cols:\n        df = df.drop(columns=drop_cols)\n        print(f\"Dropped identifier columns: {drop_cols}\")\n\n    if \"Exited\" not in df.columns:\n        raise ValueError(\"Target column 'Exited' not found in DataFrame.\")\n\n    df[\"Exited\"] = df[\"Exited\"].astype(int)\n\n    # Show any missing values\n    missing = df.isna().sum()\n    print(\"Missing values per column (non-zero only):\")\n    display(missing[missing > 0])\n\n    return df\n\n\nraw_df: pd.DataFrame = load_bank_churn_data(DATA_PATH)\ndf: pd.DataFrame = clean_bank_churn_data(raw_df)\n\nprint(\"Shape:\", df.shape)\nprint(df[\"Exited\"].value_counts(normalize=True).rename(\"Exited proportion\"))\n\ndisplay(df.head())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We have a snapshot of customers, some of whom **exited** (churned) and others\nwho stayed. Next we simulate a **retention campaign experiment** on top of this.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Simulating a retention campaign (randomised experiment)\n\nThe dataset does not have an actual campaign, so we **simulate** one.\n\nConceptually, in a real RCT (randomised controlled trial):\n\n- Each customer is randomly assigned to:\n  - **Treatment** (`treatment = 1`): receives a retention offer.\n  - **Control** (`treatment = 0`): no offer.\n- After some time, we observe whether they churned (`churn_after_campaign`).\n\nTo simulate this realistically, we:\n\n1. Build a **baseline churn risk model** from the original data.\n2. Define a **heterogeneous treatment effect** `tau(x)` based on customer features.\n3. For each customer construct potential churn probabilities:\n   - `p_control(x)` \u2013 probability of churn with no offer.\n   - `p_treated(x)` \u2013 probability of churn with an offer.\n4. Randomly assign treatment and sample an observed outcome from the\n   appropriate Bernoulli distribution.\n\nThis gives us synthetic RCT data `(X, treatment, churn_observed)` with known\n\"true\" individual-level treatment effects.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.1 Baseline churn risk model\n\nWe first fit a simple **Logistic Regression** as a baseline risk model\n`P(Exited = 1 | X)`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "TARGET_COL: str = \"Exited\"\n\nX_all: pd.DataFrame = df.drop(columns=[TARGET_COL])\ny_all: pd.Series = df[TARGET_COL]\n\n# Define categorical and numeric columns\ncategorical_cols: List[str] = [c for c in [\"Geography\", \"Gender\"] if c in X_all.columns]\nnumeric_cols: List[str] = [c for c in X_all.columns if c not in categorical_cols]\n\nprint(\"Categorical columns:\", categorical_cols)\nprint(\"Numeric columns:\", numeric_cols)\n\nnumeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n\ncategorical_transformer = Pipeline(\n    steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_cols),\n        (\"cat\", categorical_transformer, categorical_cols),\n    ]\n)\n\nbaseline_log_reg = Pipeline(\n    steps=[\n        (\"preprocess\", preprocessor),\n        (\n            \"clf\",\n            LogisticRegression(\n                max_iter=1000,\n                random_state=RANDOM_STATE,\n                n_jobs=-1,\n            ),\n        ),\n    ]\n)\n\n# Fit baseline model on all data (for simulation purposes)\nbaseline_log_reg.fit(X_all, y_all)\n\np_base: np.ndarray = baseline_log_reg.predict_proba(X_all)[:, 1]\n\nprint(\"Baseline churn probability summary:\")\nprint(pd.Series(p_base).describe())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.2 Define heterogeneous treatment effects\n\nWe now define a simple **treatment effect function** `tau(x)`:\n\n- Base effect: the campaign reduces churn by ~5 percentage points.\n- Larger effect for certain profiles, for example:\n  - Customers with **2+ products** and **active** \u2192 +10 points uplift (campaign works best).\n  - Customers with very low **credit score** \u2192 almost no effect.\n\nWe implement this by:\n\n- Starting from the baseline probability `p_base`.\n- Constructing `p_control` and `p_treated` per customer.\n- Ensuring probabilities stay in `[0.01, 0.99]`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def compute_potential_outcomes(\n    df_features: pd.DataFrame,\n    base_probs: np.ndarray,\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Compute potential churn probabilities under control and treatment.\n\n    The function encodes a simple heterogeneous treatment effect rule.\n\n    Args:\n        df_features: Original feature DataFrame (without target).\n        base_probs: Baseline churn probabilities from a model.\n\n    Returns:\n        Tuple of (p_control, p_treated, true_ite) arrays.\n        - p_control: churn prob with no campaign.\n        - p_treated: churn prob with campaign.\n        - true_ite: p_control - p_treated (reduction in churn due to campaign).\n    \"\"\"\n    p_base = np.asarray(base_probs, dtype=float)\n\n    # Start from baseline as control probability\n    p_control = np.clip(p_base, 0.01, 0.99)\n\n    # Base treatment effect (absolute reduction in churn probability)\n    base_effect = 0.05  # 5 percentage points\n\n    # Extra effect for certain segments\n    extra_effect = np.zeros_like(p_control)\n\n    # Customers with 2+ products and active membership respond well\n    if {\"NumOfProducts\", \"IsActiveMember\"}.issubset(df_features.columns):\n        mask_good_segment = (\n            (df_features[\"NumOfProducts\"] >= 2) & (df_features[\"IsActiveMember\"] == 1)\n        )\n        extra_effect[mask_good_segment.to_numpy()] += 0.10\n\n    # Customers with very low credit score respond less\n    if \"CreditScore\" in df_features.columns:\n        mask_low_score = df_features[\"CreditScore\"] < 500\n        extra_effect[mask_low_score.to_numpy()] -= 0.03  # reduce effect\n\n    # Treatment effect for each individual\n    tau = base_effect + extra_effect\n    tau = np.clip(tau, 0.0, 0.3)  # cap effect\n\n    p_treated = np.clip(p_control - tau, 0.01, 0.99)\n\n    true_ite = p_control - p_treated  # positive = reduction in churn\n\n    return p_control, p_treated, true_ite\n\n\np_control, p_treated, true_ite = compute_potential_outcomes(df, p_base)\n\nprint(\"True ITE (p_control - p_treated) summary:\")\nprint(pd.Series(true_ite).describe())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.3 Simulate treatment assignment and observed outcomes\n\nWe now:\n\n1. Assign each customer **randomly** to treatment or control (`p=0.5`).\n2. For each customer:\n   - If `treatment = 0`, draw churn from `Bernoulli(p_control)`.\n   - If `treatment = 1`, draw churn from `Bernoulli(p_treated)`.\n\nThis gives us:\n\n- `treatment` \u2013 0 or 1.\n- `churn_observed` \u2013 0 (stayed) or 1 (churned) **after** the simulated campaign.\n\nWe also keep `p_control`, `p_treated`, and `true_ite` for evaluation.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "n: int = df.shape[0]\n\n# Randomised treatment assignment\np_treat: float = 0.5\ntreatment = np.random.binomial(1, p_treat, size=n)\n\n# Observed churn after campaign\nchurn_observed = np.where(\n    treatment == 1,\n    np.random.binomial(1, p_treated),\n    np.random.binomial(1, p_control),\n)\n\nsim_df = df.copy()\nsim_df[\"treatment\"] = treatment\nsim_df[\"churn_observed\"] = churn_observed\nsim_df[\"p_control\"] = p_control\nsim_df[\"p_treated\"] = p_treated\nsim_df[\"true_ite\"] = true_ite\n\nsim_df.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.4 Quick randomisation and effect checks\n\nWe check:\n\n- Whether treatment assignment is balanced.\n- The **true ATE** implied by our potential outcomes.\n- The **empirical ATE** estimated from the simulated data.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Treatment balance\nprint(sim_df[\"treatment\"].value_counts(normalize=True).rename(\"treatment_share\"))\n\n# True ATE from potential outcomes\ntrue_ate = float(sim_df[\"true_ite\"].mean())\nprint(f\"True ATE (expected churn reduction): {true_ate:.4f}\")\n\n# Empirical ATE from simulated outcomes\nemp_ate = float(sim_df.loc[sim_df[\"treatment\"] == 1, \"churn_observed\"].mean() -\n                sim_df.loc[sim_df[\"treatment\"] == 0, \"churn_observed\"].mean())\n\nprint(f\"Empirical ATE (treatment - control churn): {emp_ate:.4f}\")\nprint(f\"Empirical churn reduction (control - treatment): {-emp_ate:.4f}\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We now have a synthetic **RCT** with heterogeneous uplift.\n\nNext, we build an uplift model to **learn where the campaign works best**.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Train\u2013test split for uplift modelling\n\nWe want to train an uplift model and evaluate it on a **hold-out test set**.\n\nWe split the simulated dataset into train and test, keeping:\n\n- Features `X` (original columns, excluding `Exited`).\n- Treatment indicator `treatment`.\n- Outcome `churn_observed`.\n- `true_ite` for evaluation only.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Features for uplift modelling (exclude original Exited and uplift-specific columns)\nX_uplift = sim_df.drop(columns=[\n    \"Exited\",\n    \"churn_observed\",\n    \"treatment\",\n    \"p_control\",\n    \"p_treated\",\n    \"true_ite\",\n])\n\ny_uplift = sim_df[\"churn_observed\"]  # post-campaign churn\nT_uplift = sim_df[\"treatment\"]\ntrue_ite_all = sim_df[\"true_ite\"]\n\nX_train, X_test, y_train, y_test, T_train, T_test, ite_train, ite_test = train_test_split(\n    X_uplift,\n    y_uplift,\n    T_uplift,\n    true_ite_all,\n    test_size=0.3,\n    random_state=RANDOM_STATE,\n    stratify=T_uplift,\n)\n\nprint(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We reuse the same **preprocessor** (numeric + categorical) as before.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "categorical_cols_uplift: List[str] = [c for c in [\"Geography\", \"Gender\"] if c in X_uplift.columns]\nnumeric_cols_uplift: List[str] = [c for c in X_uplift.columns if c not in categorical_cols_uplift]\n\nprint(\"Categorical cols (uplift):\", categorical_cols_uplift)\nprint(\"Numeric cols (uplift):\", numeric_cols_uplift)\n\nnumeric_transformer_u = Pipeline(steps=[(\"scaler\", StandardScaler())])\n\ncategorical_transformer_u = Pipeline(\n    steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n)\n\npreprocessor_u = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer_u, numeric_cols_uplift),\n        (\"cat\", categorical_transformer_u, categorical_cols_uplift),\n    ]\n)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Uplift model \u2013 two-model approach\n\nThere are several uplift modelling strategies. Here we use the classic\n**two-model approach**:\n\n1. Train one model on the **treated** group: `P(Y = 1 | X, treatment = 1)`.\n2. Train another model on the **control** group: `P(Y = 1 | X, treatment = 0)`.\n3. For a new customer with features `x`, estimate:\n   - `p_treated_hat(x)` and `p_control_hat(x)`.\n   - Uplift in terms of **churn reduction**:\n\n   ```text\n   uplift_hat(x) = p_control_hat(x) - p_treated_hat(x)\n   ```\n\nA large **positive** `uplift_hat(x)` means:\n\n> \"This customer is expected to churn much less if we treat them than if we do nothing.\"  \n> \u2192 **High-priority target** for retention campaigns.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def fit_two_model_uplift(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    T_train: pd.Series,\n    preprocessor: ColumnTransformer,\n) -> Tuple[Pipeline, Pipeline]:\n    \"\"\"Fit two separate models: one for treated, one for control.\n\n    Args:\n        X_train: Training features.\n        y_train: Training outcome (0/1 churn).\n        T_train: Treatment indicator for training rows.\n        preprocessor: ColumnTransformer for preprocessing.\n\n    Returns:\n        Tuple of (model_control, model_treated) pipelines.\n    \"\"\"\n    treated_mask = T_train == 1\n    control_mask = T_train == 0\n\n    X_treated = X_train.loc[treated_mask]\n    y_treated = y_train.loc[treated_mask]\n\n    X_control = X_train.loc[control_mask]\n    y_control = y_train.loc[control_mask]\n\n    print(\"Treated samples:\", X_treated.shape[0])\n    print(\"Control samples:\", X_control.shape[0])\n\n    model_treated = Pipeline(\n        steps=[\n            (\"preprocess\", preprocessor),\n            (\n                \"clf\",\n                RandomForestClassifier(\n                    n_estimators=300,\n                    max_depth=None,\n                    min_samples_split=4,\n                    min_samples_leaf=2,\n                    random_state=RANDOM_STATE,\n                    n_jobs=-1,\n                ),\n            ),\n        ]\n    )\n\n    model_control = Pipeline(\n        steps=[\n            (\"preprocess\", preprocessor),\n            (\n                \"clf\",\n                RandomForestClassifier(\n                    n_estimators=300,\n                    max_depth=None,\n                    min_samples_split=4,\n                    min_samples_leaf=2,\n                    random_state=RANDOM_STATE + 1,\n                    n_jobs=-1,\n                ),\n            ),\n        ]\n    )\n\n    model_treated.fit(X_treated, y_treated)\n    model_control.fit(X_control, y_control)\n\n    return model_control, model_treated\n\n\nmodel_control, model_treated = fit_two_model_uplift(\n    X_train=X_train,\n    y_train=y_train,\n    T_train=T_train,\n    preprocessor=preprocessor_u,\n)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def predict_uplift(\n    model_control: Pipeline,\n    model_treated: Pipeline,\n    X: pd.DataFrame,\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Predict control and treated churn probabilities and uplift.\n\n    Args:\n        model_control: Model trained on control group.\n        model_treated: Model trained on treated group.\n        X: Feature DataFrame for which to predict uplift.\n\n    Returns:\n        Tuple of (p_control_hat, p_treated_hat, uplift_hat) arrays.\n        uplift_hat = p_control_hat - p_treated_hat (expected churn reduction).\n    \"\"\"\n    p_control_hat = model_control.predict_proba(X)[:, 1]\n    p_treated_hat = model_treated.predict_proba(X)[:, 1]\n\n    uplift_hat = p_control_hat - p_treated_hat\n\n    return p_control_hat, p_treated_hat, uplift_hat\n\n\np0_hat_test, p1_hat_test, uplift_hat_test = predict_uplift(\n    model_control, model_treated, X_test\n)\n\nprint(\"Predicted uplift summary:\")\nprint(pd.Series(uplift_hat_test).describe())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We now have, for each test customer:\n\n- `p0_hat_test` \u2013 predicted churn probability if **not treated**.\n- `p1_hat_test` \u2013 predicted churn probability if **treated**.\n- `uplift_hat_test` \u2013 estimated **reduction in churn** if treated.\n\nWe can compare this to the **true individual treatment effect** `true_ite` used\nin the simulation.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Compare predicted uplift to true ITE (simulation ground truth)\n\nfrom scipy.stats import pearsonr\n\ncorr, p_value = pearsonr(uplift_hat_test, ite_test.to_numpy())\nprint(f\"Correlation between predicted uplift and true ITE: {corr:.3f} (p={p_value:.3g})\")\n\nplt.scatter(ite_test, uplift_hat_test, alpha=0.3)\nplt.xlabel(\"True ITE (churn reduction)\")\nplt.ylabel(\"Predicted uplift (churn reduction)\")\nplt.title(\"Predicted uplift vs simulated ground truth\")\nplt.axhline(0, color=\"black\", linestyle=\"--\")\nplt.axvline(0, color=\"black\", linestyle=\"--\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "A positive correlation means the uplift model is **learning something** about\nwhere the campaign works better or worse.\n\nHowever, uplift is usually evaluated not just by correlation, but by how well\nit **concentrates treatment effect** in the top-ranked customers.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Uplift-by-quantile analysis\n\nA practical way to inspect uplift is to:\n\n1. Rank customers in the **test set** by their predicted uplift.\n2. Split them into quantiles (e.g., 5 or 10 groups).\n3. For each quantile:\n   - Compute **observed churn rates** for treated vs control customers.\n   - Estimate **observed uplift**:\n\n   ```text\n   observed_uplift = churn_rate_control - churn_rate_treated\n   ```\n\nIf the model is useful, the **top uplift quantiles** should show the **largest\nobserved uplift** (largest churn reduction when treated).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def uplift_by_quantile(\n    uplift_scores: np.ndarray,\n    y: pd.Series,\n    T: pd.Series,\n    n_quantiles: int = 5,\n) -> pd.DataFrame:\n    \"\"\"Compute observed uplift by quantile of predicted uplift.\n\n    Args:\n        uplift_scores: Array of predicted uplift scores (higher is better).\n        y: Observed outcome (1 = churn).\n        T: Treatment indicator (1 = treated, 0 = control).\n        n_quantiles: Number of quantile bins.\n\n    Returns:\n        DataFrame with metrics per quantile.\n    \"\"\"\n    df_local = pd.DataFrame(\n        {\n            \"uplift_hat\": uplift_scores,\n            \"y\": y.to_numpy(),\n            \"T\": T.to_numpy(),\n        }\n    )\n\n    # Higher uplift = better \u2192 we rank descending\n    df_local[\"quantile\"] = pd.qcut(\n        -df_local[\"uplift_hat\"],\n        q=n_quantiles,\n        labels=[f\"Q{i+1}\" for i in range(n_quantiles)],\n    )\n\n    rows = []\n    for q in df_local[\"quantile\"].cat.categories:\n        subset = df_local[df_local[\"quantile\"] == q]\n        if subset.empty:\n            continue\n\n        treated = subset[subset[\"T\"] == 1]\n        control = subset[subset[\"T\"] == 0]\n\n        churn_treated = treated[\"y\"].mean() if not treated.empty else np.nan\n        churn_control = control[\"y\"].mean() if not control.empty else np.nan\n\n        observed_uplift = churn_control - churn_treated\n\n        rows.append(\n            {\n                \"quantile\": q,\n                \"n\": len(subset),\n                \"n_treated\": len(treated),\n                \"n_control\": len(control),\n                \"churn_treated\": churn_treated,\n                \"churn_control\": churn_control,\n                \"observed_uplift\": observed_uplift,\n            }\n        )\n\n    return pd.DataFrame(rows)\n\n\nuplift_q_df = uplift_by_quantile(\n    uplift_scores=uplift_hat_test,\n    y=y_test,\n    T=T_test,\n    n_quantiles=5,\n)\n\nuplift_q_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Plot observed uplift by quantile\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=uplift_q_df, x=\"quantile\", y=\"observed_uplift\")\nplt.axhline(0, color=\"black\", linestyle=\"--\")\nplt.ylabel(\"Observed uplift (churn reduction)\")\nplt.title(\"Observed uplift by predicted uplift quantile\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "If the uplift model is effective, the **top quantiles** (e.g. Q1, Q2) should\nshow **higher positive observed uplift** than the lower quantiles.\n\nThis means that, if you could only treat a subset (e.g. just Q1), you would\nget more churn reduction than treating a random subset of the same size.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Business interpretation \u2013 using uplift scores\n\nIn a real bank retention campaign, your constraints might be:\n\n- Limited **budget** (you cannot treat everyone).\n- Limited **operational capacity** (calls per month, emails with human follow-up).\n\nThe uplift model supports decisions such as:\n\n1. **Who to target?**\n   - Rank customers by `uplift_hat` (expected churn reduction if treated).\n   - Choose the top `K` or top `X%` subject to budget.\n\n2. **How to evaluate a campaign design?**\n   - Compare uplift-by-quantile between different models or strategies.\n   - Estimate **incremental churn reduction** (or revenue) when targeting\n     only high-uplift customers vs everyone.\n\n3. **How to combine with value (CLV)?**\n   - Multiply predicted uplift (churn reduction) by customer value.\n   - Rank customers by **expected incremental CLV**:\n\n   ```text\n   incremental_value \u2248 uplift_hat(x) * CLV(x)\n   ```\n\n   - This is particularly powerful when combined with the CLV notebook.\n\nThe key mindset shift is:\n\n> Do not target everyone with high churn risk. Target those where\n> the **treatment actually changes the outcome** in a meaningful way.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Limitations and possible extensions\n\nThis notebook uses a **simulated experiment**, which is great for learning but\ncomes with limitations:\n\n- The true data-generating process is **hand-crafted**, not from a real RCT.\n- We used a simple two-model uplift approach; in practice you might use:\n  - Meta-learners (T-learner, S-learner, X-learner).\n  - Dedicated uplift models (e.g. uplift random forests, causal forests).\n- We focused on a single outcome (churn). Real campaigns might track multiple\n  KPIs (activation, product uptake, net revenue, etc.).\n\nPotential next steps:\n\n1. Add **costs** and **budget constraints** to derive optimal targeting rules.\n2. Integrate uplift with **CLV** to maximise **incremental LTV**.\n3. Use libraries for **causal inference** / **uplift modelling** for more\n   advanced estimators.\n4. Simulate **non-randomised** policies and use causal methods to correct bias.\n\nEven with these simplifications, this project demonstrates how to:\n\n- Frame a churn retention campaign as a **treatment effect** problem.\n- Build an **uplift model** to prioritise customers.\n- Evaluate uplift in a way that reflects **incremental impact** rather than\n  just classification accuracy.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}