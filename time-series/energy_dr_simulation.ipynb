{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demand Response / Flexibility Simulation (Household Energy)\n",
        "\n",
        "This notebook builds a **toy demand response (DR) simulator** on top of\n",
        "household energy data.\n",
        "\n",
        "We assume you have the **Individual household electric power consumption**\n",
        "dataset, but the notebook is self-contained: it can also recompute daily\n",
        "features and clusters if needed.\n",
        "\n",
        "## Goals\n",
        "\n",
        "- Use **daily clusters** as behavioural segments (evening-heavy, day-heavy,\n",
        "  low-use days, etc.).\n",
        "- Identify **flexible hours** (evening peaks) and **shoulder hours**.\n",
        "- Define a simple **price signal** (DR control events with high prices).\n",
        "- Simulate **behaviour change**:\n",
        "  - 10\u201325% reduction in evening consumption,\n",
        "  - energy shifted towards shoulder hours.\n",
        "- Evaluate:\n",
        "  - Energy shifted out of peak window,\n",
        "  - Peak reduction,\n",
        "  - Impact on costs under a simple price curve.\n",
        "- Visualise:\n",
        "  - Before/after load shapes for example days,\n",
        "  - Distribution of savings per day and per cluster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup and data\n",
        "\n",
        "We use the **Individual household electric power consumption** dataset.\n",
        "\n",
        "Expected file:\n",
        "\n",
        "```text\n",
        "data/household_power.csv\n",
        "```\n",
        "\n",
        "With (at least) the usual columns:\n",
        "\n",
        "- `Date`, `Time`\n",
        "- `Global_active_power`\n",
        "- `Global_reactive_power`, `Voltage`, `Global_intensity`\n",
        "- `Sub_metering_1`, `Sub_metering_2`, `Sub_metering_3`\n",
        "\n",
        "If you already ran the *daily pattern clustering* project and exported\n",
        "`data/daily_profiles_with_clusters.csv`, this notebook will reuse it.\n",
        "Otherwise, it will recompute clusters from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.1 Imports and paths\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (11, 5)\n",
        "\n",
        "DATA_PATH = Path(\"data\") / \"household_power.csv\"\n",
        "CLUSTER_EXPORT_PATH = Path(\"data\") / \"daily_profiles_with_clusters.csv\"\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Expected dataset at {DATA_PATH.resolve()}\\n\"\n",
        "        \"Download 'Individual household electric power consumption' and save as 'data/household_power.csv'.\"\n",
        "    )\n",
        "\n",
        "raw = pd.read_csv(DATA_PATH)\n",
        "raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. From raw data to hourly kWh\n",
        "\n",
        "We:\n",
        "\n",
        "- Combine `Date` and `Time` into a timestamp.\n",
        "- Convert energy-related columns to numeric.\n",
        "- Resample from 1-minute to **hourly**.\n",
        "- Compute hourly `kwh` from average `Global_active_power`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_household_power(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean and resample the household power dataset to hourly kWh.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        Raw dataframe with Date/Time, Global_active_power and related columns.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Hourly dataframe indexed by timestamp with at least:\n",
        "        - kwh: energy in that hour (approx. mean kW * 1 hour)\n",
        "        - global_active_power: mean kW in that hour\n",
        "        - sub_metering_1/2/3: hourly sums (if available)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    if not {\"Date\", \"Time\"}.issubset(df.columns):\n",
        "        raise ValueError(\"Expected 'Date' and 'Time' columns in dataset.\")\n",
        "\n",
        "    df[\"timestamp\"] = pd.to_datetime(\n",
        "        df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str), errors=\"coerce\"\n",
        "    )\n",
        "    df = df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
        "\n",
        "    num_cols = [\n",
        "        \"Global_active_power\",\n",
        "        \"Global_reactive_power\",\n",
        "        \"Voltage\",\n",
        "        \"Global_intensity\",\n",
        "        \"Sub_metering_1\",\n",
        "        \"Sub_metering_2\",\n",
        "        \"Sub_metering_3\",\n",
        "    ]\n",
        "    for col in num_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    df = df.set_index(\"timestamp\").sort_index()\n",
        "\n",
        "    hourly = pd.DataFrame()\n",
        "    if \"Global_active_power\" in df.columns:\n",
        "        hourly[\"global_active_power\"] = df[\"Global_active_power\"].resample(\"H\").mean()\n",
        "        hourly[\"kwh\"] = hourly[\"global_active_power\"]\n",
        "\n",
        "    for col in [\"Sub_metering_1\", \"Sub_metering_2\", \"Sub_metering_3\"]:\n",
        "        if col in df.columns:\n",
        "            hourly[col] = df[col].resample(\"H\").sum()\n",
        "\n",
        "    hourly = hourly.dropna(subset=[\"kwh\"])\n",
        "    return hourly\n",
        "\n",
        "\n",
        "hourly = clean_household_power(raw)\n",
        "hourly.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Quick view of hourly consumption\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hourly[\"kwh\"].plot(alpha=0.7)\n",
        "plt.title(\"Hourly energy consumption (kWh)\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.show()\n",
        "\n",
        "sample_start = hourly.index.min() + pd.Timedelta(days=7)\n",
        "sample_end = sample_start + pd.Timedelta(days=7)\n",
        "sample = hourly.loc[sample_start:sample_end]\n",
        "sample[\"kwh\"].plot()\n",
        "plt.title(\"Sample week of hourly consumption\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Daily features and clusters\n",
        "\n",
        "We create one row per day with:\n",
        "\n",
        "- Daily totals, max, mean.\n",
        "- Day/night/evening fractions.\n",
        "- 24h profile: kWh per hour (h_00..h_23).\n",
        "\n",
        "If an exported `daily_profiles_with_clusters.csv` exists, we reuse the\n",
        "`cluster` labels from there. Otherwise we run KMeans to create clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_daily_profile_frame(hourly_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Create daily features and 24h profiles from hourly kWh.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    daily_features : pd.DataFrame\n",
        "        One row per date with aggregate features and fractions.\n",
        "    daily_profiles : pd.DataFrame\n",
        "        One row per date, columns `h_00`..`h_23` with kWh at each hour.\n",
        "    \"\"\"\n",
        "    df = hourly_df.copy()\n",
        "    df[\"date\"] = df.index.date\n",
        "    df[\"hour\"] = df.index.hour\n",
        "\n",
        "    profile = df.pivot_table(\n",
        "        index=\"date\",\n",
        "        columns=\"hour\",\n",
        "        values=\"kwh\",\n",
        "        aggfunc=\"mean\",\n",
        "    )\n",
        "    profile.columns = [f\"h_{h:02d}\" for h in profile.columns]\n",
        "\n",
        "    daily = df.groupby(\"date\").agg(\n",
        "        total_kwh=(\"kwh\", \"sum\"),\n",
        "        max_kwh=(\"kwh\", \"max\"),\n",
        "        mean_kwh=(\"kwh\", \"mean\"),\n",
        "    )\n",
        "\n",
        "    def _fraction_sum(mask: pd.Series) -> pd.Series:\n",
        "        return df.loc[mask, :].groupby(\"date\")[\"kwh\"].sum()\n",
        "\n",
        "    day_mask = (df[\"hour\"] >= 8) & (df[\"hour\"] < 18)\n",
        "    night_mask = (df[\"hour\"] < 6) | (df[\"hour\"] >= 22)\n",
        "    evening_mask = (df[\"hour\"] >= 18) & (df[\"hour\"] < 23)\n",
        "\n",
        "    day_kwh = _fraction_sum(day_mask)\n",
        "    night_kwh = _fraction_sum(night_mask)\n",
        "    eve_kwh = _fraction_sum(evening_mask)\n",
        "\n",
        "    daily[\"day_kwh\"] = day_kwh\n",
        "    daily[\"night_kwh\"] = night_kwh\n",
        "    daily[\"evening_kwh\"] = eve_kwh\n",
        "\n",
        "    daily[\"day_frac\"] = daily[\"day_kwh\"] / daily[\"total_kwh\"]\n",
        "    daily[\"night_frac\"] = daily[\"night_kwh\"] / daily[\"total_kwh\"]\n",
        "    daily[\"evening_frac\"] = daily[\"evening_kwh\"] / daily[\"total_kwh\"]\n",
        "\n",
        "    features = daily.join(profile, how=\"inner\")\n",
        "    return features, profile\n",
        "\n",
        "\n",
        "daily_features, daily_profiles = build_daily_profile_frame(hourly)\n",
        "daily_features.index = pd.to_datetime(daily_features.index)\n",
        "daily_features[\"weekday\"] = daily_features.index.dayofweek\n",
        "daily_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Attach or compute clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if CLUSTER_EXPORT_PATH.exists():\n",
        "    existing = pd.read_csv(CLUSTER_EXPORT_PATH, index_col=\"date\", parse_dates=[\"date\"])\n",
        "    if \"cluster\" in existing.columns:\n",
        "        daily_features[\"cluster\"] = existing.loc[daily_features.index, \"cluster\"].to_numpy()\n",
        "    else:\n",
        "        print(\"Cluster file found but no 'cluster' column \u2013 will recompute.\")\n",
        "else:\n",
        "    print(\"No precomputed clusters found \u2013 will compute KMeans clusters.\")\n",
        "\n",
        "if \"cluster\" not in daily_features.columns:\n",
        "    cluster_cols = [c for c in daily_features.columns if c not in [\"weekday\"]]\n",
        "    X = daily_features[cluster_cols].fillna(0.0).to_numpy()\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    sil_scores: Dict[int, float] = {}\n",
        "    for k in range(2, 7):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=20)\n",
        "        labels = kmeans.fit_predict(X_scaled)\n",
        "        sil_scores[k] = silhouette_score(X_scaled, labels)\n",
        "\n",
        "    best_k = max(sil_scores, key=sil_scores.get)\n",
        "    print(\"Silhouette scores:\", sil_scores)\n",
        "    print(\"Using K=\", best_k)\n",
        "\n",
        "    kmeans_final = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=50)\n",
        "    daily_features[\"cluster\"] = kmeans_final.fit_predict(X_scaled)\n",
        "\n",
        "daily_features[\"cluster\"].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. DR events, flexible hours and price signal\n",
        "\n",
        "We define a simple DR control policy:\n",
        "\n",
        "- **Flexible hours** (peak window): 18:00\u201321:59 (hours 18\u201321).\n",
        "- **Shoulder hours** where load can be shifted: 14\u201317 and 22\u201323.\n",
        "- **DR event days**: days in the top 20% of total_kwh (heaviest days).\n",
        "- **Price curve**:\n",
        "  - Off-peak (night): cheap (0.8 units).\n",
        "  - Normal hours: base price (1.0).\n",
        "  - DR peak hours on event days: high price (3.0).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "FLEX_HOURS = list(range(18, 22))\n",
        "SHOULDER_HOURS = list(range(14, 18)) + [22, 23]\n",
        "\n",
        "def select_dr_event_days(daily: pd.DataFrame, quantile: float = 0.8) -> List[pd.Timestamp]:\n",
        "    \"\"\"Select DR event days as those with total_kwh above the given quantile.\"\"\"\n",
        "    threshold = daily[\"total_kwh\"].quantile(quantile)\n",
        "    mask = daily[\"total_kwh\"] >= threshold\n",
        "    return list(daily.index[mask])\n",
        "\n",
        "\n",
        "dr_event_days = select_dr_event_days(daily_features, quantile=0.8)\n",
        "len(dr_event_days), dr_event_days[:5]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_price_series(hourly_df: pd.DataFrame, dr_days: List[pd.Timestamp]) -> pd.Series:\n",
        "    \"\"\"Construct a simple price time series.\n",
        "\n",
        "    - Off-peak (hours < 6 or >= 22): 0.8\n",
        "    - Normal: 1.0\n",
        "    - On DR days, flexible hours (18\u201321): 3.0\n",
        "    \"\"\"\n",
        "    idx = hourly_df.index\n",
        "    hours = idx.hour\n",
        "    dates = idx.date\n",
        "\n",
        "    base_price = np.full(len(idx), 1.0)\n",
        "    offpeak_mask = (hours < 6) | (hours >= 22)\n",
        "    base_price[offpeak_mask] = 0.8\n",
        "\n",
        "    dr_dates = {d.date() if isinstance(d, pd.Timestamp) else d for d in dr_days}\n",
        "    peak_mask = np.isin(dates, list(dr_dates)) & np.isin(hours, FLEX_HOURS)\n",
        "    base_price[peak_mask] = 3.0\n",
        "\n",
        "    return pd.Series(base_price, index=idx, name=\"price\")\n",
        "\n",
        "\n",
        "price_series = build_price_series(hourly, dr_event_days)\n",
        "price_series.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick check on one DR day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "example_day = dr_event_days[0]\n",
        "mask_day = hourly.index.date == example_day.date()\n",
        "price_series[mask_day].plot(marker=\"o\")\n",
        "plt.title(f\"Price curve for DR event day {example_day.date()}\")\n",
        "plt.ylabel(\"Price units\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Behaviour model: shifting flexible load\n",
        "\n",
        "Each cluster gets an assumed **flexible fraction** of peak-window load.\n",
        "\n",
        "On DR days we:\n",
        "\n",
        "- Reduce flexible window load by that fraction.\n",
        "- Redistribute the removed energy to shoulder hours, proportional to\n",
        "  their baseline load.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_clusters = sorted(daily_features[\"cluster\"].unique())\n",
        "flex_fraction_by_cluster: Dict[int, float] = {}\n",
        "for cid in unique_clusters:\n",
        "    # Simple heuristic: higher cluster id -> more flexible\n",
        "    flex_fraction_by_cluster[cid] = 0.10 + 0.05 * cid\n",
        "\n",
        "flex_fraction_by_cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def simulate_dr(hourly_df: pd.DataFrame,\n",
        "                daily_feat: pd.DataFrame,\n",
        "                dr_days: List[pd.Timestamp],\n",
        "                flex_frac_by_cluster: Dict[int, float]) -> pd.DataFrame:\n",
        "    \"\"\"Simulate a DR program by shifting flexible evening load.\n",
        "\n",
        "    For each DR event day:\n",
        "    - Determine that day's cluster and its flexible fraction.\n",
        "    - Reduce evening load (flex window) by that fraction.\n",
        "    - Redistribute the removed energy to shoulder hours.\n",
        "    \"\"\"\n",
        "    dr_df = hourly_df.copy()\n",
        "    dr_dates = {d.normalize() for d in dr_days}\n",
        "\n",
        "    for day in dr_dates:\n",
        "        day_mask = dr_df.index.normalize() == day\n",
        "        if not day_mask.any():\n",
        "            continue\n",
        "\n",
        "        date_key = day.date()\n",
        "        try:\n",
        "            cluster = int(daily_feat.loc[pd.Timestamp(date_key), \"cluster\"])\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        flex_frac = flex_frac_by_cluster.get(cluster, 0.10)\n",
        "\n",
        "        hours = dr_df.index[day_mask].hour\n",
        "        flex_hours_mask = day_mask & np.isin(dr_df.index.hour, FLEX_HOURS)\n",
        "        shoulder_hours_mask = day_mask & np.isin(dr_df.index.hour, SHOULDER_HOURS)\n",
        "\n",
        "        if not flex_hours_mask.any() or not shoulder_hours_mask.any():\n",
        "            continue\n",
        "\n",
        "        flex_load = dr_df.loc[flex_hours_mask, \"kwh\"]\n",
        "        shoulder_load = dr_df.loc[shoulder_hours_mask, \"kwh\"]\n",
        "\n",
        "        total_flex_energy = flex_load.sum()\n",
        "        energy_to_shift = total_flex_energy * flex_frac\n",
        "\n",
        "        dr_df.loc[flex_hours_mask, \"kwh\"] = flex_load * (1.0 - flex_frac)\n",
        "\n",
        "        shoulder_total = shoulder_load.sum()\n",
        "        if shoulder_total > 0:\n",
        "            extra_per_hour = energy_to_shift * (shoulder_load / shoulder_total)\n",
        "            dr_df.loc[shoulder_hours_mask, \"kwh\"] = shoulder_load + extra_per_hour\n",
        "\n",
        "    return dr_df\n",
        "\n",
        "\n",
        "hourly_dr = simulate_dr(hourly, daily_features, dr_event_days, flex_fraction_by_cluster)\n",
        "hourly_dr.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Example DR day \u2013 baseline vs DR\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "example_day = dr_event_days[0]\n",
        "mask_base = hourly.index.date == example_day.date()\n",
        "mask_dr = hourly_dr.index.date == example_day.date()\n",
        "\n",
        "plt.plot(hourly.index[mask_base].hour, hourly.loc[mask_base, \"kwh\"], label=\"baseline\")\n",
        "plt.plot(hourly_dr.index[mask_dr].hour, hourly_dr.loc[mask_dr, \"kwh\"], label=\"DR\", linestyle=\"--\")\n",
        "plt.xlabel(\"Hour of day\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.title(f\"Example DR day \u2013 baseline vs DR load ({example_day.date()})\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Metrics: energy shifted, peak reduction, cost impact\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_daily_metrics(hourly_base: pd.DataFrame,\n",
        "                          hourly_dr: pd.DataFrame,\n",
        "                          price: pd.Series,\n",
        "                          dr_days: List[pd.Timestamp]) -> pd.DataFrame:\n",
        "    \"\"\"Compute DR metrics per event day.\n",
        "\n",
        "    Returns one row per DR day with:\n",
        "    - baseline_peak, dr_peak, peak_reduction\n",
        "    - baseline_peak_energy, dr_peak_energy, energy_shifted_peak\n",
        "    - baseline_cost, dr_cost, cost_savings\n",
        "    \"\"\"\n",
        "    rows: List[Dict[str, float]] = []\n",
        "    dr_dates = {d.normalize() for d in dr_days}\n",
        "\n",
        "    for day in sorted(dr_dates):\n",
        "        base_mask = hourly_base.index.normalize() == day\n",
        "        dr_mask = hourly_dr.index.normalize() == day\n",
        "        if not base_mask.any() or not dr_mask.any():\n",
        "            continue\n",
        "\n",
        "        hours = hourly_base.index[base_mask].hour\n",
        "        flex_mask_day = np.isin(hours, FLEX_HOURS)\n",
        "\n",
        "        base_load = hourly_base.loc[base_mask, \"kwh\"].to_numpy()\n",
        "        dr_load = hourly_dr.loc[dr_mask, \"kwh\"].to_numpy()\n",
        "        day_price = price.loc[base_mask].to_numpy()\n",
        "\n",
        "        if flex_mask_day.any():\n",
        "            base_flex = base_load[flex_mask_day]\n",
        "            dr_flex = dr_load[flex_mask_day]\n",
        "            baseline_peak = float(base_flex.max())\n",
        "            dr_peak = float(dr_flex.max())\n",
        "            baseline_peak_energy = float(base_flex.sum())\n",
        "            dr_peak_energy = float(dr_flex.sum())\n",
        "            energy_shifted_peak = baseline_peak_energy - dr_peak_energy\n",
        "        else:\n",
        "            baseline_peak = dr_peak = baseline_peak_energy = dr_peak_energy = energy_shifted_peak = 0.0\n",
        "\n",
        "        baseline_cost = float((base_load * day_price).sum())\n",
        "        dr_cost = float((dr_load * day_price).sum())\n",
        "\n",
        "        rows.append(\n",
        "            {\n",
        "                \"date\": day.date(),\n",
        "                \"baseline_peak\": baseline_peak,\n",
        "                \"dr_peak\": dr_peak,\n",
        "                \"peak_reduction\": baseline_peak - dr_peak,\n",
        "                \"baseline_peak_energy\": baseline_peak_energy,\n",
        "                \"dr_peak_energy\": dr_peak_energy,\n",
        "                \"energy_shifted_peak\": energy_shifted_peak,\n",
        "                \"baseline_cost\": baseline_cost,\n",
        "                \"dr_cost\": dr_cost,\n",
        "                \"cost_savings\": baseline_cost - dr_cost,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(rows).set_index(\"date\")\n",
        "\n",
        "\n",
        "price_series = build_price_series(hourly, dr_event_days)\n",
        "metrics_df = compute_daily_metrics(hourly, hourly_dr, price_series, dr_event_days)\n",
        "metrics_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Aggregate view of DR impact\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metrics_df[[\"energy_shifted_peak\", \"peak_reduction\", \"cost_savings\"]].describe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metrics_df[\"energy_shifted_peak\"].hist(bins=20)\n",
        "plt.xlabel(\"Energy shifted out of peak window (kWh)\")\n",
        "plt.title(\"Distribution of shifted energy per DR day\")\n",
        "plt.show()\n",
        "\n",
        "metrics_df[\"cost_savings\"].hist(bins=20)\n",
        "plt.xlabel(\"Cost savings per DR day (price units)\")\n",
        "plt.title(\"Distribution of cost savings per DR day\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Savings by cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "day_to_cluster = daily_features[\"cluster\"].to_dict()\n",
        "metrics_df[\"cluster\"] = [day_to_cluster.get(pd.Timestamp(d), np.nan) for d in metrics_df.index]\n",
        "\n",
        "metrics_df.groupby(\"cluster\")[\"energy_shifted_peak\", \"cost_savings\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.boxplot(data=metrics_df, x=\"cluster\", y=\"cost_savings\")\n",
        "plt.title(\"Cost savings per DR day by cluster\")\n",
        "plt.ylabel(\"Savings (price units)\")\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(data=metrics_df, x=\"cluster\", y=\"energy_shifted_peak\")\n",
        "plt.title(\"Energy shifted out of peak by cluster\")\n",
        "plt.ylabel(\"kWh\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Wrap-up\n",
        "\n",
        "We built a **simple DR / flexibility simulator** over household energy data:\n",
        "\n",
        "- Clusters act as segments with different assumed flexibility.\n",
        "- On high-load DR days, evening load is reduced and shifted to shoulders.\n",
        "- We measured peak reduction, shifted energy, and cost savings.\n",
        "- We analysed which clusters contribute most to savings.\n",
        "\n",
        "Next steps could include learning flexibility parameters from real DR events,\n",
        "using probabilistic forecasts, or scaling to many households.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Policy / utility perspective\n",
        "\n",
        "The simulator we built is deliberately simple, but it mirrors how a utility or\n",
        "aggregator might reason about a **DR program**:\n",
        "\n",
        "- **Targeting**: clusters with higher `energy_shifted_peak` and `cost_savings`\n",
        "  are good candidates for DR offers (they respond well to signals or have more\n",
        "  flexible load to shift).\n",
        "- **Product design**:\n",
        "  - Evening peaker clusters \u2192 time-of-use tariffs with strong evening signals.\n",
        "  - Day-heavy clusters \u2192 incentives for midday shifting (solar or low wholesale prices).\n",
        "  - Low-use clusters \u2192 simpler, low-fixed products; DR impact per customer is small.\n",
        "- **System impact**: peak reduction and energy shifted translate directly into\n",
        "  lower peak generation / network stress and, in many markets, lower imbalance\n",
        "  or capacity charges.\n",
        "- **Customer impact**: cost savings per day and per cluster give a first-order\n",
        "  view of how attractive the DR program might feel to different segments.\n",
        "\n",
        "This kind of analysis is typically extended with:\n",
        "\n",
        "- **Uplift modelling** to estimate the causal effect of DR signals on load.\n",
        "- **Comfort constraints** (minimum evening use, maximum deferral time, etc.).\n",
        "- **Portfolio scaling** from a single household to thousands of meters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. DR campaign report\n",
        "\n",
        "To make the simulator more report-friendly, we aggregate per-day KPIs into a\n",
        "simple **campaign report**:\n",
        "\n",
        "- Number of DR event days.\n",
        "- Total and average energy shifted out of peak.\n",
        "- Total and average cost savings.\n",
        "- Breakdown by cluster.\n",
        "\n",
        "We also export the detailed per-day metrics to CSV for downstream BI or\n",
        "dashboarding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overall campaign KPIs\n",
        "n_days = len(metrics_df)\n",
        "total_shifted = metrics_df[\"energy_shifted_peak\"].sum()\n",
        "avg_shifted = metrics_df[\"energy_shifted_peak\"].mean()\n",
        "total_savings = metrics_df[\"cost_savings\"].sum()\n",
        "avg_savings = metrics_df[\"cost_savings\"].mean()\n",
        "\n",
        "campaign_summary = pd.DataFrame(\n",
        "    {\n",
        "        \"n_dr_days\": [n_days],\n",
        "        \"total_shifted_kwh\": [total_shifted],\n",
        "        \"avg_shifted_kwh_per_day\": [avg_shifted],\n",
        "        \"total_savings_units\": [total_savings],\n",
        "        \"avg_savings_units_per_day\": [avg_savings],\n",
        "    }\n",
        ")\n",
        "campaign_summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cluster-level campaign KPIs\n",
        "cluster_campaign = (\n",
        "    metrics_df.groupby(\"cluster\")[\"energy_shifted_peak\", \"cost_savings\"]\n",
        "    .agg([\"count\", \"sum\", \"mean\"])\n",
        ")\n",
        "cluster_campaign"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Export detailed DR metrics for BI / dashboarding\n",
        "dr_metrics_path = Path(\"data\") / \"dr_campaign_daily_metrics.csv\"\n",
        "dr_metrics_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "metrics_df.to_csv(dr_metrics_path, index_label=\"date\")\n",
        "print(\"Exported DR daily metrics to:\", dr_metrics_path.resolve())"
      ]
    }
  ]
}